id,title,tenant_id,owner_person_id,creation_timestamp,state,description,goal,max_students,difficulty,completion_criteria,academic_session_id,project_type,tag_asg
4361,Operation oriented NoSQL database,ug4,1334,2020 02 11 12:45:47 00,1,"I'm pretty sure no one really likes programming in SQL, but I'm also not sure that many of the NoSQL DBs are much more fun.But, one that is popular is Redis. It is in an memory DB which supports various data structures and operations on them. The data structures are things like lists, hash maps, etc. The users' interaction is all at the data level. In other words, they operate on lists etc. not their higher level operations. Those operations are then replicated and persisted. The data can be reconstructed by replaying the operations   however, as already said, they are very low level.The idea in this project is to let the developer manage the in memory model themselves and only deal with persistence and replication etc at the highest possible level.For example, suppose I am modelling a widget factory and typical operations that write to the model include: rebalancing all the widget doodads by a factor of 1.5; incrementing the nogglebats. I have easily built an in memory version of this. I want to persist and replicate those operations. Each of them requires thousands or millions of changes to the underlying model, touching large parts of my in memory model. If I do this in SQL or something like Redis, I have to work out how to map all those low level operations and pay for them to be transmitted somewhere for replication and persistence.Instead, when I make the call "";rebalance_widget_doodads(1.5)""; I can just persist and replicate that   i.e. the very string  "";rebalance_widget_doodads(1.5)"";. Later I can recreate the in memory model by replaying those calls, and I can likely gossip about them to other servers to do replication.   Compare this to Redis, where I would have to persist and replicate millions of calls to low level data structures   and, worse, as the developer, I couldn't even use the native data types I normally have available. It would be so much closer to what developers actually want to deal with.So, at the simplest, we need a way to indicate which objects and APIs are persistable and to serialise/deserialise the calls (ideally in a human readable format). In Java/Scala, this might be marking methods or classes with some annotation. Depending on the language you choose this could be easy or hard. That should be all the programmer would need.   From then on, any call to those methods would be persisted and replicated. Care would be needed if the methods were not just statics.Such a system could be heavily optimised to increase the scope of the project.",Build an operation oriented NoSQL database,1,3   Hard,An operation oriented NoSQL database,8,0,f
4362,Automatic Conversion Between Flavours of SQL,ug4,1334,2020 02 11 12:46:13 00,1,"There is no agreed upon version of SQL, so once an organisation commits to using one database provider, they can then become locked in.This project aims to partially solve this problem.   Given SQL in one flavour, for example, MySQL, emit equivalent SQL that will have identical semantics when run in another flavour, e.g. Postgres.There will likely need to be a great deal of compiler work, so this is not for the faint of heart.",Build a transpiler between two or more flavours of SQL,3,4   Very Hard,A transpiler between one or more SQL flavours. Tests demonstrating success.,8,0,f
4363,Auto Parallelisation Through Deep Learning,ug4,1334,2020 02 11 12:46:32 00,1,"The automatic parallelisation of sequential programs has long been a grail of compiler research, but faces a multitude of difficult problems. A significant challenge is in correctly identifying code which can be parallelised. Traditional static analysis approaches prove too conservative in practice and will miss valuable opportunities for parallelisation.The goal of this project is to develop a new approach to identifying potential parallelism, based on the automatic detection of parallelisable patterns in code. Given a corpus of example programs matching a particular pattern, e.g. stencil codes, a deep learning language model will be trained to identify the presence of this pattern in unseen programs by comparing likeliness of the code to the example programs. These example programs may be automatically generated from an algorithmic template, and unseen programs for testing the model may be taken from open source projects. Once identified, instances of these patterns may be parallelised. An excellent project will be able to automatically parallelise identified patterns.",Develop deep machine learning models to suggest auto parallelisation opportunities in legacy code,1,Challenging,"Minimum criteria include developing a tool for generating permutations of an algorithmic template, e.g. stencil codes, and training a discriminative language model on these generated codes which can predict with reasonable accuracy whether an unseen program matches the pattern. A good project will extend the number of patterns and will be able to identify and isolate an instance of the pattern from a larger program. An excellent project will  be able to automatically parallelise the identified patterns.",8,0,f
4364,Parsing the STAC corpus,ug4,1778,2020 02 11 15:01:59 00,1,"The STAC corpus (https://www.irit.fr/STAC/corpus.html) is a corpus of negotiations taking place in the multi player win lose game the Settlers of Catan. The "";units""; file contains mentions and anaphoric relations for resources, and dialogue acts (e.g., offer, question).   The "";discourse""; file contains annotations for discourse structure: that is, coherence relations that connect discourse units (where the units themselves can be extended segments of coherently related units, thereby generating a hierarchical structure).   The discourse structure annotations comply with Segmented Discourse Representation Theory (SDRT, Asher and Lascarides 2003, Lascarides and Asher 2007). There are two versions of the corpus: one in which only linguistic units are labelled, and another in which coherence relations between linguistic and non linguistic units are labelled (e.g., the game move of rolling a 7 and a player saying "";YAY!""; may be annotated with the latter being a COMMENTARY on the former).   There is published work on predicting the dialogue acts and discourse structure for the linguistic only STAC corpus (e.g, Afantenos et al, 2015).   There is scope for taking this in two alternative directions: (a) adapting and testing the Afantenos et al (2015) model on the situated version of the STAC corpus; or (b) adapating and implementing an alternative parsing model to that of Afantenos et al 2015 (e.g., by using the structured NNs from Liu and Lapata (2018))   and comparing the results on the linguistics only version of the STAC corpus.",To implement and test an existing model of semantic parsing on the STAC corpus,2,4   Very Hard,The student should deliver the following: (i) the design of a model for learning a mapping from dialogues to dialogue acts and/or discourse structure; (ii) an implementation of this model; (iii) evaluation of the model against held out test data from the STAC corpus.,8,0,f
4365,Learning to play Draughts,ug4,1778,2020 02 11 15:47:08 00,1,"There are many ways of learning strategies for playing games from agent simulations.   This project involves choosing a particular method   e.g., Monte Carlo Tree Search (MCTS) or Deep Reinforcement Learning (RL)   for learning to play Draughts.   The rules of the game are here:https://en.wikipedia.org/wiki/DraughtsYour tasks would be as follows:1) Build a   game and simulation environment for Draughts (or find an existing implementation that you can build on).2) Design and implement an agent (or agents) that play Draughts, via simple heuristics based methods.3) Design and implement a model for   learning to play Draughts (e.g., MCTS or DRL).4) Evaluate these models against some baseline (e.g., the heuristics based agents you developed in the first place, or random play).",To use a MCTS or a similar approach to learn to play Draughts,1,1   Easy,Implementaiton of a Draughts simulation environoment; implementation of heuristics based players; testing and evaluation of a player that has learned its strategy from game simulations.,8,0,f
4366,Generating a Compiler Backend from an Architecture Specification,ug4,1390,2020 02 11 16:48:29 00,1,"The development of compiler backends is a largely manual effort despite the availability of some automated tools, which provide automation of part of the task, e.g. instruction selector generators. In this project we wish to investigate the feasibility of automating the automatic generation of an entire compiler backend from a machine readable architecture specification. This could, for example, be based on the ArchC architecture description language, which is used in the locally developed GenSim tool suite for simulator generation and produce an LLVM compiler backend.We envisage the student working on this project to define a minimal instruction set architecture (with maybe 10 20 simple instructions) and to explore the challenges of processing and translating the instruction specifications for different classes of instructions into matching rules for an LLVM backend. The student would investigate novel solutions to the problems in matching the semantics of instruction templates with those of IR nodes, and synthesising instruction sequences for the implementation of IR nodes. To demonstrate feasibility of this process a prototype LLVM backend generator tool would be developed for the minimal target architecture.",To investigate the feasibility of generating compiler backends automatically from architecture specifications written in an architecture description language,1,3   Hard,"A tiny instruction set architecture specified in the ArchC architecture description language, from which a (simple) LLVM backend is generated automatically.",8,0,f
4367,Retrofitting GraphBLAS to Legacy Code,ug4,1390,2020 02 11 16:49:07 00,1,"Graphs are important data structures used in many applications. As software engineering practices have evolved over time, and the sizes of graphs to process have increased, novel and efficient graph abstractions such as GraphBLAS have been developed. In this project the student will investigate the challenges in retrofitting GraphBLAS to existing graph processing applications, i.e. legacy code. The project will be implementation driven with examples taken from standard benchmarks and other publicly available sources. While legacy graph processing applications are transformed to take advantage of GraphBLAS the student will document the transformation effort and challenges, while distilling "";lessons learned""; from this experience. The project will include a performance evaluation of the transformed codes, and also incorporate other metrics such as code complexity measures.",To convert graph based legacy code to make use of GraphBLAS,2,2   Moderate,"Completed the transformation of a small number of C/C   based legacy applications using user defined graph data structures and algorithms to make use the GraphBLAS specification, and evaluated the resulting code in terms of software engineering metrics as well as performance.",8,0,f
4369,Writing a minimal AArch64 JIT compiler in C,ug4,1390,2020 02 11 16:50:20 00,1,"This software development project is concerned with the retargeting of an existing minimalistic JIT compiler to support the AArch64 architecture. Starting point is an existing x86 64 version of the JIT compiler. The project would initially focus on the support of AArch64 and evaluate the basic compiler in terms of performance, code size, and code quality.Extensions of the project are possible and comprise various JIT code optimisations. Furthermore, targeting of specialised instructions, e.g. for vector processing, would also be possible once the basic implementation goals have been achieved.",To port an existing x86 64 JIT compiler to AArch64,1,2   Moderate,"Minimalistic compiler ported to AArch64 and evaluated (performance, code size, code quality) on an ARM server. Comparison against original x86 64 version.",8,0,f
4370,Interactive Task Learning using Corrective Dialogue Moves,ug4,1778,2020 02 12 10:48:39 00,1,"This student would work with Yordan Hristov and Mattias Appelgren (who are PhD students jointly supervised by Ram Ramamoorthy and me).   So far, Yordan Hristov has developed state of the art models of symbol grounding, using weakly supervised data.   The training exemplars are all of the form of a photo real image and accompanying utterances like "";This is a blue block"";.   On the other hand, Mattias Appelgren has developed an AI agent that learns from corrective feedback: e.g., the robot has just put a yellow block on a blue block in a tower building task, and the domain expert says "";No, put red blocks on blue blocks"";. In this case, the agent must learn to ground the colour red from an image and the utterance, knowing that because it's a correction the top block is *not* red.   The purpose of this project is to bring these pieces of work together: instead of learning to ground colour terms via corrective feedback, you will aim to learn to recognise more complex concepts: e.g., "";No, put lettuce on washing powder"";.     If things go well, we might even tackle learning abstract concepts: e.g., "";No, put light things on heavy things"";, where the agent will need to learn which objects are light and which are heavy.                        ",To contribute to developing an AI agent that learns to solve its planning problem via trial and error and corrective dialogue moves from a domain expert,1,3   Hard,"The completion criteria will be experiments that measure the cumulative cost as the robot attempts its packing/tower building task, compared against a baseline that doesn't attempt to interpret the rule expressed by the domain expert says when he corrects the agent (i.e., the baseline just exploits ""No"").",8,0,f
4371,Automatic Proposal of Experiments to Discriminate Between Alternative Repairs to a Faulty Theory,ug4,1358,2020 02 12 14:08:51 00,1,"The ABC system repairs faulty logical theories. It is given a theory, T, as a set of axioms in the decidable logic Datalog, and two sets of observations, as ground propositions. One set is of ground propositions that are observed to be true of the environment and the other is of ground propositions observed to be false. Theory T is used for predictions about the environment. Its predictions are wrong if it proves something false (incompatibility) or fails to prove something true (insufficiency). The ABC system then tries to repair T either by adding/deleting axioms, deleting/adding preconditions to rules or changing T's language. Language changes are implemented by reformation and consist of splitting/merging predicates or constants, or adding/deleting arguments of predicates. ABC will often propose alternative fault free repairs to a faulty theory. If, however, the set of either true or false observations were increased, then ABC might detect new faults in these previously fault free repaired theories. The aim of the project is to suggest one or more new observations whose truth or falsity would determine which repairs to prefer. Ideally, if a new observation is found to be true, then roughly half the repaired theories would be insufficient and if it is found to be false then the other half would be incompatible. This process could be repeated until only one repaired theory remained fault free. There are various optional extensions to this project in implementing a mechanism for determining the truth value of the proposed observations. Which one(s) should be implemented would depend on the progress of the student. For instance, the simplest option would be to ask the user. We would expect at least this one to be implemented via a GUI. Or a web query could be made to see if the relevant knowledge were in the public domain. Our FRANK query answering system might used for this. Most ambitiously, automated instruments could be used to make appropriate observations. This last option is probably more at PhD level though. ",Develop a mechanism that will suggest what observations need to be made to discriminate between alternative ways of repairing a faulty theory. Explore options for gathering these observations.,1,3   Hard,"An algorithm that, give a set of alternative, fault free repaired theories, suggests new observations to be made to reject roughly half of them as faulty. Plus some method for determining whether the observation is true or false.",8,0,f
4378,Measure cookie setting behavior of web pages showing cookie privacy warnings,ug4,1351,2020 02 12 23:01:33 00,1,"Cookies can be used to track people as they visit websites across the Internet. Due to GDPR and other privacy regulation, organizations need user "";consent""; in order to engage in that type of tracking leading to warnings like the one below appearing on many websites. Or a more tricky version that does not use the word "";cookie"";.The Project: Measure the content and behavior of these warnings. You will need to build an automated web scraping tool which looks for these types of warnings and collects information from them. For example the content of the message and what kinds of options the user has. You will also need to explore the actual behavior of the warning in regards to cookies and settings. For example, are cookies set before the "";Accept cookies""; button is even hit (the message says it is)? If the user hits "";Decline cookies""; are all the cookies removed? Does the cookie content change if any of the buttons are pressed? The Challenge: While a simple goal, this project has some non obvious challenges. The first one is collecting the data set itself. You will need to find or build a tool capable of automatically identifying these dialogs, screenshot them, and run several cookie tests. You will also need to learn quite a bit about how cookies work, not just the text file part, but also issues like auto expiry and the differences between session and persistent cookies. You may also have to learn about technologies like Do Not Track (DNT) to see if sending different information to the site has any impact. Higher Marks: If what above looks too easy, then there is also the option of adding a user study to see how users expect these dialogs to behave. If you were to click "";Decline cookies""; in the above image would you expect it to delete all the cookies? Some of the cookies? And does what you expect match what it does?MInf: The above project is about the size of a UG4 project, but thankfully there is no end to privacy problems involving cookies. For an MInf student, you would complete the above project as described in the first year. For the second year you would identify an aspect of cookies you found interesting and we would scope a second study around that. Meetings: I am holding virtual office hours every morning 10am 11am at the following link: https://eu.bbcollab.com/guest/bdc065c679e64974a94001f8262dd7d0","Write a program that crawls web pages, collects cookie warnings, and automatically tests their behavior under various situations.",3,2   Moderate,"Write a program that crawls web pages, collects cookie warnings, and automatically tests their behavior under various situations.",8,0,f
4377,"Firewall administration, the game",ug4,1351,2020 02 12 22:58:35 00,1,"Managing the Firewall policy rules for a large network is quite challenging even for a skilled system administrator. Firewall policy rules contain constructs such as chains, rule ordering, and networking concepts. All of these are incredibly challenging to teach to new potential system administrators.The goal of this project is to create a game that simulates the problems firewall administrators face on a daily basis and helps a student learn about the various aspects of firewall administration in a game type environment. The game would be used to help teach students what network administration actually looks like. It would also be used in research studies to create a realistic environment to test new approaches in network administration.A player might start trying to secure a single computer that contains valuable data. They might then move on to handling a network where they get continuous tickets from users and alerts about new attacks that they need to manage. Information visualization approaches might also be integrated to assist the player in understanding what is happening on the network.A potential work program for this project would look like:1. Learn about firewall administration by doing a literature review, going through tutorials, and setting up a small test system to test out various configurations. Come up with some theories about what you think people need to know about firewalls.2. Mock up a game that targets specific aspects of firewall administration3. Test the mock up on a small number of other people.4. Design and build the game.5. Evaluate the game by having others play it.The game can be in any format that matches the target population. So a card game, computer game, web game, or app are all acceptable.This project can be undertaken by more than one student at once. However, I will require the students to select different Firewall applications to base their games on. For example: IPTables, Cisco, pf, ipfw, and ipfilter. This project has a wide range to the difficulty. It could be a quite simple project for someone with game experience or a good understanding of firewalls. There are also some research aspects to it, so a skilled student could make the game more advanced or do actual publishable research on it. MeetingsI am holding virtual office hours every morning 10am 11am at the following link: https://eu.bbcollab.com/guest/bdc065c679e64974a94001f8262dd7d0",Create a game that emulates the experience of being a security system administrator in charge of managing Firewall policy rules.,3,Variable,A program which emulates the typical duties of a Firewall system administrator in a game like environment. Play testing with other students.,8,0,f
4380,Structure learning of bipartite Ising models,ug4,3794,2020 02 13 11:03:51 00,1,"In https://arxiv.org/abs/1906.06595, Goel proposed the first algorithm to learn the underlying structure of an Ising model with arbitrary fields, provided that the structure is bipartite. This builds upon previous work of Bresler.In this project, you should implement this algorithm. Moreover, in order to generate samples, you need to also implement a sampling algorithm from the Ising model.",Implementing an algorithm of learning Ising models on a bipartite graph,2,4   Very Hard,Implementation of the learning algorithm. Implementation of the sampling algorithm. Experimentally study the performance,8,0,f
4381,Implementation of the Bayes filter algorithm,ug4,3794,2020 02 13 11:04:23 00,1,"In https://arxiv.org/abs/1907.06033, we proposed a perfect sampling algorithm that works for general spin systems. This is the first such that can utilise spatial mixing properties. In this project, you should implement this algorithm, and test it on discrete lattices, such as Z^d",Implementing the Bayes filter algorithm,1,1   Easy,Implementation of the algorithm. Experimental result on the grid.,8,0,f
4382,Implementation of a SAT counting algorithm,ug4,3794,2020 02 13 11:04:53 00,1,"In https://arxiv.org/abs/1911.01319, we proposed a Markov chain algorithm to sample solutions to k SAT uniformly at random. In this project, you should implement the algorithm and study, experimentally, when the algorithm is efficient.",Implementation of the algorithm,1,3   Hard,Implementation of the algorithm. Experimental study of the algorithm.,8,0,f
4383,Verification of networks of real time systems,ug4,1319,2020 02 13 19:15:49 00,1,"Real time systems operate with clocks and hard time limits on their behavior. Timed automata are a formal model to describe the behavior of real time systems, and their properties can be checked (see survey papers [2,3]). There also exist large well developed tools for working with timed automata, e.g., UPPAL (link [4]).However, this tool does not handle scalable networks of timed systems. Such networks consist of some number N of timed systems that run in parallel. Small groups of bounded size may communicate/synchronize by local hand shake. The goal is to prove that such a network operates correctly for any size N.A recent algorithm (see [1]) performs this verification of timed networks by a combination of abstraction and acceleration techniques. The worst case complexity is high (PSPACE hard, as anything about timed automata), but it is likely that the main loop converges after a small number of iterations in many instances.The project is to implement this algorithm and test it on examples.   ",Implement and test a recent algorithm for the verification of scalable networks of real time systems,1,2   Moderate,"1. Implementation of the algorithm from [1] (in Java, C  , or some other language). 2. Evaluation of the algorithm on test cases.",8,0,f
4384,Computing multipebble simulations symbolically with BDDs,ug4,1319,2020 02 13 19:16:34 00,1,"Buchi automata are automata on infinite words. Various simulation preorders are defined on Buchi automata. They under approximate language inclusion, but are easier to compute. These notions of simulation (direct, delayed, fair) differ in their handling of acceptance conditions.These simulations are generalized by the addition of multiple pebbles, which lead to larger relations at the expense of higher computational effort (i.e., there is a trade off). Simulations can be expressed by formulae in fixpoint logic (mu calculus) and thus calculated symbolically by using binary decision diagrams (BDDs) to describe large sets.The task is to implement these algorithms in Java, using a standard BDD library and evaluate the performance of the algorithm.The project requires fluency in automata theory, and the ability to study advanced concepts in the literature like BDDs, multipebble simulations and fixpoint logics. ","Implement an algorithm to compute direct multipebble simulation symbolically with binary decision diagrams (BDDs) in Java, and evaluate its performance.",1,3   Hard,Completion: Implementation of an algorithm for symbolic computation of direct multipebble simulation in Java. Performance evaluation on test cases. Comparison to an existing explicit state implementation that computes under approximations of these multipebble simulations. Possible extensions: similar algorithms for delayed/fair multipebble simulation.,8,0,f
4385,Optimizing algorithms in automata theory with Java fork join concurrency,ug4,1319,2020 02 13 19:18:17 00,1,"The RABIT tool (see [1]) is the world's best performing tool for checking language inclusion/equivalence of Buchi automata. RABIT is implemented in Java. It works by using a combination of many techniques, but (variants) of simulation preorder play a central role. For background on the algorithms in RABIT see [2]. For more general background see [3].However, RABIT is only partly parallelized. Some of the core algorithms have parallel versions (using Java fork join) concurrency, while others do not.The goal of this project is to improve the RABIT tool by re implementing several of its core algorithms in a concurrent way. Then you will evaluate your concurrent algorithms on examples to check how much speedup is attained over the sequential version.",Parallelize some automata theory algorithms in the RABIT tool by using Java fork join concurrency,2,1   Easy,"1. Concurrent re implementation of some core algorithms in RABIT. 2. Empirical evaluation on large test cases to assess the speedup over the sequential version. 3. Presentation of the results (tables, graphs, etc.)",8,0,f
4387,Sharing is Caring: Implementing Virtualised Wi Fi,ug4,1361,2020 02 17 12:22:17 00,1,"In popular locations, such as airports, stadiums, and cafÃ©s, Wi Fi infrastructure is frequently managed by local businesses and wireless service providers are required to share the access points (APs). While bandwidth partitioning can be easily handled by the AP for the downlink, this is a difficult task for the uplink, since Wi Fi operates with a decentralised medium access protocol. To tackle this problem, we have designed a virtualisation algorithm that can be executed at the access point, without requiring modifications to the end user equipment. The algorithm manipulates dynamically the contention configurations, in order to distribute the available bandwidth among service providers according to contracted shares. The aim of this project is to develop a prototype implementation of this virtualisation mechanism, using off the shelf APs, in order to demonstrate its practical feasibility and identify possible performance limitations.","Implement a Wi Fi virtualisation mechanism for commodity access points, that enables wireless infrastructure sharing among multiple operators.",2,4   Very Hard,A prototype system that enables the virtualisation of wireless access points.,8,0,f
4388,Investigation Vulnerabilities of Internet connected blood pressure monitors,ug4,1361,2020 02 17 12:27:08 00,1,"We are witnessing the rapid adoption of networked devices with embedded sensors and software in our homes and work environment. Objects can collect and exchange vast data volumes in the emerging Internet of Things ecosystem, which spans smart meters, appliances, surveillance cameras, urban infrastructure, autonomous cars, and many more. At the same time, open hardware platforms and drivers are becoming prevalent, which allows individuals with minimal networking and programming skills to hack such commodity web connected devices. This constitutes a major roadblock towards deploying IoT in critical public services, as long as their security remains under scrutiny. The goal of this project is to demonstrate the most common security vulnerabilities present in Internet connected blood pressure monitors. Once such pitfalls have been uncovered, the student will investigate security mechanisms that could be built into devices with limited capabilities.",Investigate the security vulnerabilities of wireless IoT devices.,1,2   Moderate,An analysis of the critical security vulnerabilities encountered and possible solutions.,8,0,f
4389,Indoor air quality monitoring,ug4,1361,2020 02 17 12:53:41 00,1,"Indoor air quality (IAQ) affects the health and wellbeing of building/office users, impacting negatively on their productivity and mood. This project will develop a prototype IoT system for indoor air quality monitoring, based on an embedded board and a range of sensors that can measure different pollutants. The system will be integrated with the cloud and trialled across different parts of the Appleton Tower and Informatics Forum, to inform future HVAC improvements that could be pursued to mitigate the problems identified. A Twitter based notification system will be implemented as well, whereby alerts will be sent when the air quality index crosses certain boundaries. Correlation analyses with outdoor weather conditions and office activity will also be conducted based on the measurements gathered.",Implement an indoor air quality system and perform data analysis based on the measurements collected,1,1   Easy,A working IAQ monitoring prototype and analyses based on data collected in AT/IF,8,0,f
4390,Visualization of composed cryptographic games and automatic derivation of simple cryptographic reductions,ug4,4012,2020 02 17 12:54:29 00,1,"The cryptographic game of a complex protocol can be represented as thecomposition of multiple stateful packages that interact using procedure calls(aka oracle queries) Contrary to general programs, we assume that the callgraph is non recursive and that each interaction between the environment andthe protocol terminates.Only procedures in the same package can share state directly. Otherwise statesharing only happens through calls.This allows to represent a protocol as a directed graph where nodescorrespond to packages and named edges correspond to calls from the call graph.In the paper "";State Separation for Code Based Game Playing Proofs""; wedeveloped a theory that allows to represent cryptographic reductions as graphrewriting. The graph of the composed cryptographic game is split into twoparts, one corresponding to the reduction, the other to a game for a buildingblock, consisting of fewer packages and smaller state. Based on an assumptionon the building block, the game for the building block is replaced by acomputationally indistinguishable game that offers stronger guarantees. Thisprocess can be repeated for the resulting composed game.A simple example for this is given in Figure 2 from the paper. A KEM DEMconstruction combines a key encapsulation mechanism (KEM) building blockbased on public key cryptography with a data encapsulation mechanism (DEM)building block based on symmetric key cryptography. Subfigure 2.(b) shows thefirst cut in the graph for the reduction to   the KEM assumption. Subfigure2.(d) shows the second cut for the reduction to the DEM assumption.",Visualization of composed cryptographic games and automatic derivation of simple cryptographic reductions,1,2   Moderate,"The student will identify a suitable graph and graphics library as the starting point, e.g. mxgraph which is the basis for draw.io, and will derive pictures depicting the structure of the proof and the reduction from a proof specification. Additional points can be gained by developing a proof search mechanism that finds simple proofs automatically by performing graph matching and rewriting using the specifications of assumptions expressed as graph rewriting rules.",8,0,f
4391,Localisation and Shape Estimation of a Robotic Bronchoscope in Lung Surgery,ug4,5340,2020 02 17 13:27:30 00,1,"Bronchoscopy is a procedure that lets doctors look at your lungs and air passages. During bronchoscopy, a thin tube (bronchoscope) is passed through your nose or mouth, down your throat, and into your lungs. One of the main challenges in bronchoscopy is localizing the exact position of the bronchoscope inside the lung and registering its position with respect to the human Anatomy.  The goal of this project is to use several sensors such as an electro magnetic tracker and a camera alongside a mathematical model of the bronchoscope to localise the bronchoscope and estimate its shape.  Students will have an opportunity to work with a multidisciplinary team of engineers, computer scientists, and  clinical experts. The student will have access to the robotics facilities at Bayes Center and Medical Research Facilities at Queen's Medical Research Institute, which are equipped with  state of the art robotics, computing, and imaging equipment.",Estimate the position and shape of a flexible bronchoscope inside the lung using image processing and sensor fusion algorithms,4,2   Moderate,Estimate the position of a bronchoscope inside a lung model with submillimeter accuracy.,8,0,f
4392,Simultaneous Localisation and Mapping (SLAM) of The Airways in Lung Bronchoscopy,ug4,5340,2020 02 17 13:27:55 00,1,"Using the bronchoscope, your doctor can view all of the structures that make up your respiratory system. These include your larynx, trachea, and the smaller airways of your lungs, which include the bronchi and bronchioles. The goal of this project is to use the visual feedback from the endoscopic camera place at the tip of the bronchoscope to construct and update a 3D map of the airways  while simultaneously keeping track of the bronchoscope within it.  Students will have an opportunity to work with a multidisciplinary team of engineers, computer scientists, and  clinical experts. The student will have access to the robotics facilities at Bayes Center and Medical Research Facilities at Queen's Medical Research Institute, which are equipped with  state of the art robotics, computing, and imaging equipment.",Use machine learning and image processing algorithms to segment and map the bronchial tree inside the lung,4,2   Moderate,The algorithms will be tested on endoscopic images. Their performance will be measured by average localisation error for each point after the full dataset has been mapped. It should be less than 0.1 mm. Computation speed should be equal or faster than 10 frames per sec.,8,0,f
4393,Model based Control of Continuum Robots,ug4,5340,2020 02 17 13:28:13 00,1,"Continuum robots are continuously flexible manipulators that can traverse confined spaces, manipulate objects in complex environments, and conform to curvilinear paths in space.  Continuum robots have also made a significant impact on robotic surgery. The enhanced dexterity and manipulability offered by continuum manipulators are an important factor enabling increasingly less invasive and more complex procedures.An example of Continuum Robots is Concentric Tube Robot (CTR).  Concentric tube robots are composed of series of pre curved elastic tubes that can be axially translated and rotated with respect to each other to control the shape of the robot.The goal of this project is to improve the existing mathematical models for CTRS to include and develop a model based controller that employs the feedback of the position of robot tip (from camera or a magnetic sensor) to  move the robot with submillimeter accuraciesIn this project, you will explore theoretical aspects of control and robotics and perform experimental work on a state of the art flexible surgical robot.",Develop a closed loop controller for position control of a flexible surgical robot,2,2   Moderate,Control the position of the robot and improve the accuracy of the existing model based controllers.,8,0,f
4394,Learning autonomous robotic grapsing,ug4,1990,2020 02 17 15:42:10 00,1,"BackgroundIn well defined cases, model based control, optimization, and planning could achieve good performance given substantial effort in the tuning of dozens or hundreds of hyperparameters. However, in dynamic and changing environments, those technologies are challenged due to the increasing number of unmodelled elements and uncertainties. Particularly in real world applications, physical interaction such as grasping and manipulation is a core process of most tasks, where the dimensionality of the control problem scales up significantly.Project detailsDespite the past methods solve some problems of complex physical interactions under well defined conditions, a more promising direction is to use machine learning to achieve self learning and accumulate experience for robotic manipulation and grasping. This project aims to use machine learning approaches (reinforcement learning, supervised learning) to achieve generality of the control policies for handling unspecified objects. The current machine learning approaches for grasping and manipulation are heavily vision based and this project plans to study autonomous grasping and manipulation by fusing multimodal sensory feedback, by integrating computer vision, impedance and force control, haptic sensing into the high level motion generation for producing truly reactive and intelligent behaviours. This project focuses on using the learning from demonstration approach to realize data efficient reinforcement learning and supervised learning. The students are expected to:develop an interface for the remote control of manipulation and grasping tasks. build up database of demonstrated grasping and manipulation, eg the user demonstrates the skill through using the robot. develop meta learning framework to efficiently learn new policies for grasping novel objects.develop an auto grasping system for novel objects in an unstructured environment, mainly using simulations (gazebo, vrep, pybullet, mujoco etc), camera calibration and  hand camera calibration, object detection, control of force based manipulators and software related development to make the whole process automatic (ROS, C  , Python skills are required)build a grasping dataset collection system using non contact method (leap motion, sensor glove etc), including data processing, filtering of IMU/tactile sensor, development of a wearable glove, software codebase and grasping database. (ROS, C  , Python skills are required)Note that this project has 2 positions, one focuses on No.4 auto grasping and the other focuses on No. 5 grasping database. Students who are interested in the project please send CVs to Zhibin Li (zhibin.li@ed.ac.uk), Wenbin Hu (wenbin.hu@ed.ac.uk), and Shuaijun Wang (shuaijun.wang@ed.ac.uk) to further discuss suitability and details of the project.  ",To achieve domain adaptive robotic grasping and generality of the control policies for handling unspecified objects.,2,4   Very Hard,"Auto grasping: Given the novel objects from the test set, the robot arm and hand should be able to complete grasping with success rate >80%. Grasping dataset: the database should include the most common types of objects in daily life, and the database should be able to train useful policies by supervised learning.",8,0,f
4395,Learning versatile dynamic motor skills for a quadruped robot,ug4,1990,2020 02 17 16:04:21 00,1,"Motivation and backgroundCompared to the human walking, many walking gaits realized by humanoid robots based on current engineering approaches are still very awkward and unnatural. Despite the engineering control approach is deterministic and analytic, as demonstrated by the Boston Dynamics' new Atlas robot, it still lacks of the elegance and energetic efficiency.A good way to develop a complex control system is to design a high level objective at the task or performance level, then give the authority to the optimization process to figure out the details. This optimization process can by realized either by a model based approach, ie using mathematical optimization tools to optimize the dynamics model; or by a model free approach, using machine learning to build up a statistical model and control policies via trials. This is similar to biological movements in nature that follows optimality principles, since they are shaped by iterative optimization processes, ie evolution.Goal: dynamic locomotion of a quadruped robot that traverses a variety of rugged terrainsPrior work: DeepMind Control Suite, OpenAI gym (see reference)Approach: Robot model    ANYmal robot in Pybullet physics simulator (see reference); method    deep reinforcement learning algorithmsTechnical aspects:Learn how to use   Pybullet physics engine;3D Model of the ANYmal robot in Pybullet simulator   with the same physical parameters, based on the existing Cheetah robot;Retrain the deep reinforcement learning in DeepMind Control Suite for the ANYmal model on the flat ground, achieve balanced locomotion;Add Lidar scanner in ANYmal model;Create obstacles in Pybullet simulator  ;Retrain the deep reinforcement learning using the height map scanned by Lidar or cameras.Expected outcomes: the 3D simulated robot should be able to trot, bounce, gallop on the flat ground, and negotiate with simple obstacles with visual input. Note: students are encouraged to send their CVs by emails for discussing the suitability to Zhibin Li (zhibin.li@ed.ac.uk), Kai Yuan (kai.yuan@ed.ac.uk), Wanming Yu (wanming.yu@ed.ac.uk)",Learn versatile dynamic motor skills for a quadruped robot to traverse a variety of rugged terrains,1,4   Very Hard,"Successful implementation of  the 3D simulated robot should be able to trot, bounce, gallop on the flat ground, and negotiate with simple obstacles with visual input.",8,0,f
4396,Advanced Microscopy Image Analysis for Life Science Applications,ug4,1990,2020 02 17 16:12:42 00,1,"Fluorescent microscopy imaging is an essential tool for biological and biomedical sciences. It generates a large pool of data which needs bias free and automated image data analysis and processing algorithms. The aim of the project is developing novel approaches for image processing and analysis by developing a pipeline of software image processing algorithms (image processing techniques and machine learning algorithms): Life Cycle Tracking of Vesicles using Computer Vision and Image Processing.In particular, the project will target processing temporal image sequences, where individual structures need to be registered, segmented and tracked. Obtained datasets will be individually analysed and compared to each other in order to investigate temporal morphological changes. The main task is to recognise and extract the bright areas (which are the areas of interests), compute the size and the centre of the area, in presence of background noises, and occlusion of the surrounding false image pixels. The techniques may require the use of object recognition using computer vision techniques   (deep learning and machine learning algorithms) for identifying bright areas (objects) in images.Students who are interested in the project please contact (with CVs) Zhibin Li (zhibin.li@ed.ac.uk) and  Jack Wilkinson (Jack.Wilkinson@ed.ac.uk) to further discuss suitability and details of the project. ",Developing image processing algorithm which will be able to follow and quantify temporal morphological changes on the cellular level,1,1   Easy,To produce identification of bright areas (or particularly the occlusion with several lumped samples) with a success rate more than 80% of our example datasets.,8,0,f
4386,Obfuscation for Privacy preserving Syntactic Parsing,ug4,1388,2020 02 17 11:35:25 00,1,"The goal of the project is to propose a new approach to preserve the privacy of information with cloud computing for Natural Langugage Processing (NLP) problems. Our approach is based obfuscation: we propose to develop analysis preserving transformations that can be used with an original text to get a transformed text that is sent to the server. Once the server applies the NLP analysis on the obfuscated text, it returns the response to the client, which further ``decrypts'' it to get the final result. Unlike traditional approaches to homomorphic encryption, ours does not incur significant additional computational cost or communication overhead. In addition, our approach does not require anyre adaptation of the NLP solver on the server side. It only requires the client to transform his input, and possibly send several requests to the server to unravel the correct NLP analysis for the text.The goal of this project is to improve a current system that does obfuscation for natural language data.Relevant paper: https://arxiv.org/abs/1904.09585Group webpage: http://bollin.inf.ed.ac.uk",develop a model/system for obfuscating natural language text,1,3   Hard,"developing a model/algorithm for obfuscation of text (in the context of syntactic parsing), conducting experiments testing the approach.",8,0,f
4398,Litter: A location based Twitter app,ug4,1324,2020 02 18 15:27:19 00,1,"In order to fully understand a short message (like a tweet), it is important to understand the context in which it was said.  One such context is time (when it was said, and the context of the discussion around it) and another is location (where it was said, and the local environment of the author).  A short message service such as Twitter records when a message was sent, but not where it was sent from.  The idea of this project is to create a short message service which records where a message came from, given that it was sent from a mobile device.  This would allow people to exchange messages which are sensitive to context, and which should only be read in context.  For example a user might post a message which says "";This is the best view of Edinburgh Castle""; and users would be able to read this message when they were near the original poster's location, or when viewing it on a map, so that they understand the location context in which the original post was made.We refer to these location based messages as "";litter""; in that a user can "";drop""; litter somewhere (when they post it) and a reader can "";pick up""; litter (when they read it).  Users can either drop the litter again (an upvote function) or "";bin""; it (a downvote function).  Litter which receives more downvotes than upvotes can at some point be removed by a moderator.The idea of the project is to implement an Android app which uses the user's location for posting and reading messages and which is backed by a Firebase database to store messages and broadcast them to other users.  Android apps can be programmed in Java or Kotlin (a modern strongly typed language which runs on the JVM).","To implement a mobile phone app which allows users to post and to read location based short messages, similar to tweets.",2,1   Easy,An Android application which connects to Firebase Cloud Firestore to synchronise data.,8,0,f
4399,Analysis of Differential Paths in ARX Ciphers Using ARXTools,ug4,6523,2020 02 19 09:28:34 00,1,"ARX ciphers are a class of symmetric key cryptographic algorithms based on the simple arithmetic operations addition modulo an integer, bitwise rotation and exclusive or. Example of such algorithms are the block ciphers TEA, XTEA and Speck, the stream ciphers Salsa20 and ChaCha and the hash functions Skein and Blake.The analysis of ARX algorithms is notoriously difficult due to intricate dependencies between the bits of consecutive operations. In a sequence of papers from 2012 and 2013, Leurent (see References) has conducted an extensive research of the dependency effect in ARX algorithms. As a result the author was able to find inconsistencies in several published differential paths in two popular algorithms    the hash functions Skein and Blake.Leurent's techniques have been automated in the form of the software tool ARXTools, freely available at (see References). In this project the student/s will apply ARXTools to check the consistency of existing differential paths on several ARX algorithms. Possible targets would be the block ciphers TEA and Speck and the stream ciphers Salsa20 and ChaCha.The project is suitable for more than one student, where each student will be analysing a separate algorithm.The project is suitable for Minf thesis.",Apply the software tool ARXTools to analyse ARX based cryptographic algorithms,2,4   Very Hard,"1. Compile ARXTools and get familiar with its use. 2. Take an existing differential path for a given cipher (e.g. Speck, Salsa20, TEA, etc.) and use ARXTools to analyse it for inconsistencies. 3. Report the results",8,0,f
4400,Classifying Mathematical Articles into Their Fields,ug4,1388,2020 02 19 10:58:30 00,1,"Mathematics is at the basis of most scientific disciplines, and papers in almost all areas of science include formulae, proofs or definitions for various mathematical constructs.Reading such papers can be sometimes complex, especially when these papers are affluent with mathematical notation. The goal of this project is to classify such mathematical content into a specific mathematical field.As such, the problem in this project can be formulated as a document classification problem. Given a mathematical article, classify it into one of many categories such as"";algebraic geometry,""; "";algebra,""; or "";real analysis."";Possible research questions:1. What is more important in such article classification, formulae or the text itself?2. Is there a need for deep structure parsing to classify an article into its field? Or is a shallow classification using bag of words sufficient?A dataset from arXiv will be provided for the goal of this classification.For more information on the motivation behind this problem and its importance, see the article by the Committee on Planning a Global Library of the Mathematical Sciences:Developing a 21st Century Global Library for Mathematics Researchhttps://arxiv.org/abs/1404.1905.The page for my research group (part of Edinburgh NLP) can be found here: http://bollin.inf.ed.ac.uk/.",building a classifier that classifies mathematical articles into their respective fields,2,1   Easy,development of a system that classifies mathematical articles into their field,8,0,f
4401,"Measure what a ""normal"" URL looks like for users",ug4,1351,2020 02 19 13:31:47 00,1,"When advising users how to identify malicious emails (phishing) one of the most common recommendations is to look at the URL to determine if it leads to phishing or not. However, many studies, including [1], show that users are very bad at this task. The question is what environmental factors might be contributing to this poor performance. In other words, humans generally learn to interact with the world through repeated day to day experiences rather than literally applying advice. For example, even though many web pages ask you to check "";I have read the Terms of Service"";, most people don't read them because they have learned that doing so is a waste of time [2] since they have checked this box many times and nothing bad has happened. In this project, we want to better understand what the day to day world of URL clicking looks like so we can better understand what a "";weird""; and "";normal""; URL looks like for people. The project The main goal of this project is to measure the URLs people see every day and create an understanding of what a "";normal""; URL looks like and how much "";normal""; varies between people. Obviously the easiest approach would be to build a plugin or proxy that just records all the URLs a person sees. The problem is that most people are not ok with someone else seeing their entire browsing history. And even if they were ok with it privacy wise, some URLs contain login credentials in the URL itself (think invite URLs) which are a security concern. A large component of the project will be deciding up front what information we want about the URLs and building a plugin that will record facts about the URLs rather than the raw URLs themselves. The project will also need to address the issue of user/participant trust. A participant in the study should be able to trivially view the data being collected about them and determine that the study is not damaging their privacy. Similarly, the project also needs to make sure that it collects the data most needed by scientists. [3] provides a fairly comprehensive list of facts that can be extracted about a URL itself. But other information may be needed, such as what type of page the URL was found on. URLs seen in email clients may be fundamentally different than URLs seen on news websites or coursework sites. RequirementsThe project involves working with very sensitive data in a respectful way. Most students should be able to learn the programming concepts necessary to complete the technical aspects of the project. However, I reserve the right to mark anyone unsuitable that in my opinion does not appreciate the sensitivity of the data.MeetingsI am holding virtual office hours every morning 10am 11am at the following link: https://eu.bbcollab.com/guest/bdc065c679e64974a94001f8262dd7d0",Build a browser plugin that records in a privacy sensitive way information about all the URLs a person sees during normal browsing.,1,Variable,Build a browser plugin that captures all the URLs people see in a privacy sensitive way. The capture needs to think about both user privacy as well as the needs of scientists. The student is expected to run the plugin on several people's browsers for at least a week.,8,0,f
4403,Beyond Species Classification: Re Identification of Individual Animals in Wildlife Images,ug4,6985,2020 02 19 15:18:14 00,1,"Ecologists and conservation biologists have begun to rely more and more on machine learning tools to process large quantities of image data. Existing solutions enable the automatic classification of an input image into one of several thousand different species of plants or animals e.g. [1]. However, a much more valuable piece of information required for monitoring wildlife populations is if a given image contains an individual animal they have seen previously. The problem now is not to determine the animal species, but to instead determine if it is an individual that has been seen before e.g. [2].  Large scale face identification of humans is a problem fraught with many ethical issues. In this project we will investigate related models, but instead apply them to the problem of animal re identification. The project will use existing animal datasets along with a novel dataset of individual dogs that is being collected by collaborators.  If you are interested in the project please send me an email (oisin.macaodha@ed.ac.uk) with [Project: Animal ID] in the title. Please also include a short text describing (i) why you are interested in the project, (ii) a few sentences regarding possible solutions that you might try, (iii) any relevant experience or projects you have done in the past, and (iv) your student number and any marks in related courses. Update:There will be an information session for this project on Monday 16th March 4 4:30pm and Tuesday 17th March 4 4:30pm in Informatics Forum 2.33. Both sessions will be the same, so you only need to attend one. This is your opportunity to ask questions about the project so please come prepared. I'm cancelling the face to face meetings and instead you can email me with questions (no later than Tues 17th Tues 24th at 5pm) and I will put the answers here:https://docs.google.com/presentation/d/1 bQ5QWyecfinaOyE0VA4jQ6 X28RGEsoThjDIWbOE0U/edit?usp=sharingI will mark students as suitable for the project by the end of the day on the 18th 25th. ",Develop deep learning solutions for re identification of individual animals in images,1,3   Hard,Develop and evaluate deep networks for individual animal re identification,8,0,t
4402,Efficient Deep Learning for Acoustic Biodiversity Monitoring,ug4,6985,2020 02 19 14:55:06 00,1,"There is a critical need for robust and accurate tools to scale up biodiversity monitoring and to manage the impact of anthropogenic change. The monitoring of different species and their population dynamics acts as an important indicator of ecosystem health. It can also provide valuable information relevant to food security as many species perform essential services such as pest control and pollination. Audio sensors are an important way to monitor species that use sound to navigate and communicate e.g. bats.The core of this project is to develop efficient supervised machine learning   algorithms that can detect the presence of different bat species in high frequency audio recordings. In real world deployment settings battery life and computational limitations are important concerns. As a result, the goal of this project is to develop efficient models that are accurate and power efficient. It will build upon an initial automatic solution that we have deployed in the Olympic Park in East London   https://naturesmartcities.com/.This cluster project provides capacity for three students with distinct projects related to:Investigating novel efficient deep learning architectures for audio classification and detection that can run on low power device (e.g. RaspberryPi).Designing learning based low computational cost audio event triggers that can be used as a first pass (i.e. filter) when processing streaming data.Deep convolutional neural networks are designed to be invariant to the location of objects in images. However, for audio we don't want frequency invariance, we just want time invariance. This is because different species vocalizations occur within specific frequency bands. The goal here is to develop and investigate new models that do not discard valuable frequency information.If you are interested in the project please send me an email (oisin.macaodha@ed.ac.uk) with [Project: Deep Acoustic Learning] in the title. Please also include a short text describing (i) why you are interested in the project and which of the three sub topics interests you, (ii) a few sentences regarding possible solutions that you might try, (iii) any relevant experience or projects you have done in the past, and (iv) your student number and any marks for relevant courses. Update:There will be an information session for this project on Monday 16th March 4:30 5pm and Tuesday 17th March 4:30 5pm in Informatics Forum 2.33. Both sessions will be the same, so you only need to attend one. This is your opportunity to ask questions about the project so please come prepared.  I'm cancelling the face to face meetings and instead you can email me with questions (no later than Tues 17th Tues 24th at 5pm) and I will put the answers here:https://docs.google.com/presentation/d/13wRq94w1YiTiZ7C52S4PH46m8m10cEFgI8bbEnj5g90/edit?usp=sharingYou will also find a link to a presentation at that address (first link on slide 3) that gives more background to the project. I will mark students as suitable for the project by the end of the day on the 18th 25th. ",Designing and implementing efficient machine learning algorithms for acoustic wildlife monitoring,3,3   Hard,Develop and evaluate audio classification models on existing benchmark audio data.,8,0,t
4405,Reimplement abstract game algorithm in Java,ug4,1322,2020 02 19 20:21:26 00,1,"Twenty years ago I developed (based on earlier work by and with Colin Stirling) a rather general algorithm for searching for winning strategies in formal games, where the game might involve infinite numbers of positions which, however, could be explored by lumping together positions that could be treated in the same way. For example, if the current game position involves an integer, but all that matters for determining the next move is whether the integer is positive or negative, the algorithm splits the current game position into two, one for positive and one for negative integers, and explores them separately. In certain cases, it is possible to analyse such a game completely in finitely many steps, even though there is an infinite number of positions, because they fall neatly into finitely many cases. This has applications in verification. I described the paper in [1] and implemented it in Standard ML as part of the Edinburgh Concurrency Workbench (main file: [2]).The Edinburgh Concurrency Workbench is no longer under active development and Standard ML is no longer a good choice of language; however, the algorithm might still be of interest, if it were freed from the legacy software it is currently embedded in. The aim of this project is to reimplement it in Java. The project is Easy in the sense that this could   and initially, should   be done in a naive way, by translating the ML as directly as possible into Java, writing unit tests as you go. (Standard ML is close enough to Haskell that knowledge of Haskell will enable you to read the source code.) Understanding the proof of correctness of the algorithm is not required to begin with (though one might hope it would emerge over the course of the project.) There is then scope for adding challenge by carefully refactoring the Java into clean, clear, idiomatic, maintainable code in the process, adding well chosen tests to maximise confidence in its correctnessinvestigating its properties and performance  perhaps (getting challenging) writing a more easily understandable proof of correctness based on the new structureeven, improving the algorithm.",Take an existing algorithm implemented in Standard ML and reimplement it in Java,1,1   Easy,An implementation of the abstract game algorithm in Java.,8,0,f
4406,Pedagogically sensible lab sessions for Software Design and Modelling,ug4,1322,2020 02 19 20:26:00 00,1,"In the lab sessions of course Software Design and Modelling, students work with a variety of model driven development tools. Designing and running such a session is a challenge in itself, because:the question of how to teach and learn MDD is itself a topic of research on which there are different opinions; see various papers in the Educators' Symposia associated with the MODELS conference, see refs;many of the most interesting tools we might want to use are not very intuitive, nor always maturestudents will need feedback on whether what they have produced is correctthere could be up to 100 students and only one member of staff, so such feedback, and most help, must be available on a self service basis.Each session should have defined learning objectives and a justification as to why the proposed activities are a sensible way for students to attain them. Depending on the work to be done, it may be necessary to provide a textual description of the tasks, a justified choice of tooling to be used, links to external documentation, inputs to model transformations, partially completed artefacts such as models, transformations or templates, automated tests, etc.The lab sessions used so far are linked from the schedule page for SDM [5]. Note, however, that these lab sessions are minimalistic: it would be better to have a lot more support for students to test their own work than these have, for example.The topics can be negotiated and students' own ideas would be welcome, but might include:model to model transformation using ATLtriple graph grammars using eMOFLONimprovements to existing labs, such as more precise questions with automated self checks.This could be made an MInf project, provided the student was seriously interested in pedagogy and its evaluation   in that case I envisage that the first year would focus on developing one or more lab sessions (as for the one year version) which could then be trialled and improved in the SDM course in semester 1 of 2021/22: such work, done to high quality, could be publishable. Of course that specific plan would depend to some extent on SDM being run, by Stevens or someone else willing to accommodate the research element, in 2021/212, which formally is something that cannot be guaranteed, so it would be necessary to have a plan B.NB to take this project, you must already have achieved well in SDM: planning to take it in 2020 will not suffice, since the material you would need to support is in the second half of SDM.","Drawing on available research concerning how to teach software modelling and model driven development, design one or more challenging yet accessible two hour lab sessions for the course Software Design and Modelling, along with all necessary artefacts. Consider carefully how to ensure that what is provided is bug free, how to help students who get stuck, and how students' own work can be efficiently checked.",4,2   Moderate,At least one usable lab session.,8,0,f
4407,MegamodelBuild,ug4,1322,2020 02 19 20:30:36 00,1,"I started the development of the MegamodelBuild project [1], making use of the pluto build system [2] , to illustrate ideas presented in the MODELS'18 paper Towards sound, optimal, and flexible building from megamodels [3]. The situation addressed by the paper is: software development involves multiple models (a very general term for any representation of some of the information about the software, e.g. a UML model, a Java file, an XML configuration file etc.), and there is a network of (usually binary) consistency relations between them which must, from time to time, be enforced (e.g. class names in a UML model might be updated with respect to some Java code). Many technologies exist for restoring consistency along a single edge in the network   compilers and code generators are examples, as are model transformation engines, both unidirectional and bidirectional. However, the result of restoring consistency in the whole network depends on the order and direction in which these are used, as well as on their individual behaviour. The paper shows how to adapt the pluto build system to manage this, using an orientation model to describe current requirements concerning, for example, which models are to be taken as authoritative and not modified in the consistency restoration process.However, while pluto is a fully working system, MegamodelBuild is currently just a proof of concept. Students taking this project will contribute to making MegamodelBuild practically usable. For example, you might contribute to one or more of:integrating facilities for handling EMF based models (rather than the dummy text based models the tool uses at present)making a Builder component wrapping one particular technology (e.g. a model to model transformation engine such as ATL, a model to text e.g. document generation tool, or perhaps one of the view managemetn tools discussed in [4])designing and supporting a suitable syntax for orientation models (to represent the placeholder plain text format currently used) and providing means to manipulate, visualise and validate theminvestigating possible integration between MegamodelBuild and modelling frameworks such as EMF in Eclipse or MDEForge [5].Engineering quality, including good documentation, will be important in this project. I have rated it as Hard not because I expect the programming involved to be especially challenging but because experience shows that it is challenging to get to grips with the ideas of model driven development if they are new to you, and with the complex landscape of research tools and software. ","Contribute to the open source MegamodelBuild project for managing the maintenance of consistency between models during software development, for example, by implementing model management features and/or a Builder component for a transformation language",4,3   Hard,"A definite, well engineered, documented contribution to the MegamodelBuild tool has been made.",8,0,f
4408,A web interface for PMark,ug4,1340,2020 02 20 11:29:49 00,1,"PMark is an experimental tool for computing assignment grades using criteria based marking. This currently runs on Linux with a command line interface. The aim of this project is to create a web based interface which will allow the tool to be more easily evaluated by other users. The tool itself is written in Perl, but a web interface could interface directly with the Perl code, or communicate via the simple command line and CSV interface, so there are several options for an implementation technology and language. If more than one student is assigned this project, they must be prepared to negotiate the use of different technologies to differentiate the work.",Create a web based interface for the PMark marking program,2,1   Easy,"A working, evaluated web based interface for PMark.",8,0,f
4410,Alternative implementations of PMark,ug4,1340,2020 02 20 11:44:21 00,1,"PMark is an experimental tool for computing assignment grades using criteria based marking. PMark is currently written in Perl. The aim of this project is to create an alternative implementation of the core algorithm (in some other language) which can be used to validate the specification and the existing implementation. The use of a functional language would be particularly interesting. The intention would be to generate sample data and compare the results from multiple implementations. If more than one student is assigned this project, they must be prepared to negotiate the use of different languages to differentiate the work.",A working implementation of PMark in some other language,2,2   Moderate,A working PMark implementation,8,0,f
4411,Analysing and simplifying chord progressions,ug4,1340,2020 02 20 12:08:36 00,1,"Jazz standards based on functional harmony usually contain harmonic embellishments of the basic chord progression. The aim of this project is to write code which will accept a chord progression and attempt to determine the structurally important chords, and generate a simplified harmony.",A program which will accept a chord progression and generate a simplified version,2,2   Moderate,A program which will accept a chord progression and generate a simplified version,8,0,f
4413,Graphical animations of some concepts in number theory,ug4,1328,2020 02 20 15:33:57 00,1,"Many beautiful proofs in mathematics   particularly number theory   are quite hard to grasp, but would become much more accessible with the help of a  nice interactive  graphical tool for visualizing some of the main ideas. Examples include Gauss's theory of primitive roots and its development in the theory of Dirichlet characters, which plays a key role in the proof that there are infinitely many primes in the arithmetic progression a, a b, a 2b,  ¦ whenever a and b are coprime. The purpose of the project is to explore some of these possibilities, and to develop such a tool for whatever area of number theory seems most promising.The first phase of the project will be to create a simple graphical calculator for modulo arithmetic, which can already be used to illustrate concepts such as the Chinese remainder theorem and primitive roots. Thereafter, the development of the project can be fairly open ended, and can be tailored to the student's tastes and interests.  It would be possible for several students to work on different incarnations of this project, people with some level of  collaboration between them. The project  would be  ideally suited to CS/Maths students; others are welcome to apply if they have reasonable mathematical credentials.",To design and implement an educational tool for making some concepts from number theory readily visualizable,2,Variable,The completion of a graphical tool that demonstrably assists in the understanding of some non trivial concept in number theory.,8,0,f
4414,Mandelbrot Maps: Web application for exploring fractals,ug4,1380,2020 02 20 15:41:27 00,1,"The Mandelbrot curve is a dense, interesting shape.  Zoom in on any portion of it, and you see a different dense, interesting shape; or sometimes a tiny almost duplicate of the original shape, nested inside the original.  This process can be continued forever.The Mandelbrot curve indexes the Julia curves.  Each point on the complex plane used to display the Mandelbrot curve generates a corresponding Julia curve.  Astonishingly, the Julia curve for a given point usually resembles the appearance of the Mandelbrot curve near that point.Mandelbrot Maps, written in 2008 by MSc student Iain Parris and updated in 2009  2012 by MSc and UG4 students Sky Welch, Hannah Taub, Alasdair Corbett, Edward Mallia, Taige Liu, Charlie Wu and is a web application that lets one explore such shapes (and beats any similar application on the web or in the Play store). Recent versions includeFractal Mapsavailable for Android from Google Play, andhttps://jmaio.github.io/mandelbrot maps/mmaps.freddiejbawden.comon the web. This project is to improve the existing application.  Possible areas of improvement include: Improved presentation of Tan's theorem (see below). Improve the Android implementation, or reimplement for iPhone. See how high a rating you can get in the Android or Apple store. Make educational videos using the current system. See how many hits you can get on You Tube. More robust web engineering. Port it to display on Microsoft Surface, Puffersphere, or other unique device.Exploit multi core architecture to make it run faster. Exploit multiple computers over the web to make it run faster. Port from Java to Flash or Javascript, or exploit other web technologies.Adapt the application  for use in public engagement, such as the Science Festival, or student recruiting.The system can be improved to better illustrate Tan's theorem relating Mandelbrot and Julia curves, which states that the Mandelbrot curve and the corresponding Julia curve at the same point look almost identical when suitably magnified and rotated; this requires an improved user interface that supports the necessary magnification and rotation.The project involves opportunities to develop skills at mobile computing, touch interfaces, web programming, GPU programming, web assembly (WASM), multicore programming, and distributed programming, all of which will be increasingly important in the future.","Improve the existing Mandelbrot Maps application for exploring Mandelbrot and Julia curves, available through Google Play for Android.",2,2   Moderate,An implementation of Mandelbrot Maps incorporating Tan's theorem or one of the other improvements described above.,8,0,f
4415,Pretty Printing for BNFC meta,ug4,1380,2020 02 20 15:43:12 00,1,"BNFC meta is a fantastic Haskell library written by Duregard andJansson.  It exploits Template Haskell to embed BNF Convertergrammars, parsers, and printers in Haskell.  The purpose of thisproject is to extend BNFC meta so that in addition to parsers (whichconvert strings to internal trees) and printers (which convertinternal trees to strings) it also generates pretty printers (whichconvert internal trees to strings with layout and indentation thatmakes them easier to read).  The project will build on both BNFC metaand Wadler's pretty printing library.  The result will be to extenda widely used libary for Haskell to make it even more powerful,a benefit to the world and a nice addition to the student's resume.",Extend  BNFC meta to support not only generation of parsers and printers but also of pretty printers.,1,3   Hard,"An updated, tested version of BNFC meta, that can be installed with Hackage and Cabal.",8,0,f
4416,System F in Polymorphic Blocks,ug4,1380,2020 02 20 15:44:23 00,1,""";Polymorphic Blocks are a novel block based UI, in which a connector's shape visually represents the structure of the data being passed through the connector. We use Polymorphic Blocks to add visual type information to block based programming environments like Blockly or Scratch.""; (Quoted from here.) Polymorphic Blocks are implemented on top of Blockly, a JavaScript library for building user interfaces that manipulate graphical programs, created by Google.The goal of this project is to apply Polymorphic Blocks to implement System F, the polymorphic lambda calculus of Girard and Reynolds, the system at the core of programming languages including Haskell, ML, and Scala. Success would show that graphic languages such as Blockly or Scratch can represent the full power of polymorphic programminglanguages.A recent research attempt involving graphical representation of polymorphism, with some spectacular visuals, is here. It goes by the splendid name maramification.",Implement System F in Polymorphic Blocks,2,Variable,Demonstrated and tested implementation of System F in Polymorphic Blockly.,8,0,f
4417,Haskell Notebooks: applying Jupyter/IHaskell,ug4,1380,2020 02 20 15:50:46 00,1,"Notebooks provide a popular web based interface to a programminglanguage.  Originally introduced for Mathematica, they are now availablefor a range of languages including Python and Haskell.The goal of this project is to improve Haskell support for IHaskellnotebooks by adding new graphic datatypes to Haskell, similarto those found in Dr Racket and/or by exploiting Jupyter Notebooksto support graphics based learning materials for Haskell.",Exploit Jupyter Notebooks/IHaskell to provide learning materials with a graphics emphasis,2,Variable,"A library for Haskell to support support new graphics datatypes for Jupyter/IHaskell, available via Hackage and Cabal; or graphics based learning materials based on Jupyter Notebooks.",8,0,f
4418,Towards a Closed Formula for the Additive Differential Probability of XOR,ug4,6523,2020 02 20 15:54:35 00,1,"Let $x,y,a,b,c$ be $n$ bit integers. We are interested in finding the number of solutions $(x,y)$ to the equation $(x^y)   ((x   a) ^ (y   b)) = c$ for arbitrary but fixed $a,b,c$, where $^$ denotes the binary exclusive or (XOR) operation. This problem has applications in cryptography for computing a quantity known as the additive differential probability of addition (ADP XOR). The corresponding dual problem is to find the number of solutions $(x,y)$ to the equation $(x y) ^ ((x^a)   (y^b)) = c$. This is related to the computation of the XOR differential probability of addition (XDP ADD).The computation of both ADP XOR and XDP ADD has been extensively studied in the literature [1 5]. In [ 2] has been proposed a closed formula for the computation of XDP ADD, while the most efficient method to compute ADP XOR is by the algorithm proposed in [3,5]. This algorithm is based on matrix multiplications and is linear in the word size $n$. There is no known formula for the computation of ADP XOR.In this project the student will research the problem of deriving a closed formula for the computation of ADP XOR. The starting point of the work will be the S functions framework proposed in [ 5] and the finite state machine (FSM) described in [ 3].",Research the problem of deriving a closed formula for the computation of the additive differential probability of the exclusive OR (XOR) operation with applications in cryptography and cryptanalysis,1,3   Hard,The minimum completion criteria would be: Implement the algorithm for computing ADP XOR described in [ 5] Study and implement the FSM described in [ 3] Use the FSM as a starting point for closed formula derivation,8,0,f
4419,Implementation of an Algorithm for Optimal Probability Computation Using Transition Probability Matrices,ug4,6523,2020 02 20 16:02:23 00,1,"Consider the following problem:Given are $k$ square matrices $A_0,A_1,\ldots,A_{k 1}$ of dimensions $n \times n$ with elements in the interval $\{0,1\}$, a column vector $C = (1,0,0,\ldots,0)$ of dimension $n\times 1$, a row vector $L = (1,1,1\ldots,1)$ of dimension $1\times n$ and an integer $N &gt; 0$. Find a sequence of matrices $M_{N 1}, M_{N 2}\ldots, M_{1}, M_{0}$, where $M_i \in $\{A_0,A_1,\ldots,A_{k 1}\}$, $0 \le i &lt; N$ such that the product $L M_{N 1} M_{N 2}\ldots M_{1} M_{0} C$ is maximized. Typical values for the parameters are: $k=8$, $n \in\{2,4,8,16\}$, $N = 32$.The described problem is encountered in the areas of cryptography and cryptanalysis for computing the probability of certain events and has been studied before. In particular, in reference [ 1] is proposed an algorithm that computes a solution using a dynamic programming approach. The algorithm runs in worst case exponential time, although detailed analysis of its complexity is not provided.In this project the student will implement the algorithm from [ 1] and will analyse its complexity both theoretically and experimentally. The student will investigate the "";worst case inputs""; to the algorithm and will research possibilities for improvement.","Implement the algorithm proposed in reference [1], estimate its complexity theoretically and experimentally and research possible improvements",1,2   Moderate,"Minimum completion criteria: Implement the algorithm, Analyse the complexity theoretically, Analyse the complexity experimentally, Analyse the complexity experimentally. Optional: Research possible improvements",8,0,f
4420,Pretty printing for first year students,ug4,1380,2020 02 20 16:04:07 00,1,"A previous ug4 project by Razvan Ranca has resulted in a sophisticated pretty printing library for Haskell. It is described here:https://hackage.haskell.org/package/GenericPrettyThe goal of this project is to put this library to work improving the life of first year students. That is, to work out how to integrate the pretty printer so that it is easy for first year students in Introduction to Computation when writing and debugging Haskell programs. The goal will be to design a new variant of the standard prelude for Haskell that directly supports pretty printing, and make it easy for first year students to adopt. The same technique might also be applied to automatically creating generators for QuickCheck, making it easier for first year students to test their work.",Adapt an existing generic pretty printer for use in the first year functional programming class,2,3   Hard,"A variant of the standard prelude that supplies automatic pretty printing support, suitable for use by first year students, together with suitable learning materials.",8,0,f
4421,Bringing the stupid computer to life,ug4,1380,2020 02 20 16:18:06 00,1,"First year students at Edinburgh are taught to mimic how a "";stupid computer""; executes their programs, by writing out a sequence of equations. For instance, if given the definition of the fold function,      fold :: (a  &gt; b  &gt; b)  &gt; b  &gt; [a]  &gt; b      fold f e [] = e      fold f e (x : xs) = f x (fold f e xs)they could simulate it's execution on a list as follows:            fold ( ) 0 [1, 2, 3]      =            1   fold ( ) 0 [2 ,3]      =            1   (2   fold ( ) 0 [3])      =            1   (2   (3   fold ( ) 0 []))      =            1   (2   (3   0))      =            6The purpose of this project is to produce a software tool that will accept a Haskell program and a term as input, and produce a trace as output. For instance, the above trace would be produced if given the three lines defining fold above and the term "";fold ( ) 0 [1,2,3]"";.Existing Haskell tools can be exploited, so that building such a simulator for equational reasoning should not be too difficult. It's existence will be a huge boon for students at Edinburgh and elsewhere.","Build a step by step execution engine for Haskell, to aid first year students",2,3   Hard,"A simulator for Haskell, that takes a Haskell program and term as input and produces as output a sequence of equal terms simulating its execution.",8,0,f
4425,Access constraints discovery,ug4,5654,2020 02 20 21:00:51 00,1,"Background:  Bounded evaluation has been proposed to answer queries by accessing a bounded amount of data when the queries are boundedly  evaluable [1,2]. This is made possible by providing DBMS with a new data access method that fetches values instead of tuples or  columns [3]. More specifically, the access method, denoted by  \fetch, is guided by a notion called access constraint \psi =  R(X &gt;Y, N), such that given any X value \bar a, \fetch(\bar a, \psi) returns all distinct Y values associated with \bar a by accessing  no more than N tuples.To make practical use of bounded evaluation, i.e., query evaluation using  \fetch, it is important to identify a suitable set of access constraints. Such access constraints could be either data independent (N is a constant) or data dependent (N changes when D becomes larger).  We "";define""; the notion of  data independence \wrt a given big dataset D. More specifically,  an access constraint \psi = R(X &gt;Y, N) is data independent on an instance D of relation R if for any reasonably large samples of  D, for each X value in the samples, there are a stable number of associated Y values that is smaller than N.  Main Problem1:  With this empirical definition of data independence access constraints, we are able to define the following problem:Input: A database schema \R, a database \D of \R.Output: All data independent access constraints over \R for \D.Main Problem2: Data dependent, query driven access constraints selection:Input: A database schema \R, a database \D of \R, a set \Q of queries over \R, a natural number NOutput: a set \A of access constraints over \R such that the size of the indices of access constraints in \A over \D does not exceed N.Objective: maximize the number of queries that are boundedly evaluable under \AProposed Approach:  For problem1, we will be extending an algorithm for discovering functional dependencies for discovering access constraints. A functional dependency is of the form R(X &gt;Y) and it holds on an instance D of R if for each X value there exists a unique Y value in D. By relaxing this "";uniqueness""; of the associated Y values, we are expected to discover data independent access constraints for \D as well, with minor extension to the algorithm. There has been a host of work on discovering functional dependencies (see [4] for a survey).For problem2, we will formulate the problem as an ILP and use an ILP solver to compute the optimal solution.  Assessment:  This project can support up to 2 ug4 projects.  To complete this project, the students will need to do the following:  understand the big picture of bounded evaluation and the position of access constraints  study one of the two main problems:For problem 1, do the following:          analyze functional dependency discovering algorithms, assess their suitability for extending to discover access constraints, and extend one of them for discovering access constraints       implement the extended algorithm       carry out an experimental study of the algorithmFor problem 2, do the following:     formulate and analyze   the optimization problem     reduce to ILP and prove the correctness     use an ILP solver to solve the problem.  ",Study one of the two main problems in the project description.,2,Variable,"Assessment: This project can support up to 2 ug4 projects. To complete this project, the students will need to do the following: understand the big picture of bounded evaluation and the position of access constraints. study one of the two main problems: For problem 1, do the following: analyze functional dependency discovering algorithms, assess their suitability for extending to discover access constraints, and extend one of them for discovering access constraints. Implement the extended algorithm, carry out an experimental study of the algorithm. For problem 2, do the following: formulate and analyze  the optimization problem, reduce to ILP and prove the correctness, use an ILP solver to solve the problem.",8,0,f
4426,Online relation caching for SparkSQL,ug4,5654,2020 02 20 21:52:05 00,1,"This project studies the effectiveness of various online caching algorithms for SQL query answering with SparkSQL.The cost of SQL processing with SparkSQL with relations stored in either a NoSQL database or HDFS is typically dominated by the cost of fetching the relations into Spark's runtime memory, which is a common performance bottleneck for most of SQL over NoSQL systems [1].  To alleviate this, one approach is to cache fetched data in SparkSQL's memory cache for future queries since SQL over NoSQL systems are widely used to handle large valumes of queries (i.e., high throughput). There are a number of online caching algorithms in place, which are designed for caching (paging) in operation systems. It is, however, unknown whether they are effective for SQL processing. This project aims to evaluate the effectiveness of these online caching algorithms for SQL processing. As an optional challenge, we can also study relation caching for a given sequential query workload.",Evaluate the effectiveness of online caching algorithms for SQL processing with Spark,1,2   Moderate,"Implement online caching algorithms for SQL processing over SparkSQL, using the built in caching functionality of Spark. Evaluate the effectiveness of these methods for various SQL query workloads with SparkSQL.",8,0,f
4412,Generalization in Reinforcement Learning,ug4,7966,2020 02 20 13:18:30 00,1,"The project is about Generalization of Reinforcement Learning. That is, the measurement of how robust an agent is at solving problems beyond the specific platforms it had been trained on. To achieve generalization, an agent must realize which aspect of the training data teaches a relevant robust skill of the game that should be learned and focused on, and which is an approximate memorization of specific trajectories. Using  Openai's gym,  an agent's generalization can be measured and tackled directly.Using a game in Openai's gym environment as dataset. I plan to look into different methodologies intended to reduce overfitting, such as: lower number of nodes, higher regularization, bagging, data augmentation and correct algorithm architecture.  Results would be evaluated on an unseen set of randomized levels for a specific game. The model will be trained a different set of randomized levels and number of randomized training levels will be limited and ensure it generalizes. This way, the agent will be measured by how much it generalizes to new data rather than potentially overfit to the level it was trained.The expected outcome is a model that effectively manages to solve unseen levels in the game.","Build a Reinforcement Learning agent to solve levels of games it had not seen, by training on a limited set of different levels in the same openai's gym game",1,3   Hard,A successful implementation of the simulated agent should be able to solve previously unseen Openai's Gym game levels after it was trained on a limited number of different levels.,8,0,f
4427,Implementing and Evaluating Algorithms for Constraint Shortest Paths,ug4,5654,2020 02 20 22:53:18 00,1,"This project aims to implement and evaluate algorithms for the constrained shortest path problem [1], which has wide applications in, e.g., route planning. Given a graph G and two nodes u and v of G, where each edge of G has a length and a cost, the constraint shortest path problem is to compute a path p from u to v such that (1) the total cost of edges in p is no larger than a given number C and (2) the length of p is minimum among all such paths.  There are a number of algorithms, such as [1][2][3]. This project is to implement these algorithms and evaluate their efficiency empirically and analytically. As an optional exercise, we can also study and develop parallel algorithms for the problem (e.g., by parallelizing the algorithm of [1]) and study its parallel scalability.                                                                                  ",Implementing and Evaluating Algorithms for Constraint Shortest Paths,1,3   Hard,"implement algorithms for the constrained shortest path problem, evaluate the algorithms empirically and analytically, (optional) develop parallel algorithms for the problem",8,0,f
4429,A workbench for element theory phonology,ug4,1395,2020 02 21 08:45:17 00,1,"Government phonology, and more generally element theory, is a theory in which the units of speech sounds are "";elements""; representing primitive aspects (such as A for low vowels, I for palatality, etc.), which are combined in various ways to produce the sounds of speech. GP is particularly associated with autosegmental phonology, in which different aspects are represented on parallel "";tiers"";. There are many variants and new proposals for the details, and (as often in phonology) examples are worked by hand, with the consequent unreliability.The aim of this project is to provide a workbench so the researcher can specify a theory, implement examples, and manipulate them according to the rules of the specified theory (and incidentally provide nice printable output).Construction of such a tool forms a UG4 project; the project could be extended to MInf by broadening the class of theories included and using to investigate claims made in the literature. (Such an MInf would require a strong linguistics background, including at least all the natural language courses in the MInf.)",To provide a tool for implementing and evaluating analyses in element theoretic phonological theories.,1,2   Moderate,Pass: implementing in a tool the theory of Harris (1994). Merit: a more general tool allowing several different theories.,8,0,f
4430,An integrated C/Lisp Emacs debugger,ug4,1395,2020 02 21 08:46:18 00,1,"Emacs comprises a core written in C, including a Lisp engine, together with a large suite of Lisp libraries. Most code written for Emacs is written in Lisp, as any user can load Lisp into an Emacs. There are two debugging facilities for Lisp   a simple debugger, which requires no special action to use, but does not provide source code viewing; and the edebug Lisp library, which requires that code be instrumented, and then gives source level control of debugging. However, neither system works perfectly when writing Lisp code that operates with low level operations such as keyboard input    as the debugger runs in the same Emacs as the debugged code, it is impossible to completely avoid interference. Moreover, many low level operations are part of the C core, not in Lisp. The aim of the project is to extend the internal debugging interface and combine it with the use of GDB, to produce a debugger that works externally (like GDB), gives source level control of debugging at both C and Lisp levels, and switches automatically between them. Emacs is a large and complex code base, so this project, although more an engineering challenge than a conceptual challenge, is suitable only for a student with experience of large programs in C, and preferably also with Emacs Lisp development.",To provide a system for debugging Emacs libraries non invasively and with seamless integration of the C and Lisp levels.,2,2   Moderate,Full completion is the implementation of a system as described above. Partial completion would be non invasive Lisp level debugging.,8,0,f
4431,Secure file restrictions for general applications,ug4,1395,2020 02 21 08:47:10 00,1,"For everyday security requirements, filesystems such as AFS (used in DICE) or various encrypted filesystems are generally considered good enough.However, many applications (such as text editors like Emacs, or web browsers) save information, possibly including sensitive information (e.g. autosave files) in other places than the current directory. While it may be possible to configure them not to do so, this is not always easy, and requires individual knowledge of the application.The aim of this project is to exploit the Unix/Linux LD_PRELOAD facility to monitor all file accesses by an application, and take appropriate action if data is written outside a protected area. Actions might be simply to forbid it (probably resulting in an application crash), or to request permission from the user, or to redirect the access to a safe area.A basic version of the project would do just that; a more sophisticated version would have a well defined security model, allow user specification and exceptions, and have a careful analysis of the security provided, as well as evaluating usability.",To provide a method of preventing applications from writing secure data outside protected filespace,2,1   Easy,"For pass level, achieving the goal of preventing or flagging unsafe file access by applications outside a single specificed protected area.",8,0,f
4432,Tracking beak and foot movements for weaver bird nest building,ug4,1341,2020 02 21 09:58:24 00,1,"Weaver birds build complex nests from grass strands, in several stages, each of which requires complex manipulation and control [1]. They first attach or knot some grass to a branch, then add pieces to form a ring, and then weave strands into the structure to create a hanging basket for their eggs. To better understand how they do this, we would like to track, using computer vision, the position of the beak and feet which manipulate the grass in videos of nest building behaviour [2]. This poses several interesting issues for visual tracking, including segmenting the relevant parts of the bird, dealing with occlusions, fitting 3D models, etc. If the project goes well, it could be extended into automated classification of the behaviours.",To automatically track the position of the beak and feet in videos of birds that are constructing nests,1,4   Very Hard,Tracking software able to locate beak and feet of birds in video.,8,0,f
4433,Estimate robot ego motion with an event based camera,ug4,1341,2020 02 21 10:00:01 00,1,"Unlike frame based video cameras which compose a video by taking snapshots of the whole frame at a fixed frequency, event based cameras asynchronously report pixels that have brightness changes (see this video:  https://www.youtube.com/watch?v=LauQ6LWTkxM&amp;feature=youtu.be&amp;t=30). Based on such features, event based cameras have great potential for fast and low power vision algorithms for robots. The task of this project is to  look into  visual motion  perception  models and estimate the ego motion of a moving   camera   on a robot base[1].  Possible extensions include:   Increase the complexity of the environment.    Add moving objects in the test environment and detect the ego motion without being affected.  The choice of visual motion perception pattern is an open question, but using a spiking neural network (SNN) with fast processing speed would be of particular interest. ","Moving an event based  camera in a static environment, estimate the ego motion of the camera based on the visual motion information extracted from the camera input.",1,3   Hard,Reproduce the ego motion of a moving event based camera with decent robustness and accuracy.,8,0,f
4434,Online Inverse Dynamics Learning using Sparse Gaussian Processes,ug4,7109,2020 02 21 10:01:35 00,1,"Supervisors: Gabriella Pizzuto and Michael MistryRobot control is improved with a more accurate dynamics model, in a way that known forces are compensated through feedforward control. Obtaining a precise model of a robot's dynamics is vital for compliant low gain control and improved human robot interaction. As a result of unmodelled non linearities such as sensory noise and friction, obtaining an adequate dynamics model analytically is a non trivial challenge.A recurring technique that has been used in this field is that of Gaussian Process Regression (GPR); however, GPR suffer from heavy computational requirements, and thus have been limited mainly to offline learning. In this project, our focus will be on studying how sparse GPs can be used in this field.",To develop an online inverse dynamics learning approach using sparse Gaussian Processes for robotic manipulators.,1,Variable,"The student is expected to: (1) develop a sparse GPR framework to learn the inverse dynamics model of a robotic manipulator; (2) Evaluate the model on data recorded from a real robot manipulator; (3) compare its performance to using GPR or a similar technique, in terms of computation time and tracking performance.",8,0,f
4436,Building a hexapod robot platform to test insect inspired navigation algorithms,ug4,1341,2020 02 21 10:06:15 00,1,"Navigating robots are usually wheeled for simplicity, which is fine for flat terrains but not very useful in rough ones. This project aims to use a six legged robot from a kit (T Hex [1]). An example where the same robot is used can be found in [2], additionally equipped with a 360 ° panoramic camera, able to move omnidirectionally and apply standard navigation algorithms using vision, an Inertial Measurement Unit (IMU) and proprioception (motor encoders). The robot will be used as a base in order to test insect inspired navigation strategies [3] and compare their performance to standard navigation algorithms.Possible extensions:The robot will be able to count its steps to estimate the distance travelled, as has been shown for ants [4].Use an insect brain inspired network for visual navigation [3].Simultaneous Localisation and Mapping (SLAM) using a panoramic camera [5].",To have a hexapod robot walking with standard gaits on uneven terrain and collecting data for navigation,1,Variable,"Minimum is that data collected from the six legged robot base is used off line to test navigation algorithms. Ideally, the six legged robot is able to navigate.",8,0,f
4437,Analysis of the DAPHNE dataset,ug4,6878,2020 02 21 10:07:22 00,1,"[It has been agreed that this self proposed MInf project will be  supervised by Professor D K Arvind]Delhi Air Pollution: Health and Effects (DAPHNE) is a five year project (2017 22) funded by the UK Medical Research Council and the Natural Environment Research Council to investigate health effects of air pollution in the early years. There are two arms to the study: (i) effect on the respiratory health in the first two years of the offspring due to exposure of the mother during pregnancy; (ii) effect on exacerbation in young asthmatics. This project will concentrate on the latter arm of the DAPHNE project. All 272 subjects wear two sensors: a personal exposure monitor on the belt and a patch on the chest which measures the respiratory rate/flow, for 48 hours in three cycles over a 12 month period. The sensor data is time  and location stamped and collected on the mobile phone for onward transmission to the GoogleCloud repository. In addition, blood/urine samples are collected from the subjects to detect bio markers which indicate oxidative stress due to pollution exposure, and the subjects maintain a diary of their condition during the 12 months of data collection, both during and in between sensor data and biomarker collection.This project will use a combination of statistical techniques and novel machine learning methods to investigate associations between air pollution exposure and the respiratory vital signs in young asthmatics. Of particular interest would be to develop personalised exposure response relationships which can be used to predict responses in locations based on historical GPS tagged pollution concentrations. There will be opportunity to exploit contextual information based on the type and intensity of the subject's activity which can be derived from the sensor patch.","Spatio temporal analysis of the DAPHNE dataset to investigate associations between personal exposure and respiratory rate/flow sensor data, biomarker, diary entries and clinical notes on the health outcome of asthmatic adolescents.",1,Variable,(i) Implementation of machine learning methods to investigate associations between personal exposure and changes in breathing rate/flow in young asthmatics. (ii) Implementation of a method to derive personalised exposure response relationships for young asthmatics. (iii) Implementation of a method to predict responses to locations based on historical concentration levels and the personal exposure response relationships.,8,0,t
4438,A neural circuit to explain the bee dance,ug4,1341,2020 02 21 10:11:42 00,1,"Bees 'dance' for each other in the hive to communicate information about food sources they have discovered in the world. A bee observing the dance can decode the direction (relative to the sun) and distance it needs to fly to find the food. This behaviour has long fascinated biologists but till today there has been no explanation of what is happening in the bee's brain when dancing, or when watching a dance [1]. We have recently published a computational model of the neural circuit in the bee's brain that allows it to store direction and distance information [2]. This project would extend the model to explain 1) how this circuit could control the dance behaviour of the 'teacher' bee and 2) how a 'learner' bee might sense the information in the dance and transfer it into its own brain circuit. Although the work would initially be done in simulation,   a possible extension would be to implement the neural circuit on robots that would perform the bee dance.",Extend an existing model of a central brain circuit of the bee to test if it can explain how bees learn food locations from each other.,1,1   Easy,Extend the existing computational model to control a simulated bee dance. Simulate a learner bee that can acquire direction and distance information from the teacher bee,8,0,f
4439,Translations between relational algebra and relational calculus,ug4,2060,2020 02 21 10:15:24 00,1,"The equivalence between relational calculus (RC) and relational algebra (RA) is one the fundamental theorems of database theory. Formal translations [1] between these languages are taught in introductory database courses, such as Database Systems (INFR10070) at this university. The goal of this project is to implement these translations as a tool that can be used by students to support their learning.The tool will run as a web application with a responsive GUI that, in addition to correctly translate RA expressions to equivalent RC expressions and vice versa, will  offer  the following features:  Visualisation of the syntax tree of the expression being translated, showing the corresponding translation of each subexpression when requested (e.g., by selecting it or by hovering over it with the mouse).Random generation of well formed RC/RA expressions (for students to practise on) according to simple configurable parameters (e.g., max number of atoms, max level of quantifier nesting).For reasons of portability and potential integration of the code into larger projects, the server side logic of the tool should be programmed in Java and released under the MIT License. There is an existing Java codebase that implements RA expressions parsed using ANTLR, and that  could be extended to RC expressions.",Implementing a web app that translates relational algebra expressions to relational calculus ones and vice versa,1,1   Easy,"A web application that correctly translates between RA and RC, and provides the additional functionalities (1 and 2) described above.",8,0,f
4440,Formal semantics of the SQL language,ug4,2060,2020 02 21 10:22:15 00,1,"Even though the SQL standard has been around for more than three decades we still do not fully understand what to expect when executing a query. This is mostly due to the vagueness of the standard and the ambiguity of the natural language in which it is expressed. A formal semantics for a core fragment of SQL has been recently proposed [1], implemented in Python and experimentally validated on PostgreSQL. The project aims at extending the current semantics and its implementation to also include arithmetic operations, grouping and aggregation. Even though the existing codebase is in Python, the implementation can be carried out also in Java or C, but it would have to be done from scratch.The new semantics will be then validated by running experiments on a large number of randomly generated SQL queries to confirm that it captures the behaviour of PostgreSQL. To this end, the student will also have to design and implement (either from scratch, or by extending an existing codebase in Python) a random SQL query generator.Please note that this project is only suitable for advanced MInf students with a solid formal background and a keen interest in applied database theory.  ","Extending the semantics of a core fragment of SQL to include arithmetic operations, grouping and aggregation, and experimentally validate it using PostgreSQL.",1,4   Very Hard,"Extending the existing formal SQL semantics and its Python implementation to  capture arithmetic operations, grouping and aggregation. Developing a random SQL query generator that can be configured by means of  meaningful parameters, such as the max level of nesting and max number of tables in a FROM clause. Running experiments to compare the answers produced by the implementation of the new semantics with those produced by PostgreSQL on a large number (100000 ) of randomly generated SQL queries and databases.",8,0,f
4441,Preservation of Codd semantics in databases with SQL nulls,ug4,2060,2020 02 21 10:24:12 00,1,"SQL nulls are commonly interpreted as non repeating marked nulls, but even simple queries may produce answers that break this interpretation. Unfortunately, the class of queries preserving the Codd interpretation of SQL nulls cannot be captured syntactically. However, sufficient syntactic restrictions for preservation can be obtained by leveraging NOT NULL constraints on the database schema. For a thorough background on the topic and further details, please refer to  [1] or [2] (the latter is a freely accessible pre print of the latter).This research project has the following goals (see second part of Section 7 in [2]):Refine the restrictions of [1] by exploiting the associativity of the union and intersection operators, and by handling selection conditions in a finer grained way [HARD]Investigate the setting where multiple occurrences of a tuple with nulls are allowed in a database, and devise suitable restrictions for preservation in this context  [HARD] Come up with restrictions for the preservation of Codd semantics directly on the syntax of the SQL language, taking into account its formal semantics [3] [VERY HARD]The theoretical study with which this project is concerned could lead to the publication of scientific papers.Please note that this project is suitable only for advanced students with an interest for applied database theory (see requirements below).",Devise syntactic restrictions under which SQL nulls behave as non repeating marked nulls,3,Variable,"Each goal can be undertaken and completed separately from the others, therefore this project has capacity for 3 students. Successful completion will be judged on the quality and depth of the investigation performed by the student on the assigned/chosen goal, and the clarity and correctness of the results.",8,0,f
4442,Practical Quantum Algorithms for Problems on Partially Ordered Sets and Graphs,ug4,4282,2020 02 21 13:38:51 00,1,"A new class of quantum algorithms, with unexplored performance, is the class of variational quantum algorithms. These  algorithms are suitable for existing noisy quantum devices, and the extent that they offer quantum speed up depends on the specifics. Interestingly, one can run such algorithms (and test them) on specific quantum hardware or emulators. The major task for this project is to map different graph problems (starting with the known Max Cut but going to other problems involving partially ordered graphs) to a Hamiltonian problem (which is the basis for the VQA method). Standard (but non optimised methods exist). The task will then be to test the performance of the chosen quantum hardware for this specific problem and potentially extrapolate when could we expect such approach to lead to a realised quantum speed up.",Check the performance of variational quantum algorithms for graph problems,2,Variable,Review of VQA and its application on graph problems. Mapping of one unexplored graph problem to a Hamiltonian. Implementation of algorithm using parameters from an existing quantum hardware (preferably either IBM's or Rigetti's),8,0,f
4443,Missing pitch estimation and interpolation from speech waveforms with neural networks,ug4,1392,2020 02 21 13:38:54 00,1,"Prosodic features such as stress and intonation/pitch play important role in human speech communication, but they are difficult for computers/machines to use,mainly because automatic and reliable estimation of those features are still challenging in real situations and they are not continuous by its nature.The present project focuses on automatic estimation of pitch trajectories from speech waveforms, particularly recovery of missing pitch where physical fundamental frequency (F0) does not exist.To that end, we will explore feed forward neural networks for F0 estimation and recurrent neural networks for pitch trajectory estimation.",To develop a neural network model that estimates continuous pitch trajectories from speech waveforms whose fundamental frequencies (F0) are discontinuous.,2,2   Moderate,"Develop a neural network model that automatically estimates smooth pitch contours from speech waveforms, and carry out evaluation experiments.",8,0,f
4444,Automatic detection and classification of human head gestures,ug4,1392,2020 02 21 13:39:48 00,1,"Non verbal gestures such as facial expressions and body gestures play  an important role in human human communication. This project considers  head gestures (such as nodding and shaking) in dialogue conversation  and seeks to develop a system that analyses motion capture data of  head movements to detect events of head gestures and classify the  gestures automatically. A bootstrap machine learning approach based on deep learning or hidden  Markov models will be explored to cope with a large amount of data. If time allows, clustering approaches  will be explored to find various gesture units automatically.",To develop an automatic annotation system of human head gestures on motion capture data using machine learning techniques.,2,2   Moderate,"Develop an automatic annotation system of head gestures on head motion data, carry out evaluation experiments, and analyse the result.",8,0,f
4445,Natural head motion synthesis with Generative Adversarial Networks,ug4,1392,2020 02 21 13:40:32 00,1,"Automatic synthesis of realistic motions of a talking head (animated facial agent / avatar) is one of the crucial factors to make the talking head alive.   We have developed a novel system of head motion synthesis, in which trajectories of head motions are automatically estimated from given speech or text using  deep neural auto encoder and regression networks.  In this project, we seek to apply generative adversarial networks (GANs) to improve the naturalness of head motions. We will at first consider an unconditional GAN that generates motion trajectories randomly from a Gaussian noise, and we switched to a conditional GAN whose output is conditioned on speech or text features.",To automatically generate natural head movements of a talking head using Generative Adversarial Networks (GANs).,1,3   Hard,"Develop the two GAN based head motion synthesis systems, create corresponding video clips of a talking head, and carry out objective and subjective evaluations.",8,0,f
4446,Text driven head motion synthesis of animated agents using neural networks,ug4,1392,2020 02 21 13:41:03 00,1,"The project aims to apply recent technologies of neural networks to  automatically learn typical motions of human heads during conversation  and associate the motions with the utterance of the speaker, so that  the learned model can generate head motions of an animated agent from text.In the literature of animated talking faces, a lot of studies have  done to create natural lip motions that are synchronised with speech   (i.e. lip sync). The current project, however, put more emphasis on  the synthesis of natural head motions, which has not been studied that  much compared with lip sync regardless the fact that head motions play  crucial roles to make synthetic talking heads natural. We will be  particularly interested in the neural network models that are able to  learn typical head motion patterns (trajectories) in a unsupervised  manner.",To develop a system that is capable of synthesising a lifelike talking head from text using neural networks,2,Variable,"Develop a text driven talking face system, create video clips for evaluations, and carry out objective and subjective evaluations.",8,0,f
4447,Speech driven animation of human heads in conversation using neural networks,ug4,1392,2020 02 21 13:41:59 00,1,"Automatic synthesis of realistic motions of a talking head (animated facial agent / avatar) is one of the crucial factors to make the talking head alive. We have developed a novel system of head motion synthesis, in which smooth trajectories of head motions are automatically estimated from given speech signals using state of the art deep neural auto encoder and regression networks. In this project, we further explore the architecture to improve the performance. In particular, we seek to develop an auto encoder that is trained with both speech and motion data so that the network learns relationships/interactions between speech and motions.This project seeks to extend the framework so that the head moves not only when the agent is speaking, but also the interlocutor is speaking and the agent is listening.",To develop a system that automatically generates motion trajectories of a human head from speech in conversation using neural networks,1,Variable,"Develop a system that automatically estimates the motion trajectories of the head from speech signals of two people in conversation, create corresponding video clips of a talking head, and carry out objective and subjective evaluations.",8,0,f
4448,Practical Quantum Algorithms for Cryptanalysis,ug4,4282,2020 02 21 13:55:04 00,1,"It is known that many existing widely used cryptosystems will be broken, when a large fault tolerant quantum computer is developed. On the other hand, there exist systems that are believed to be secure against those known attacks. In this project we will mainly look on a new class of heuristic quantum algorithms called Variational Quantum Algorithms (Ref 1). While the performance of such algorithms is not well analysed, they have the advantage that these algorithms may work even with existing noisy intermediate scale quantum devices. We will look the performance of these algorithms, for a given quantum hardware, for cryptosystems. In particular, we will look both cryptosystems that are known to break with large quantum computers (such as RSA) and to other ones (based on Problems on Lattices) that are known to be resilient to existing quantum attacks.",Explore the resilience of cryptosystems to non standard quantum attacks,2,3   Hard,Review of VQA and its application on graph problems. Mapping of one cryptographic problems to a Hamiltonian problem. Implementation of algorithm using parameters from an existing quantum hardware (preferably either IBM's or Rigetti's,8,0,f
4449,Quantum Algorithms for Near Term Devices: Computational Chemistry,ug4,4282,2020 02 21 13:58:34 00,1,"Variational quantum algorithms is a class of quantum algorithms that are suitable for existing noisy quantum devices. However, their performance (and potential speed up) is not well explored. The performance depends on the specific hardware (levels of noise, natural/simpler operations) and on the problem to be solved (specifically the cost of mapping a desired problem to its quantum version). In this project, one hardware will be chosen (for example the one that IBM or Rigetti provided in the cloud). Then the major task will be to map different molecules (starting from cases existing in literature) to a corresponding qubit Hamiltonian (which is the basis for using the VQA method). The task will then be to test the performance of the chosen quantum hardware for this specific problem and potentially extrapolate when could we expect such approach to lead to a realised quantum speed up.",Check the performance of variational quantum algorithms for computational chemistry,1,2   Moderate,Review of VQA and its application for chemistry. Different mappings of (at least) one molecule Hamiltonian to a qubit Hamiltonian. Implementation of algorithm using parameters from an existing quantum hardware (preferably either IBM's or Rigetti's,8,0,f
4452,"OK, Piet: A semantic definition for the esoteric programming language Piet",ug4,7186,2020 02 21 14:17:52 00,1,"Piet is an esoteric programming language in which programs look like abstract paintings. It uses 20 colors, of which 18 are related cyclically through a lightness cycle and a hue cycle. A single stack is used for data storage.  You will first define a textual representation for the Piet language and implement a corresponding compiler. Dually, you will implement a translator of programs written using the textual representation to Piet image programs. You can choose the programming language for implementing these two deliverables.Using the textual representation, you will then define the semantics of the language in the K semantics framework. K is a rewrite based executable semantic framework in which programming languages, type systems and formal analysis tools can be defined using configurations, computations and rules. Configurations organize the state in units called cells, which are labelled and can be nested. Computations carry computational meaning as special nested list structures sequentializing computational tasks, such as fragments of program. Computations extend the original language abstract syntax.Finally, you should design and implement in K an extension of the language. This stage involves a creative process: you will have to imagine how can you make Piet more expressive, easier to use, and more FUn.",Defining the semantics of the esoteric programming language Piet,1,3   Hard,A K definition for an extension of the programming language Piet.,8,0,f
4453,Animatikz: A LaTeX package for animations,ug4,7186,2020 02 21 14:23:31 00,1,"You will develop a LaTeX package for animations using TikZ.PGF/TikZ is a pair of languages for producing vector graphics (technical illustrations, drawings etc.) from a geometric/algebraic description, with standard features including the drawing of points, lines, arrows, paths, circles, ellipses and polygons. PGF is a lower level language, while TikZ is a set of higher level macros that use PGF. The top level PGF and TikZ commands are invoked as TeX macros, while the PGF/TikZ graphics themselves are described in a language that resembles MetaPost. Existing solutions for animations in LaTeX, such as the packages Animate and Beamer, have a number of limitations that you should try to overcome. For example, not all pdf readers can load animations created using the package Animate. A possible solution would be to simply make available all the frames necessary for the animation as separate pages of a pdf (think of flipbooks); this should include the usual forward/backward controls to navigate through an animation.",Developing a LaTeX package for drawing animations,1,1   Easy,A LaTeX package for drawing animations.,8,0,f
4450,"Spin it, Maude! A SPIN like model checker in Maude",ug4,7186,2020 02 21 14:03:31 00,1,"SPIN is a tool for verifying the correctness of concurrent software models. Systems to be verified are described in Promela (Process Meta Language), which supports modelling of asynchronous distributed algorithms as non deterministic automata. Properties to be verified are expressed as Linear Temporal Logic (LTL) formulas, which are negated and then converted into BÃ¼chi automata as part of the model checking algorithm. Maude is a language based on rewriting logic. It can be used as a declarative programming language, and as an executable formal specification language, but it can also serve as a formal verification system. One can use Maude as a semantic framework to formally represent a wide range of systems, including models of concurrency, distributed algorithms, and network protocols. Thanks to its expressivity and flexibility, Maude is a useful meta tool for building other tools.The main goal of this project is to implement a model checker similar to SPIN in Maude.The project has three stages: 1. You will first use Maude specifications to reproduce SPIN programs and the search functionality in Maude to simulate the model checking facilities of SPIN.2. You will then develop a methodology for writing such Maude specifications for model checking a la SPIN.3. Finally, you will extend Maude with dedicated syntax and commands to ease the model checking process.",Implementing a model checker similar to SPIN in Maude,1,4   Very Hard,An extension of Maude implementing a model checker similar to SPIN.,8,0,f
4454,Quantum Algorithms for Near Term Devices: Comparing different hardware,ug4,4282,2020 02 21 15:02:37 00,1,"Variational quantum algorithms is a class of quantum algorithms that are suitable for existing noisy quantum devices. However, their performance (and potential speed up) is not well explored. The performance depends on the specific hardware (levels of noise, natural/simpler operations) and on the problem to be solved (specifically the cost of mapping a desired problem to its quantum version). In this project, we fix one known problem (max cut) and we compare how it performs under different platforms. The student will choose at least two platforms (superconducting, photonic, ion traps, cold atoms, diamond based, silicon, etc), and for each of these platforms will test the performance of the algorithm assuming different (but realistic) noise levels, and choosing the best Ansatz family of states that the hardware can offer.",Compare how different quantum hardware perform on variational quantum algorithms,1,1   Easy,"Review of VQA and the solution to Max Cut. Implementation of algorithm for at least two platforms (i.e. finding from experimental groups/papers, realistic noise models and gate sets and then implementing the algorithm for small instances) Comparison of the platforms.",8,0,f
4457,Tools for extracellular neural recording analysis,ug4,1344,2020 02 21 15:10:42 00,1,"Large scale, high density microelectrode arrays (HD MEA) now allow the simultaneous recording of the activity of thousands of neurons, both in vitro and in vivo. These arrays record the extracellular voltage changes produced by neurons with thousands of closely spaced channels at near cellular resolution, providing an unbiased sample of neural activity. In such recordings, the activity of each neuron can be seen as a small deflection in the extracellular potential (a spike)   and it is a major challenge to then reconstruct the activity of single neurons from this mixture of noisy signals, a process called spike sorting [1].To support this process, we have developed SpikeInterface [2], an open source software framework to automate and standardise such analysis:https://github.com/SpikeInterfaceThis software already contains a number of tools and algorithms to support spike sorting and quality control. In this project, you would contribute to further developments, focussing on a particular task. There are several direction a project can take, which range from developing convenience functions for common tasks (easy) to using probabilistic modelling to identify false positives and poorly detected neurons.I will arrange a meeting for all students interested in my projects, where I will give more information, in particular relating to difficulty and required background. Note that all of my projects require reasonable programming skills, ideally in python. I will generally mark students declaring interest in my projects as 'suitable' unless there is a very strong reason not to do so. So please be sure you are comfortable with the topic and difficulty before making your selection.","Develop, test and integrate a tool for extracellular neural recording analysis in the SpikeInterface framework",2,Variable,"Development, testing and documentation of a tool for extracellular neural recording analysis in the SpikeInterface framework.",8,0,f
4458,Machine learning for large scale extracellular neural recording analysis,ug4,1344,2020 02 21 15:11:08 00,1,"Large scale, high density microelectrode arrays (HD MEA) now allow the simultaneous recording of the activity of thousands of neurons, both in vitro and in vivo. These arrays record the extracellular voltage changes produced by neurons with thousands of closely spaced channels at near cellular resolution, providing an unbiased sample of neural activity. In such recordings, the activity of each neuron can be seen as a small deflection in the extracellular potential (a spike)   and it is a major challenge to then reconstruct the activity of single neurons from this mixture of noisy signals, a process called spike sorting [1].Here we will improve methodology to localise spikes in space [see ref 2], and align them with micrographs taken from the same preparation the recording was done. This project requires good understanding of probabilistic modelling and experience with a deep learning framework (pytorch).I will arrange a meeting for all students interested in my projects, where I will give more information, in particular relating to difficulty and required background. Note that all of my projects require reasonable programming skills, ideally in python. I will generally mark students declaring interest in my projects as 'suitable' unless there is a very strong reason not to do so. So please be sure you are comfortable with the topic and difficulty before making your selection.",Implement and test methodology to register recorded spike locations with true neuron locations,1,3   Hard,Implementation of a method for co registration of electrical recordings and images,8,0,f
4459,Designing and Developing a Technology Based Tool to Help Children with Autism Cope with Changes,ug4,2496,2020 02 21 15:31:39 00,1,"One of the core characteristics of children with autism is  insistence of sameness  or  restricted and repetitive behaviours and interests  [1]. For example, some individuals with autism may have rigid preferences for food (e.g. eating only specific items) or everyday objects (e.g. using a particular type of shampoo). Routines around daily activities may become real rituals, followed in tiny detail. Consequently, sometimes minor changes such as moving from one activity to another or a slight change in daily routine can cause distress [2,3]. Other changes, such as going on holiday, moving house or changing school may be extremely challenging. The goal of this project is to design and implement a technology based tool to support children with autism overcome difficulties when changes occur. The tool is not expected to address all kinds of changes, but rather to focus on a specific category of changes. The student is expected to design the tool based on a review of relevant literature and on consultation with people with experience in working with children with autism (e.g. researchers, practitioners and/or parents). The completion of the project also involves implementing and evaluating the tool. A possible approach for this project may be:Understand the domain and problem by reviewing relevant literatureExtend the understanding of the domain and problem by consulting with people with experience in working with children with autism (e.g. through interviews, focus groups, questionnaires)Derive a set of requirements for the tool based both on the literature and data collected from peopleDevelop a paper or low fidelity prototype Conduct formative evaluation with potential users. The formative evaluation may include practitioners, parents, and/or children with autism or, in their absence, typically developing children (TD)Revise the paper prototypeTime permitting, redo steps 4 5.Develop a working prototype Design and conduct summative evaluation. This evaluation may include experts in autism (e.g. researchers, practitioners and/or parents) and/or typically developing childrenThe difficulty of the project is variable, depending on the approach taken. If interested, please fill in  this Google Form. We will assess your replies and, if you are suitable, we will contact you for a quick chat.Note*:  User studies may require Ethical Approval.","To design, develop and evaluate an educational technology based tool for children with autism",2,Variable,A high fidelity prototype for a technology based tool that helps children with autism cope with changes. Evidence based evaluation of the prototype,8,0,f
4461,A graphical workbench for experimenting with hash tables,ug4,1328,2020 02 21 16:10:22 00,1,"Hash tables are part of the computer scientist's repertoire of standard data structures, and come in several flavours (e.g. bucket array tables; open address tables with probing), with the choice of hash function playing a key role. As is well known, different design choices give rise to different performance characteristics, suggesting that some kind of simulation tool is desirable for comparing various hashing schemes. Moreover, hash tables lend themselves very naturally to graphical display, suggesting the idea of a graphical tool that enables the user to gain a ready understanding of the effects of different choices.This project will develop a tool of this kind, supporting a range of different schemes, allowing user defined hash functions, and offering both manual and automated simulations of sequences of operations. The starting point will be the material on hash tables covered in Informatics 2B (or now in Introduction to Algorithms and Data Structures), though the project is somewhat open ended and there is scope for incorporating more advanced hashing schemes.","To design and implement an educational tool for simulating and visualizing hash tables of various kinds, supporting various experiments.",1,1   Easy,"A working implementation of a graphical tool that simulates at least two kinds of hash table with user defined hash functions, and supports experiments demonstrating the resulting performance differences.",8,0,f
4462,A Backend for a Coherence Protocol Generator,ug4,1378,2020 02 21 16:41:14 00,1,"ProtoGen [1][2], a project developed here at Edinburgh, is the first tool for generating cache coherence protocols given their atomic specifications. As of now it simply generates coherence controller state machines in a verification language (to enable easy verification).  The goal of the project is to implement a backend for the tool to generate coherence protocols in the SLICC [3] language used by the gem5 simulator [4], thus allowing  for a nice tool to test and evaluate coherence protocols.  Because ProtoGen is the first cache coherence protocol generator, it already is starting to get used by the industry. Developing a gem5 port will pave the way for even more wider adoption since Gem5 is used by a number of academic and industry practitioners.  Therefore, the potential impact of this project is significant. If you do a good job, your code will likely get to be used by practitioners!","The goal is to develop a gem5 backend for the ProtoGen coherence protocol generator tool. The backend will enable the automatic implementation of the cache, directory and memory controllers generated by ProtoGen in the gem5 system simulator.",2,Variable,1. Implementing the backend and ensuring that it is correct by running it in the simulator. (easy project) 2. Implementing a number of protocols and comparing the protocols with those that are already supported in gem5 (medium to hard).,8,1,f
4463,Cache Coherence Protocol Library,ug4,1378,2020 02 21 16:42:12 00,1,"The cache coherence protocol is an important component of a multicore processor. Mechanically verifying the cache coherence protocol of a multiprocessor is essential, as it is too difficult to reason about all corner cases by hand. Model checking is a commonly used technique to guarantee the absence of bugs of an abstract model of a coherence protocol.  For coherence protocols, the Murphi[1] model checker has established itself as the tool of choice. However, it may still be possible for a human translator to make mistakes in the translation from a real translation to the model accepted by the model checker.  One solution is to provide a higher level specification language and translate to various targets (e.g. Murphi)   to minimize the risk of erroneous translation. We have such a language developed here at University of Edinburgh, called ProtoGen, for specifying protocols and a compiler that then converts it to Murphi [2].  The goal of this project is   to use ProtoGen to:(a) build a library of cache coherence protocol specifications from a textbook [3] and/or an industrial coherence protocol specification [4].  (b) Verify that the protocols are correct;and  optionally(c)  Test and extend roc3 to ensure that a variety of protocols are able to be expressed in it.  ",Creating a library of verified cache coherence protocol specifications from: (a)  a text book and/or (b) An industrial coherence protocol specification,2,Variable,1. Expressing at least 5 new protocols from the textbook and ensuring they are correct. (easy). 2. Expressing the industrial coherence protocols specification and ensuring that it is correct (medium). 3. Improving on ProtoGen to fix bugs and provide new functionality (hard),8,1,f
4464,Automatic virtual channel assignment for the ProtoGen tool,ug4,1378,2020 02 21 16:43:00 00,1,"ProtoGen  [1][2], a project developed here at Edinburgh, is the first tool for generating  cache coherence protocols  given their atomic specifications.  The cache coherence protocol is an important component of a multicore processor.In its present state, ProtoGen has a limitation in that, it expects the user to manually assign coherence protocol messages to virtual channels. The goal of the project is to automate this process.  Using a state space exploration algorithm, it is possible to analyze whether a subsequent message arriving at a cache controller that could be served immediately is blocked by a previous message causing a deadlock. If this is the case, the incoming message is moved to a different communication network (by assigning it to a virtual channel). The virtual channel assignment algorithm should integrated into the ProtoGen tool.The implemented virtual channel assignment algorithm will provide ProtoGen with the ability to automatically detect and avoid deadlocks in the generated protocol.    The correctness of the virtual channel assignment algorithm can be automatically verified by using the Murphi model checker [3].",The goal of this work is to automate the assignment of cache coherence messages to virtual channels within the network to avoid deadlocks.,1,3   Hard,"Desining the virtual channel assignment algorithm, integrating on to ProtoGen and verifying correctness with protocols.",8,0,f
4465,The Speckled Puppeteer,ug4,3073,2020 02 21 18:00:56 00,1,  A person wearing a wireless inertial sensor on the back of each  hand controls the connected limbs of the virtual puppet. Changes in the linear acceleration and angular velocity of the hands in three dimensions are sensed by the 3D accelerometer and gyroscope which  are mapped to movements of the head/torso/limbs in real time in the UNITY environment.  This is  equivalent to traditional (analogue) puppeteers in the physical world manipulating the movement of the constituent  parts of the puppets using strings attached to them.  The challenge will be to design an intuitive interface   for smooth manipulation of the virtual characters. This can be extended to providing live sound accompaniment to the movements controlled by the movement of the hands.  ,"To manipulate virtual characters/avatars  in the UNITY environment using wireless inertial sensors, each attached to the back of the hands.",1,Variable,(i) Implement a system to manipulate movements of a human model in UNITY (ii) Implement an intuitive user interface (iii) Augment movement with sound,8,0,f
4467,Sensing Spaces: A Study in Mapping Air Pollution in Public Spaces using Personal Exposure Monitors,ug4,3073,2020 02 21 18:06:15 00,1,"The Airspeck mini   is a personal exposure monitor developed in the School of Informatics which is worn on the belt and measures the concentrations of airborne particulates (in terms of PM10/PM2.5/PM1 which refer to the concentrations in microgram/m^3 of particles with a diameter less than 10, 2.5 and 1 micron, respectively). The sensor readings, tagged with time and location (GPS) are communicated wirelessly to a mobile phone app for onward transmission to the server.  1. Nearcasting air quality based on crowd sourced Airspeck mini data   This part of the project will compare different machine learning models developed for spatio temporal estimation of PM concentrations in Edinburgh   Scotland,   Leon   Mexico, and Delhi   India,   with the scope for modifying them to improve their performance.  2. Integrating the crowd sourced Airspeck mini data with the Airspeck Stationary data to improve nearcasting air quality in central   The stationary version of the Airspeck monitors are attached to lamp posts and provide up to date PM concentrations at fixed points in the city. This part of the project will investigate on line learning methods to improve the spatio temporal estimation of PM levels using the stationary monitors by using the mobile Airspeck mini as ground truth data at locations and times when they are available.  3. The Airspeck monitor provides histograms over time   of particles in 24 sized bins ranging from 0.3 microns to 30 microns. The sources of indoor airborne particulate pollution such as cooking, cigarette smoking, incense, and cleaning sprays, or outdoor air pollution, such as automobile exhaust, have unique histograms. This project will implement classifiers for air pollution sources using a variety of machine learning models for comparison.4. A route planner   for clean routes in the four cities around the world.     We are familiar with route planners which provide the shortest path using different modes of transport. This part of the project will implement a route planner for pedestrians and cyclists which provides the cleanest, shortest routes in central Edinburgh based on nearcasting spatio temporal particulate levels.  ",A study in mapping air quality in urban spaces in four continents using crowd sourcing methods,3,Variable,Project 1: Comparison of results for implementations of at least two models. Project 2: Comparison of results for implementations of at least two on line models. Project 3: Comparison of results for implementations of at least two classifiers. Project 4: Implementation of the route planner with display in a GIS of your choice,8,0,t
4466,Experiments in Social Signals Analysis for COPD patients,ug4,3073,2020 02 21 18:03:43 00,1,"Compulsive Obstructive Pulmonary Disease (COPD) is an umbrella term for respiratory conditions such as chronic bronchitis, emphysema and upper airways obstruction. COPD is a challenge for the National Health Service [1]   it accounts for the second largest cause of emergency hospital admissions (one million bed days and 21% of all respiratory beds each year in the UK);  these admissions account for 54% of the total  £800 million spent on COPD; and worryingly, in 2007 the British Lung Foundation estimated around 75% of COPD cases remain undiagnosed.The RESpeck device is worn as a plaster on the chest and measures continuously respiratory rate/effort and activity types. After local processing the data is transmitted via a mobile device to the server for analysis. This project will investigate methods to analyse the data to derive social signals from the sensor data such as periods and types of activities, and number of speech episodes, which when combined with the physiological parameters will provide a better estimate of the patient's condition.This project will analyse the COPD data set and develop a scorecard which combines the physiological and social signals and correlate this to the patients' condition as reported in the diaries.  ",Develop a scorecard which combines respiratory rate/effort and social signals from COPD patients,3,2   Moderate,Software which combines physiological parameters and social signals to produces a score which is correlated with reported conditions.,8,0,t
4468,"Analysis of Air Pollution, Respiratory and Physical Activity datasets for investigating health effects of air pollution",ug4,3073,2020 02 21 18:08:58 00,1,"[The following describes a collection of six independent projects which share five datasets and a common approach towards sensor data analytics]Air pollution in the urban environment and its health effects are of increasing concern [1]. The Lancet Commission in 2017 reported that air pollution was estimated to be directly responsible for 6.5 million deaths in 2015 [2].Over the past two year a number of projects around the world have been collecting data on the health effects of air pollution  using wearable sensors developed in the School of Informatics . The Airspeck device attached to the belt monitors personal exposure to air pollution in terms of   concentrations of airborne particulates (PM10/PM2.5/PM1, i.e. concentrations in micrograms/m^3 of particles with diameters less than 10 microns, 2.5 microns and 1 micron, respectively). The Respeck device worn as an adhesive patch on the chest monitors the subject's respiratory rate/flow and physical activity. The data from both the sensors are time synchronised and spatially resolved (GPS location). The following research projects have collected Airspeck and Respeck datasets which are available to be analysed:(i) An H2020 project investigating the effect of second hand smoking on patients with respiratory diseases, such as asthma and COPD.(ii) An MRC /NERC funded project investigating the   exacerbation in the condition of young asthma patients (ranging in age between10 to 18 years) due to air pollution exposure.(iii) An MRC /NERC funded project investigating the effect of exposure of air pollution during pregnancy on the development of the foetus and the respiratory health of the offspring in the first two years.(iv) An MRC /AHRC funded project investigating the socio economic biases in exposure to air pollution in asthmatic adolescents.(v) A WHO funded project investigating exposure to air pollution during the diurnal cycle (at home, during commute and at work) for different types of workers (office, drivers, messengers, couriers).(vi) An EPSRC funded project investigating the health effects of exposure to air pollution in asthmatic adults in London  The principal aim of the following projects is to apply machine learning techniques to identify patterns in the spatio temporal Airspeck and Respeck sensor data for the applications enumerated below. The method will first be developed and experimented   on a trial dataset, and once refined and tested, applied to a selection of the five data sets listed above. Each project   is independent although they share a common approach to   sensor data analytics using   machine learning techniques and the six students will share the five data sets.   The students will therefore meet their supervisor as a group on average twice a week and will be encouraged to work together and collaborate loosely. 1. Detection of social signals such as speech episodes, coughing, drinking, eating.2. Detection of anomalous breathing patterns, including wheezing and obstructive breathing.3. Identification of sleep episodes and sleep wake patterns4. Classification of physical activity such as lying down, sitting/standing, walking, running, and climbing stairs.5. Identification of modes of transport, such as cycling, driving a car or a motorcycle, and as a passenger in a car, bus, train.6. Classification of pollution sources: cooking, vehicles, smoking, incense burning, mosquito coils.",To develop machine learning methods to identify patterns in spatio temporal sensor data sets specific to six applications.,3,Variable,"Demonstration of the feasibility of the method developed on the trial data set, following by its application on a selection of the five pollution and health data sets.",8,0,t
4469,Toward a Technology to Overcome Anxiety in Children with Autism,ug4,2496,2020 02 21 18:14:21 00,1,"Autism Spectrum Disorder (ASD) is a neurodevelopment disorder characterised by difficulties in social interaction and communication, coupled with restricted interests and repetitive behaviour [1].  Individuals with autism spectrum disorder (ASD) have also associated mental health problems (comorbidities) and anxiety is one of the prominent among these. Children with ASD are at a higher risk to experience anxiety  often because they lack an appropriate the skills needed to navigate social situations and fail to develop adaptive coping mechanisms. Research revealed that  39.6% of young people with ASD had at least one anxiety disorder diagnosis, with specific phobias, obsessive  compulsive disorder (OCD) and social anxiety being most commonly reported [2]. Anxiety can lead to increased  maladaptive  behaviour, social skills deficits and negative life experiences [3]. Prior research literature provides promising evidence for the benefit of using technology to  treat anxiety symptoms in children with ASD [4].  This project proposes the design and development of a technology (e.g. an app, a website) that can  address the specific anxiety management needs in children with ASD.A possible approach may be:  1. understand the particular  challenges encountered by children with autism when experiencing anxiety by reviewing the relevant literature2. reviewing/testing existing technologies that support children with ASD manage with anxiety, identifying their limitations3. conduct empirical studies to inform the design of a new technology to address the specific anxiety management needs in children with ASD  (e.g. focus groups, interviews)4. develop a prototype and conduct formative evaluation with potential users and/or experts (e.g. researchers in HCI)5. evaluate your technology with potential users  and/or experts and conclude on findings and future work.If you are interested, please fill in this Google Form.  ",To design and develop a new technology to addressing the specific anxiety management needs in children with ASD,2,Variable,"1. A high fidelity prototype (e.g. website, an app, a set of Alexa's skills) of a technology addressing anxiety in children with autism. 2. Evidence based evaluation of the prototype",8,0,t
4470,Supporting Interactive Inference in FRANK,ug4,4006,2020 02 21 18:21:26 00,1,"The FRANK (Functional Reasoning for Acquiring Novel Knowledge) [1,2,3,4] question answering system combines deductive reasoning and statistical inference methods to find answers to questions. This goes beyond the task of information retrieval to reasoning about the question, decomposing it, finding data to instantiate variables and then making inferences.Hypothesis:While there is a rapidly growing amount of data on the web (e.g. Knowledge graphs), enabling interaction with the user during the construction of the inference tree and the instantiation of variables could help FRANK find answers much more efficiently by reducing the search space.However, overly depending on user inputs reduces FRANK's efficiency since the user is overloaded and is slower in providing the necessary bits of information that FRANK needs.Our objective in this project is to define a model that incorporates user inputs into various aspects of FRANK's inference:  Decomposition rule selection: Selection of rules that recursively decompose the query alist until the variables at the leaves of the inference graph are instantiated. Alists (association lists) are sets of attribute value used to represent a query in FRANK.  Aggregation operation selection. Selection of inference operations (could be arithmetic, statistical, etc.) used to combine data in child nodes to their parent nodes in the inference graph.  Data source selection, and  Data values for instantiationAdditionally, given the trade off between the usefulness of the user's input (guidance) and the potential to overwhelm the user, it is important for FRANK to be selective about requesting user inputs.  ",Adding user interaction to support inference during automatic question answering,1,4   Very Hard,A model to determine how and when FRANK requests user inputs. An implementation of the model as a module in FRANK.,8,0,f
4471,Deep learning for robot control,ug4,1333,2020 02 21 18:28:34 00,1,"Learning robots need to predict the change of the sensory inputs that is caused by their control actions, which may involve complex mappings e.g. in the case of visual input. Recently, deep learning methods have become interesting for processing of visual information and their potential for robot control is beginning to become realised. The project work with an implementation of a recurrent deep neural network in the context of exploratory robot control [1], i.e. for a robot that aims particularly at performing actions that tend to produce sensory inputs which are useful for improving the deep learning sensory processing architecture. In the project, the use of a simulated robot [2] will be acceptable, but tests in a real robot (such as Sphero) is encouraged.",Implementation and testing of a neural network based algorithm for robot control,1,3   Hard,Implementation and evaluation of a neural network based algorithm for control of a simulated robot.,8,0,f
4473,Automatic Knowledge Discovery,ug4,4006,2020 02 21 18:32:11 00,1,"Many knowledge graph based QA systems depend on how triples (or facts) are found in the graph. However, they depend on surface text matching or some form of ontological alignment of relations and entities between the query and the knowledge graph. In many cases, this step is taken for granted on the assumption that the query terms will be represented the same way as presented in the KGs. This problem is further compounded when doing open domain question answering across different knowledge bases (KBs) on the web. For instance, consider a query that seeks to find the population of the UK. The relation/property "";population""; needs to be matched against properties in different KBs to determine if the KBs will have data for the domain. In this example, the following KBs contain the relation required but with different names for the same property:Dbpedia =  dbo:populationWikidata = wdt:P1082Worldbank = SP.POP.TOTLGeonames = populationThis work is directly relevant to how FRANK (Functional Reasoner for Acquiring Novel Knowledge) QA system [1,2,3] answers questions. Each query to a KB needs to have its relations and entities correctly represented before data can be successfully retrieved from KBs.  The aim of this project is to perform this alignment for different knowledge sources in a systematic way.  For a knowledge graph G, with entities E and properties P with facts represented by &lt;e_1,   p, e_2&gt; with e_i \in E and p \in P we want to achieve the following:Automatic knowledge discovery based on property names where FRANK determines equivalent properties and entities across diverse KBs including, but not limited to, Linked Data sources.This work is related to research on ontology matching [5].","Automatic knowledge discovery based on property names such that the system finds equivalent properties and entities across diverse knowledge bases including, but not limited to, Linked Data sources",1,4   Very Hard,"1. Construct a list of KBs from different domains including demographics, finance, music, commonsense, etc. that can be searched. Also identify their API endpoints. This will serve as the basis of evaluation. 2. Formalize the methodology for knowledge discovery and implement an algorithm to search and identify property names that match the properties in a posed question. 3. Rank candidate properties from successful matches such that the ranking captures the uncertainty in the matches. 4. Evaluate using existing QA datasets: e.g QALD [4], 5. Implement property search algorithm as a module that the FRANK QA systems can use during inference. 6. Extend the knowledge discovery algorithm and evaluation to entities (vertices) in the knowledge graphs.",8,0,f
4474,Using AI and Machine Learning to improve Learning,ug4,5236,2020 02 21 18:33:08 00,1,"Learning is increasingly carried out outside the context of the classroom. Examples include technologies   embedded in traditional courses such as course forums and virtual laboratories, to   courses conducted completely online like distance learning or MOOCs.   This project will use the huge amount of fine grained data of students' interactions, coupled with Big Data and artificial intelligence mechanisms, to design tools for supporting both students and teachers.     Projects will center on one of the following topics (students choose *either* topic A or B).A) Computational Modelling of student behavior in online learning. Examples of possible projects can focus on predictive models of student dropout (Machine learning); modeling of student engagement in courses over time (temporal models); detecting plagiarism or confusion in online course forums (NLP), inferring students' mastery skills from data (probabilistic models).B) Designing Intelligent, real time feedback mechanisms to support personalized teaching and learning. Examples of possible projects can include personalized recommendations to students (AI planning), simple interventions (e.g, emails) to increase motivation (Decision making), visualizations for student engagement in courses (HCI). This is a cluster project, in which several students work on individual projects that are thematically related to AI and big data in education.   There will be dedicated weekly group meetings (in addition to the general group meetings for projects) to discuss relevant background reading, and for students to help each other with technical questions. The project layout is to focus on background reading in the first two weeks, followed by programming the technological infrastructure that is necessary for each project. Data   will be made available to each project.A key aspect of this project is to evaluate the models, first in simulation or offline, and if deemed applicable, to be incorporated in a real course. ",Study the use of AI and machine learning to model and support learners and researchers,4,3   Hard,"Completion of an end to end study involving literature review, algorithm design, implemention and study",8,0,t
4475,Incrementing the MarkEd online marking tool with support for fairer and more efficient marking,ug4,2496,2020 02 21 18:51:00 00,1,"Marking is undoubtedly one of the most important teaching activities [1]. It is a central part of assessment, which is crucial to foster learning as well as the relationship between teachers and learners as stated in the University of Edinburgh's latest Taught Assessment Regulations [2]. One of the principles that must guide marking is fairness, as all students should be judged equally in terms of academic performance. Fairness requires consistency both for one marker and across markers and that is difficult to achieve. In addition, factors such as tiredness or mood could affect marking fairness. To make things worse, the emotional aspect adds burden to this activity, especially for beginner markers. Fairness is intended to be improved through moderation by the course organiser, however moderation cannot ensure it is met 100%. Because of time constraints, efficiency must also guide marking. The situation is again worse for beginner markers, whose time allocated for marking may be set up by senior colleagues with more experience. In the School of Informatics, this is the case for teaching support staff whose hours to spend in a marking role are determined by course organisers when setting up this role.Getting the right balance between fairness and efficiency in marking is quite challenging, especially since they may require contradictory activities. For example, being too quick in marking may lead to an incorrect decision about the student's performance and thus an unfair mark, while investing the maximum time to ensure fairness may make marking infeasible time wise.MarkEd is an online tool (website) for marking, feedback and moderation that currently being developed by undergraduate students. For now, it does not include considerations of marking fairness and efficiency. The aim of this project is to collect strategies, both manual and computer aided, to ensure both fairness and efficiency in marking, and then design, develop and evaluate an increment to the current design of MarkEd that would support users in being more fair and efficient.  The following are a possible series of steps for this project:Review literature on strategies, both manual and computer aided, to ensure fairness and efficiency in markingConduct user studies with academics and teaching support staff in the School of Informatics to also understand what they are doing to try to be fair and efficient.Pick up a subset of strategies from 1) and 2) and come up with a proposed increment to the MarkEd tool design (probably using a design tool like Figma [3]) that would support users in being more fair and efficient in their marking.Evaluate the proposed changes to the design with academics and teaching support staff in the School of InformaticsImplement some or all of the proposed changes to the design in MarkEd, and evaluate them again with academics and teaching support staff in the School of InformaticsThere is no need to contact us for this project. We may arrange to meet with you for a before the deadline for choosing projects.If interested, please fill in this Google Form.","The aim of this project is to collect strategies, both manual and computer aided, to ensure both fairness and efficiency in marking, and then design, develop and evaluate an increment to the current MarkEd online marking tool (under development in 2019 2020) that would support users in being more fair and efficient.",1,Variable,"A review of strategies, both manual and computer aided, to ensure fairness and efficiency in marking. An understanding of what academics and teaching support staff in Informatics do to be fairer and more efficient in their marking. An increment to the design of the MarkEd online tool (probably using a design tool like Figma) to support users in being more fair and efficient. An evaluation of this design with academics and teaching support staff in the School of Informatics. An implementation of some or all of the design in MarkEd  A final evaluation.",8,0,f
4476,Integrated information in a robot swarm,ug4,1333,2020 02 21 19:18:31 00,1,"Drawn from consciousness studies, integrated information [1] is a measure that defines integration as the degree of causal influences among elements [2].   Integrated information has a high computational complexity and requires full knowledge of a distributed system. Moreover, it is unknown whether it does explain anything about consciousness. The project is thus not meant to study consciousness, but to test the usefulness of the integrated information for an adaptive robot swarm. The idea is to use simple approximations of integrated information in the swarm, and to check whether these approximations can be used by agents in the swarm to improve in one or more of the following criteria: (i) achieve better coherence or get stuck less often, (ii) are more easily achieving learning success in a simple reinforcement learning task, and (iii) actually reach a high level of integrate information. Coding can be done in Python [3]. Acknowledgement: The project idea is due to Calum Imrie.",Evolution of control of a simulated robot swarm based on integrated information,1,3   Hard,An local approximation of integrated information is used in a small simulated robot swarm and the resulting behaviours are analysed based on behavioural measures such as swarm coherence.,8,0,f
4472,Control of behaviour in a pet robot,ug4,1333,2020 02 21 18:31:18 00,1,"The aim of the project is to develop a minimal animal like soft robot and design, train and test its behaviour in a human robot interaction scenario. The behaviour will be produced by adaptive learning [1] and reinforcement learning and will mainly consist of micro actions which are intended to assure the user of the presence of the robot. The main task in the control of the robot is reaching the joint goal of maximally active engagement with the human user and minimal annoyance of the user by the robot. A design like the Konpanion robot [2] that is available for the project is to be realised and experimentally tested in order to evaluate whether the goal is realistic within the simple design paradigm.",Generation and evaluation of elementary behaviour in a pet robot,2,Variable,Implementation of an algorithm for the generation of elementary behaviour in a pet like robot and testing the algorithm in a human robot interaction scenario,8,0,f
4477,Vision for Good,ug4,7119,2020 02 21 21:07:16 00,1,"This topic is for students with a strong foundation in image processing / machine learning to turn their hand to real world problems.Potential problems include:DeepFake detection (https://www.kaggle.com/c/deepfake detection challenge/overview)Cassava disease classification (https://www.kaggle.com/c/cassava disease/overview)Wheat Rust detection (https://www.cv4gc.org/cv4a2020/#wheat)Students are also welcome to propose another problem close to their hearts, as long as a dataset is available for it (try https://zindi.africa or https://www.kaggle.com for inspiration).Student projects are not expected to necessarily be topping the leaderboards in these areas (although a student who wishes to attempt this is certainly encouraged to try).  The project may take a number of forms:        Replicating a solution in an already finish competition (easier)        Applying an existing machine learning technique and attempting to tune it to the problem        Analysing traits of the data (lighting, geographic information, etc) to identify features that may contribute to solving the problem (ideal for a student with less strength/interest in machine learning)If you wish to apply for this project, please send a  short  email to  james.garforth@ed.ac.uk  explaining any relevant background you have, what problem you would like to tackle and how you think you would first approach it (ie. what technique you'd apply and why).  Consider what kind of data is in your chosen challenge dataset and what type of output you want. You are not bound to these statements for the project, but it helps me to see that you've given it a bit of thought.",Attempt to solve an image processing problem with positive social impact.,2,3   Hard,Implementation and evaluation of an image processing system that attempts to solve a challenge problem.,8,0,t
4478,Adversarial Data: When The Environment Fights Back,ug4,7119,2020 02 21 22:02:55 00,1,"Researchers developing visual navigation or mapping systems (for example, for autonomous cars) rely on recorded or simulated environment datasets. The relatively small variety of datasets available leaves open the risk that algorithms end up over tuned or poorly evaluated.This project aims to flip the standard navigation research paradigm: instead of adapting a new algorithm to existing datasets, we will produce new datasets that are adversarially adapted to test the limits of one or more existing algorithms.The subtasks for this project are:Using a game engine (preference Unreal) to procedurally generate simple simulated environments.Integrating the generator with one or more mapping algorithms (ideally via SLAMBench) to use them for evaluation.Adding user parameters to the generator to allow fine tuned control over the kinds of environments generated (size of objects, how they're spaced, etc).Allowing the generator itself to use feedback from evaluation of the algorithm to adapt and adversarially explore the limits of its capabilities.It is not expected that a student will address all of these in a single project, though an MInf student might try them all over two projects.If you're interested in this project (or self proposing in this area) send me an email to james.garforth@ed.ac.uk with your relevant experience to arrange a meeting or discuss.",To build a simulated environment generator that tests the limits of navigation algorithms.,1,3   Hard,A system that generates novel simulated environments and uses them to evaluate one or more navigation algorithms.,8,0,f
4480,Facilitating and authenticating the reading of Terms & Conditions and Privacy Statements,ug4,2094,2020 02 21 23:29:26 00,1,"Not all information is exciting to read:  engaging  and  important  can be orthogonal properties.   This tends to be particularly true for legal statements.     However, many of these are non optional, requiring acceptance and registering acknowledgement of having read them.   Access to information, or permission to use a piece of software, technology or data, is often contingent on giving informed consent.   In reality people are notoriously bad at reading and understanding these requirements, and often do not even bother any attempt to do so.   Consent is therefore typically not informed in practice.   Legal, moral and ethical protection is important for both consumers and providers of information.   But this can produce a possible conflict over where the onus of responsibility really lies: is it with the consumer or provider?This project will begin with a survey of Term &amp; Conditions and Privacy Statements connected with digital contracts (websites, apps, social media, etc.) with the aim of providing Best Practice Guidelines for the efficient and effective implementation of these  necessary evils .   Examples of things to monitor and measure include the quantity of text, complexity and minimum level of reading age, redundant content, the number of mouse clicks and amount of scrolling required, plus other basic usability and human accessibility factors.   Approaches adopted when presenting other types of information should also be considered, e.g. online articles often provide an estimate of the time required to finish reading them. The second stage of the project will involve taking the findings arising from the domain survey and implementing/testing them using appropriate design methodologies or prototypes to form a set of guidelines and suggestions.   Two possible approaches to consider (along with a hybrid of the two) are:Encouraging good user behaviour.   For example: highlighting the benefits, risks and providing estimated completion times; developing a system of rewards or user kudos; making the presentation of information more fun or easy to read.Compulsory implementation.   For example, developing a system that prevents skipping; identify human versus non human behaviours in information processing (nobody could read that amount of text meaningfully in 3 seconds and without scrolling), perhaps implementing a captcha style  I am not a robot  system; comprehension testing; eye tracking or gaze detection to ensure reading.The third stage, if there is sufficient time, is to create your own working exemplar(s) or to improve some of the case studies discovered in the initial survey by redesigning them utilising the results discovered in the second stage.   Alternatively, develop a plug in (or similar) which could provide helpful advice or highlight key information.",Evaluate current digital contract agreements and facilitate overcoming barriers to genuine informed consent.,1,1   Easy,"Produce a set of guidelines/suggestions, with empirical support and evidence, to improve genuine informed consent in digital contracts.",8,0,t
4481,Sloppy passwords and security awareness: can a little nudge help?,ug4,2094,2020 02 21 23:42:43 00,1,"Useful (i.e. effective) passwords are often not very usable.   Or rather many recommendations for creating passwords are not very human friendly and therefore people are discouraged from adopting them.   Suggestions are often driven by machine algorithms (character length, at least one symbol, mix of upper and lower case, include a number, etc.) and neglect many human aspects or pragmatics (memory limits, bias, limited imagination, laziness).   Not all human factors need to be seen as limitations or negative traits.   This project aims to see if human behavioural negative bugs can be turned into positive features.   In particular, can small things in the way websites display and present their interface for entering a new password lead to improvements in security?Typically, the interface design when asking someone to generate a username and password for a new account looks very similar to the one required just to log in.   Does this limit more imaginative and unique password creation?   Even  change password  facilities do not often appear much different.   The standard approach of entering their new password twice as well as their old password may simply encourage people to modify the original in a trivial way (priming, memory and repetition effects), especially if the process is not user initiated but instead because the system forces a change, say, after a fixed length of time. Even in situations where there is visual validation or online feedback, such as indicating a strength of  weak  to  very strong , this may result in a more systematic incremental approach with higher predictability, e.g. keep adding another  1  until the status changes to  strong  and then that's good enough.As well as recognising these possible weakness in user design, can similar factors be exploited to encourage humans to choose better passwords?   Richard Thaler's  Nudge Theory  suggests this should be possible, although it is not uncontroversial (despite resulting in a Nobel Prize for behavioural economics).   After an initial survey of password generation interfaces, the project will involve testing if more subtle changes and implicit features are enough to produce improved choices in passwords or if more explicit influences are required (think along the lines of explainable artificial intelligence).   For example, if you can induce a slightly threatening or paranoid environment, will people automatically adopt a more cautious approach?   Would creating a obvious panopticon effect, where there is always the possibility of being observed, produce an increased desire for privacy and hence a more secure password?   On the other hand, make people feel too nervous or uncomfortable and they may well leave the website and abandon the entire process.   Previous studies have shown that the presence of a set of eyes (even just a picture) is enough to encourage people to behave better.   Do findings similar to the  watching eyes effect  transfer into the cyber security domain?",Improve how humans generate passwords for personal online user accounts.,2,2   Moderate,A series of theory driven suggestions for improving the human generation of passwords will be tested and ranked in terms of success (and possibly cumulative effects).,8,0,f
4479,An Interactive Tool for Ethical Thinking,ug4,7119,2020 02 21 22:20:32 00,1,"It is becoming more and more commonplace that companies and researchers are (rightly) expected to consider the ethical dimensions of the work they do. Investors and funders will often include evidence of ethical decision making as a requirement on supporting a project. But Ethics is a rich and complex field, and expecting scientists or engineers with no training to be able to give it proper consideration is unreasonable, not to mention ineffective.Text based guides do exist for helping think through ethical decisions, as well an  existing web/mobile app:  https://www.scu.edu/ethics app/This project will draw inspiration from these to produce an updated app or web interface, then hopefully expand upon it with:Evidence collection / storage for reporting  A wider range of supported topicsReminders / other tools for revisiting the same ideas over the course of a projectLearning recommendations for topics, to tailor the questions to the user's particular projectIt is not expected that a student will address all of these in a single project, though an MInf student might try them all over two projects.If you're interested in this project (or self proposing in this area) send an email to james.garforth@ed.ac.uk with your relevant experience to discuss further or arrange a meeting.",Build a tool that helps companies or researchers think through ethical decisions,1,1   Easy,An interactive tool that guides users through ethical decision making.,8,0,t
4483,Privacy context game in human agent systems,ug4,7139,2020 02 22 15:28:19 00,1,"Managing privacy in online systems is a difficult task, and it would be ideal to develop artificial intelligence (AI) techniques  such as agent based models   that can preserve the privacy of the users automatically. But first we should understand privacy expectations of humans better.Understanding privacy preferences of the users is a challenge in online systems such as IoT systems or online social networks. Various studies show that privacy preferences of the users vary based on particular scenarios [1]; hence, sometimes it is difficult to generalise such preferences. Context depends on many factors such as location, people and such [1]. However, it is a difficult notion to model since a user can be involved in various contexts at the same time [2]. In this project, the goal is develop a game where agents and humans collaborate together to find out contexts of users in pictures. First, a set of images describing various privacy scenarios will be built. Second, a game will be designed to find out privacy contexts in the images. The agents will try to discover hidden information in the images, and guide the user to determine various contexts in these images. If you are interested in the project, you should send me an email to arrange a one to one meeting. The deadline to send me an email is 20th of March.",Development of a game to understand privacy contexts as perceived by humans,1,2   Moderate,Development of a game to understand privacy contexts as perceived by humans,8,0,f
4485,Sources of variability in neural activity,ug4,1344,2020 02 22 21:56:09 00,1,"Using a multiple, parallel pathways, the retina informs the brain about visual stimuli [1]. With  dense, large scale microelectrode arrays,  we can now record activity from thousands of neurons in the retina simultaneously [2].In this project we will use such recordings to investigate variability in neural responses during light stimulation [4]. Variability refers to the difference of a response of a neuron (or a group of neurons) when the same stimulus is shown multiple times. This is also called trial to trail variability. It is observed in all sensory systems, and is often viewed as a limiting  factor, or an intrinsic source of noise, in perception and cognition.  We found very slow (time scale of minutes) fluctuations in retinal activity (in our recordings  and in those from other labs), which may contribute to the observed variability. Here you will quantitatively assess  these fluctuations, and evaluate their contribution to trial to trial fluctuations. In particular, we are interested  to see if this explains previous  findings summarised in reference 3.If you are interested in this project, please read references 3 and 4, and make sure you are comfortable with with the relevant maths and analysis. This project is classed as hard as it will require learning fundamental concepts in neuroscience (NC and CCN courses are relevant).",Evaluate the contribution of very slow fluctuations to variability of recorded neural activity,1,3   Hard,"Quantify slow activity fluctuations in recordings from the retina (assess this effect in multiple recordings), and evaluate effect on trial to trail fluctuations. Results should be interpreted in the context of neural coding theory (ref 4).",8,0,f
4456,Reproducibility and provenance for neuroscientific data analysis,ug4,1344,2020 02 21 15:10:11 00,1,"A key tool in neuroscience is the recording of neural activity with extracellular probes [1]. These provide a raw signal, which has to be processed and analysed with machine learning methods before interpretative analysis can be done. A main problem is that such analysis pipelines are often implemented ad hoc, have many parameters, and a zoo of different methods and approaches currently exist. This often makes it hard to reproduce published results even when the raw data is available.We have developed SpikeInterface, an open source software framework to automate and standardise such analysis:https://github.com/SpikeInterfaceThis allows the user, with a few lines of code, to carry out complex analysis. In a next step, we plan to add a mechanism that automatically documents and stores details about every step in an analysis pipeline, so that it is possible to fully reproduce analysis.In this project, you will research and implement a method for provenance capture in SpikeInterface. References 2 and 3 provide possible starting points, and python decorators may be way to implement this (but we encourage creativity). Note that a functioning solution may, if approved by the SpikeInterface team, become part of SpikeInterface, and you will be credited as contributor.I will arrange a meeting for all students interested in my projects, where I will give more information, in particular relating to difficulty and required background. Note that all of my projects require reasonable programming skills, ideally in python. I will generally mark students declaring interest in my projects as 'suitable' unless there is a very strong reason not to do so. So please be sure you are comfortable with the topic and difficulty before making your selection.",Develop a system for provenance capture and integrate this in a framework for electropysiology data analysis,1,1   Easy,"Implement and test a system for provenance capture in SpikeInterface, and demonstrate its usefulness in a real world application.",8,0,f
4486,Topics in workflow management for an Industry 4.0 manufacturing floor,ug4,1933,2020 02 22 22:24:48 00,1,"The DigiFlow project is an Industry 4.0 project focusing on the digitization, monitoring, and optimization of industrial workflows, with a combination of IoT location sensors, Cloud infrastructure and our workflow technologies.As part of the project, we have created a model of the factory floor of an Italian company that manufactures pens. We provide live monitoring capabilities, including analytics, timelines, contextual information, alerts, as well as simulation to help optimize the schedule for the factory's machine operators. This project is aimed at developing new features in the context of DigiFlow. These may include:Complex event processing from live location dataNovel analytics and insights from workflow monitoring dataManagement of errors and deviationsAn interactive platform for scheduling and simulationVisualization of simulation analyticsOnly students who have discussed the project with the supervisor will be considered.Please include evidence of your suitability in the contact email, including (but not limited to):A short CVInformation on the courses you have taken and marks obtained so farExamples of related work you have done","To use live monitoring and simulation data to develop new features for workflow management, with applications in the manufacuring domain",2,Variable,Development and evaluation of a major feature involving data analysis in the DigiFlow platform.,8,0,f
4487,Detection of time required to find study space in the Main Library using WiFi data,ug4,1933,2020 02 22 23:00:58 00,1,"Finding empty space to study in the Main Library can be challenging, particularly in busy periods such as during exams. The University has recently started collecting WiFi data in an attempt to measure levels of occupancy in different floors and rooms. This includes anonymous timestamped logs of when a device enters or exits a particular Access Point.The aim of this project is to analyse this data in an effort to detect movement patterns in the Library and identify users that eventually settle down in a particular study space. This data will then be used to provide indications or predictions of how long it may take to find an empty study space at any given time. Due to the sensitive nature of the WiFi data in terms of user privacy, the project will require the recruitment of consenting participants for the data gathering phase.Only students who have discussed the project with the supervisor will be considered.Please include evidence of your suitability in the contact email, including (but not limited to):A short CVInformation on relevant courses you have taken and marks obtained so farExamples of related work you have done",Analysis of WiFi data from the Main Library to detect and predict the time required to find empty study space,1,3   Hard,Development and evaluation of an algorithm to measure and predict the time required to settle in a study space using WiFi data,8,0,f
4488,Incrementing the MarkEd online marking tool with support for fairer and more efficient marking,ug4,2097,2020 02 22 23:07:42 00,1,"Marking is undoubtedly one of the most important teaching activities . It is a central part of assessment, which is crucial to foster learning as well as the relationship between teachers and learners as stated in the University of Edinburgh's latest Taught Assessment Regulations [1]. One of the principles that must guide marking is fairness, as all students should be judged equally in terms of academic performance. Fairness requires consistency both for one marker and across markers and that is difficult to achieve. In addition, factors such as tiredness or mood could affect marking fairness. To make things worse, the emotional aspect adds burden to this activity, especially for beginner markers. Fairness is intended to be improved through moderation by the course organiser, however moderation cannot ensure it is met 100%. Because of time constraints, efficiency must also guide marking. The situation is again worse for beginner markers, whose time allocated for marking may be set up by senior colleagues with more experience. In the School of Informatics, this is the case for teaching support staff whose hours to spend in a marking role are determined by course organisers when setting up this role.Getting the right balance between fairness and efficiency in marking is quite challenging, especially since they may require contradictory activities. For example, being too quick in marking may lead to an incorrect decision about the student's performance and thus an unfair mark, while investing the maximum time to ensure fairness may make marking infeasible time wise.MarkEd is an online tool (website) for marking, feedback and moderation that currently being developed by undergraduate students. For now, it does not include considerations of marking fairness and efficiency. The aim of this project is to collect strategies, both manual and computer aided, to ensure both fairness and efficiency in marking, and then design, develop and evaluate an increment to the current design of MarkEd that would support users in being more fair and efficient.  The following are a possible series of steps for this project:Review literature on strategies, both manual and computer aided, to ensure fairness and efficiency in markingConduct user studies with academics and teaching support staff in the School of Informatics to also understand what they are doing to try to be fair and efficient.Pick up a subset of strategies from 1) and 2) and come up with a proposed increment to the MarkEd tool design (probably using a design tool like Figma [2]) that would support users in being more fair and efficient in their marking.Evaluate the proposed changes to the design with academics and teaching support staff in the School of InformaticsImplement some or all of the proposed changes to the design in MarkEd, and evaluate them again with academics and teaching support staff in the School of InformaticsIf interested in this project, please fill in this  Google form.  There is no need to contact us for this project. We will arrange to meet with you for a before the deadline for choosing projects.","The aim of this project is to collect strategies, both manual and computer aided, to ensure both fairness and efficiency in marking, and then design, develop and evaluate an increment to the current MarkEd online marking tool (under development in 2019 2020) that would support users in being more fair and efficient.",1,Variable,"A review of strategies, both manual and computer aided, to ensure fairness and efficiency in marking. An understanding of what academics and teaching support staff in Informatics do to be fairer and more efficient in their marking. An increment to the design of the MarkEd online tool (probably using a design tool like Figma) to support users in being more fair and efficient. An evaluation of this design with academics and teaching support staff in the School of Informatics.An implementation of some or all of the design in  MarkEd. A final evaluation.",8,0,f
4490,Deploying workflows on a blockchain,ug4,1933,2020 02 23 13:16:05 00,1,"Blockchains and their associated smart contracts offer a unique opportunity towards automated and trustworthy Business Process Management, particularly across organisations [1]. However, the need for high levels of quality assurance, correctness and verification is a major challenge.The application of existing approaches in formal verification to smart contracts can help alleviate some of these concerns. Our logic based WorkflowFM system [2], for example, can be used to develop correct by construction process models that offer higher levels of trust by default.This project will focus on investigating ways to deploy verifiable process workflows, such as those generated by WorkflowFM (or similar), to the blockchain. This may involve the following:A literature survey of recent work on blockchain workflow automationDevelopment of a workflow execution engine using smart contractsEvaluation of the prototype using existing models from real world workflows (such as from the healthcare and manufacturing domains)A critical review of the benefits, challenges, and limitations of such an approachOnly students who have discussed the project with the supervisor will be considered.Please include evidence of your suitability in the contact email, including (but not limited to):A short CVInformation on the courses you have taken and marks obtained so farExamples of related work you have done",Development and critical evaluation of a workflow execution engine using smart contracts,1,3   Hard,"A working prototype execution engine for workflows described in process calculus terms using smart contracts, sample workflows, a critical evaluation of such an approach",8,0,f
4491,Improved Workflow Execution in WorkflowFM,ug4,1933,2020 02 23 15:07:19 00,1,"In recent work we developed WorkflowFM, a logic based system for formally verified process modelling [1]. It has been used to model workflows for a number of use cases in various areas, including healthcare processes (such as patient transfers and care pathways) and manufacturing flows.The system can currently be accessed via an interactive, graphical, interface. Users can construct workflows diagrammatically and the system can then produce the corresponding code in the form of concurrent processes in pi calculus [2]. These are then translated to Scala [3], based on a custom execution library. In effect, this allows users to go all the way from a diagrammatic flow chart to an executable system with no code deployment.The current execution engine included in WorkflowFM was developed as a rapid prototype. This naturally introduced inefficiencies which are particularly noticeable in existing case studies where thousands of workflows are simulated at the same time.This project will seek to produce improvements in the implementation, for instance aimed at better term management and substitution. Any improvements will have a direct impact in actively used systems, such as Digiflow, for the live monitoring and scheduling of manufacturing processes.Only students who have discussed the project with the supervisor will be considered.Please include evidence of your suitability in the contact email, including (but not limited to):A short CV  Information on the courses you have taken and marks obtained so farExamples of related work you have done",The improvement in efficiency of the WorkflowFM execution engine,1,3   Hard,The implementation of improvements in the WorkflowFM pi calculus execution engine. The evaluation of the efficiency improvements using an existing dataset of workflows.,8,0,f
4492,Extending a Linear Logic proofs as processes embedding with units,ug4,1933,2020 02 23 15:33:25 00,1,"In recent work we developed WorkflowFM, a logic based system for formally verified process modelling [1]. It has been used to model workflows for a number of use cases in various areas, including healthcare processes (such as patient transfers and care pathways) and manufacturing flows.WorkflowFM relies on Linear Logic and the proofs as processes paradigm [2] to specify and compose processes. Each process is a linear logic sequent with terms representing its inputs and outputs. Performing inference in the logic then corresponds to composing 2 processes together.The reasoning engine is implemented as an embedding of Linear Logic in the theorem prover HOL Light [3]. The embedding includes a series of algorithms to perform more complex inference for intuitive process composition [4].The current embedding only includes the Multiplicative Additive fragment of Linear Logic. This means it can express and reason about parallel and optional inputs and outputs.The aim of the project is to extend the formalisation with units. These will allow expressing and reasoning with processes that have no inputs and/or no outputs. The reasoning algorithms will also need to be extended to be able to handle the new, more expressive logic.The project will provide an opportunity to develop familiarity and experience with theorem proving and logic inference in an applied setting and links to real world applications in healthcare and manufacturing.Only students who have discussed the project with the supervisor will be considered.",The extension of an existing Linear Logic proofs as processes embedding and its associated reasoning algorithms with units,1,4   Very Hard,"An extended linear logic embedding in HOL Light, to include units. Functioning extensions of the reasoning algorithms to be able to handle processes with no inputs and/or no outputs.",8,1,f
4493,Web based blocks world experiments for studying language evolution,ug4,1393,2020 02 23 16:41:09 00,1,"Human languages evolve from one generation to the next, and this project will help us understand the forces that drive that evolution. Concretely, this project involves updating a multi player web/javascript game based on http://shrdlurn.sidaw.xyz/ which will be used to understand how goals, context, and statistical information interact to shape the structure of human language. In this game, one player is able to put different colored blocks into spatial patterns, but doesn't know the target pattern. The other player knows the target pattern, but cannot move the blocks directly. Instead, that player must communicate the pattern to the first player using sequences of symbols, across a variety of different challenges, which may vary in difficulty. The players need not be playing simultaneously: Player 1 can give instructions that will be given to a Player 2 who arrives later. In some cases, a player may first receive instructions from an earlier player, and give instructions to a later player.It is expected that the software development/implementation phase will be relatively short, given that there's an existing code base. The bulk of the project will involve running experiments to test the hypothesis that player converge to efficient communication systems. Specifically:* shorter codes will be used for more common patterns;* codes will capture the compositional structure of the patterns, e.g., statistical dependences;* codes will reflect the players' goals. For example, if green and blue blocks are always indistinguishable with respect to victory conditions, then codes will tend not to distinguish between those blocks.","To develop and use a web based game for studying the evolution of human language, building on existing tools.",3,2   Moderate,"The project will be complete if the game works and can be used to run shrdlurn style (see references) tasks using several human participants (including  chains  of participants as described in the references). It must be: * Flexible - the games must be easily specified using configuration files and useable with multiple web frameworks/servers and deployable on a DICE machine. It should be possible to configure the game to use a wide variety of symbols for inter player communication, and a wide variety of game types (e.g., different victory conditions). * Robust - the game must be useful for collecting data in real world settings, and reliable store analyzable data. * Documented - the game must should be documented in enough detail that a third party can run the game and deploy it on a new web server. The experiments must be designed to answer a well defined scientific question, and conducted with a set of participants recruited through Amazon Mechanical Turk (the supervisor will handle participant recruitment). This will involve experimental design and pre registration, pilot experiments, data collection, and data analysis.",8,0,f
4495,Developing a new web interface for the Archive of Formal Proofs,ug4,1339,2020 02 23 17:44:16 00,1,"The Archive of Formal Proofs (AFP)  [1]  is a collection of proof libraries, examples, and large scientific developments, mechanically checked in the theorem prover Isabelle [2]. It is curated by Isabelle proof developers in a way that is similar to a scientific journal for formal proofs.  Although the AFP is generally usable and provides a vital way of exploring extensive libraries of Isabelle proofs, there seems to be much scope for coming up with an improved web platform, both in terms of look and feel but also in terms of functionalities.   A "";minimal viable product""; would need to be at least as good as the current AFP. However, a good project would substantially improve upon the existing framework and, aside from providing a more dynamic website, would look into implementing new features in order to improve navigation and search for the library.  More advanced features could involve (automatic) intelligent linking to external sources such as Wikipedia, DBLP and arXiv, rendering of papers and other documents (currently in PDF) within the website (e.g. in a way similar to  arxiv vanity [3, 4]), tagging, etc. Creative ideas from the student  will be essential.  Note: This project is marked as moderate but could become hard if sophisticated functionalities are considered. Any interested student must discuss the project with the supervisor in order to be considered for it.  Note: No student will be  considered for this project after the 12th of March.  ",Designing and implementing a new website for the AFP that incorporates new functionalities for exploring its libraries,1,2   Moderate,A  new web platform for organising and exploring formal libraries stored in the Archive of Formal Proofs,8,0,f
4496,Incrementing the MarkEd online marking tool with support for good feedback,ug4,2097,2020 02 23 18:42:39 00,1,"Offering students feedback is an essential component of assessment, as it can help students check understanding and progress in a course, clarify expectations, offer encouragement, identify where they can improve and be constructive as to how. For course teams, the feedback provided to students can itself represent feedback about teaching style, approaches, instructions, and frequent areas of confusion, all of which should be continuously improved on to deliver a better course. For the university, student answers to the yearly National Student Survey about the feedback they have received in their studies help attain and maintain standards regarding student support, create a good image and thus attract new students.However, offering good feedback is not something that a marker is always born with, and some markers are better at providing feedback than others. What constitutes  good' feedback may also differ on the educational system, and thus markers with different nationalities may come with preconceptions about this when taking their first marking role in our school. While there is a large body of literature on what constitutes  good' feedback in the UK higher education system (e.g. [1]), markers may not take the time to read it before starting marking work. Informatics teaching support training offers some advice on  good' feedback, but not all markers attend it. There is, of course, also the issue of time: marking is usually a very time consuming activity, and writing large chunks of feedback may not be feasible to markers in the time allocated for it.As part of a current undergraduate project, two students currently are developing an online tool (website) named MarkEd, which will help with marking, feedback and moderation processes in our school. The current tool does not include any considerations of the quality of the feedback provided (it only records feedback). The aim of this project is to review what constitutes  good' feedback, and design, develop and evaluate an extension to the MarkEd tool which supports markers in providing  good' feedback. The following are possible steps in tackling this project:Review educational literature on what constitutes  good' feedback, especially in the UK higher educationDiscuss with academics in the School of Informatics about the different flavours of feedback that they require/expect their hired markers to produce, and their views on what would make such flavours of feedback  good'Come up with a proposed increment to the MarkEd tool design (probably using a design tool like Figma [2]) that would support markers in providing better feedback. This could involve, for example, building a guide to good feedback to bring up based on the analysis of the feedback markers are providing to students, or re structuring the way feedback is required of the markers, or building training on feedback provision for the markers.Evaluate the proposed changes to the design with academics and teaching support staff in the School of InformaticsImplement some or all of the proposed changes to the design in MarkEd, and evaluate them again with academics and teaching support staff in the School of Informatics","The aim of this project is to review what constitutes good feedback, and then design, develop and evaluate an increment to the current MarkEd online marking tool (under development in 2019 2020) that would support markers in providing better feedback to their students.",1,Variable,"A review of what constitutes 'good' feedback, especially in the UK higher education. An understanding of what academics in Informatics require in terms of feedback, and what they consider to be 'good' feedback. An increment to the design of the MarkEd online tool (probably using a design tool like Figma) to support markers in providing better feedback. An evaluation of this design with academics and teaching support staff in the School of Informatics. An implementation of some or all of the design in MarkEd. A final evaluation.",8,0,f
4497,Automatic punctuation annotation using Transformers,ug4,1329,2020 02 23 20:55:06 00,1,"Speech recognition systems typically produce an unpunctuated stream of words as output.   However breaking the stream into sentences and adding punctuation marks is important for both human readability and further automatic processing (e.g. machine translation),   We have previously explored a variety of approaches to adding punctuation to the output of a speech recogniser [1], in particular approaches treating the problem like neural machine translation mapping a sequence of unpunctuated words to a sequence of punctuation symbols [2].   Since that work was done Tramnsformer neural networks using self attention have become the state of the art approach to sequence to sequence modelling [4].The aim of this project is to use Transformer networks for punctuation annotation (see [3] for some recently published work in this area).   After developing baseline systems, which can be compared to results in the literature, the project could be extended in a number of ways including exploring prosodic features and exploring ways in which more compact Transformer models could be used (e.g. [5]).",Development of an automatic system to add punctuation to speech transcriptions using Transformer neural networks,2,3   Hard,Construction of a Transformer based system for punctuation annotation and a set of further experiments exploring a further research question (e.g. use of prosody),8,0,f
4498,Developing Sample Efficient Deep Reinforcement Learning Methods for Robots in the Real World and Simulation,ug4,7254,2020 02 23 21:01:24 00,1,"The advent of deep learning enabled many recent breakthroughs in the field of robotics, especially for locomotion [8] and manipulation tasks [1, 2, 6, 7]. The goal of this research oriented thesis will be to develop and apply a novel deep reinforcement learning algorithm on continuous control tasks, i.e. tasks with continuous state (joint position/velocities or images) and action spaces.  Depending on interests and skills the possible topics you could work on are:    Using evolutionary methods in the context of reinforcement learning for co evolving the behaviour and morphology/design of robots to allow fast evolution in the real world [3]    Using model based reinforcement learning methods for fast adaptability and autonomous discovery of dynamic behaviour using small datasets    Combining model free and model based reinforcement learning methods for faster learning processes (see an overview here [9])    Developing novel approaches for the safe exploration of unseen states/actions throughout the learning processThe goal of this thesis is to enable you to work on state of the art research questions in the field of reinforcement learning and robot control. While you will receive the guidance and help necessary to succeed in this research endeavour, self motivation and initiative are necessary assets.This project will be supervised by Kevin Luck and Michael Mistry. Please contact Kevin Luck ( ksluck@ed.ac.uk ) for any questions about this project.",Develop a new or extend an existing reinforcement learning approach for continuous control tasks,1,3   Hard,Literature review. Develop a new or extend an existing deep reinforcement learning approach and/or apply it to a novel scenario. Compare your approach to an existing algorithm/baseline on a simulated task (OpenAiGym [5] /PyBullet [4] etc). (Optional but desired) Evaluate your method on a robot in the real world,8,0,f
4499,ML driven buffer management in PostgreSQL,ug4,5175,2020 02 23 21:45:59 00,1,"This research project focuses on a hot topic of marrying database management systems and machine learning. The goal is to observe the behaviour of the buffer manager of PostgreSQL under test workloads, and based on detected patterns, learn predictive models for temporal and spatial data prefetching. These learned models would then be used instead of the existing buffer replacement policies in PostgreSQL, and their effectiveness will be experimentally analysed.This project will add functionality to the real code of PostgreSQL. You need to delve into the actual code. A strong programming background is mandatory, and experience with C   is desirable.",Implement novel ML based buffer replacement policies in the PostgreSQL database system,1,2   Moderate,Collect page requests for different workloads in PostgreSQL. Build few predictive ML models based on the collected data. Integrate these models with the PostgreSQL buffer manager. Experimentally evaluate these models.,8,0,f
4500,Hypertree decompositions,ug4,5175,2020 02 23 21:47:56 00,1,"This project is for students interested in theory, in particular graphs and databases.The goal is to implement an algorithm that, given the hypergraph of a conjunctive query, determines the (fractional) hypertree width and computes a hypertree decomposition of that (minimal) width.  The algorithm should be implemented in Java or Scala.  The research paper containing an approximation algorithm for finding a fractional hypertree width, which we would like to implement:  W. Fischl, G. Gottlob, R. Pichler, "";General and Fractional Hypertree Decompositions: Hard and Easy Cases"";, PODS 2018More information:1) https://en.wikipedia.org/wiki/Tree_decomposition2) Slides on trees, tree width, and (fractional) hypertree width: http://www.cs.bme.hu/~dmarx/papers/marx bat trees.pdf",Implement an algorithm for computing a hypertree decomposition of a database query,1,2   Moderate,finished algorithms for computing the fractional hypertree width and the matching decomposition,8,0,f
4501,Parallel database query processing using OpenCL,ug4,5175,2020 02 23 21:49:31 00,1,"This project investigates the potential of using OpenCL, a framework for writing heterogeneous programs, for parallel processing of database queries. The project will consists of writing several OpenCL programs for increasingly more complex database queries, from simple aggregate (e.g., count) queries computed over a single table to aggregate queries joining multiple tables. The project will include an extensive experimental evaluation of a set of database queries on different parallel architectures (e.g., CPUs, GPUs, FPGAs).Prerequisites: solid programming skills, knowledge of C  ",Implementing a handful of database queries in OpenCL,1,2   Moderate,successful implementation of at least 4 different database queries in OpenCL,8,0,f
4502,An experimental study of tree based index data structures,ug4,5175,2020 02 23 21:56:03 00,1,"This project will study existing tree based indexing data structures (e.g., Bw tree, B  tree, Boost) and experimentally evaluate their performance. This evaluation aims to provide deeper insights about the behaviour of these tree based indexes under different types of workloads using performance profiling tools (e.g., Valgrind). The student will then implement a tree based index optimised for using in real time (streaming) environments. ",Head to head comparison of existing tree based indexes,1,1   Easy,Performance analysis of existing 4-5 tree based indexes. Successful implementation of a custom tree index.,8,0,f
4504,App for Gathering Experiment Data on Set Recommendation for Multiagent Coordination (iOS Front End),ug4,3865,2020 02 24 04:19:40 00,1,"Sharing economy applications need to coordinate humans, each of whom may   have different preferences over the provided service. Traditional approaches model this as a resource allocation problem and solve it by identifying matches between users and resources. These require knowledge of user preferences and, crucially, assume that they act deterministically or, equivalently, that each of them is expected to accept the proposed match. This assumption is unrealistic for applications like ridesharing and house sharing (like airbnb), where user coordination requires handling of the diversity and uncertainty in human behaviour [1].The aim of this project is to produce an App Front and Back End that will fascilitate data gathering in sharing economy applications, when users are recommended sets/lists of options, and where each individual user's (independent) choice affects the outcome for all (or a subset of all) users. A specific problem can be chosen to guide the development, but the Application Programming Interfaces (APIs) should be kept as generic as possible.This project focusses on developing an iOS Front End.Similar   projects have been put up for Android Front End and a Back End. The project can be taken at an individual level or as a group project, where students can coordinate their actions. If this is not possible, then the student would be expected to develop a simple Back End to demonstrate App functionality.","To program a phone app that will recommend users with sets/lists of items, allowing for easy data gathering from user selections of items in those sets/lists. Focussing mostly on the iOS Front End.",1,1   Easy,"Program a backend that will: output set/list recommendations, anticipate user selections (and error/cancellation signals) from the app, compute 'solutions' from user selections, inform users of their final 'solutions', store information from past interactions.  Program a phone app (Android and/or iOS) that will: receive sets/lists of items from the backend, present the information to users, allow the users to interact with the lists (at least allow for selecting an item), send this information to the backend, anticipate and receive final 'solutions' from the backend. Compile a Demo from your work.",8,0,f
4505,App for Gathering Experiment Data on Set Recommendation for Multiagent Coordination (Back End),ug4,3865,2020 02 24 04:20:16 00,1,"Sharing economy applications need to coordinate humans, each of whom may   have different preferences over the provided service. Traditional approaches model this as a resource allocation problem and solve it by identifying matches between users and resources. These require knowledge of user preferences and, crucially, assume that they act deterministically or, equivalently, that each of them is expected to accept the proposed match. This assumption is unrealistic for applications like ridesharing and house sharing (like airbnb), where user coordination requires handling of the diversity and uncertainty in human behaviour [1].The aim of this project is to produce an App Front and Back End that will fascilitate data gathering in sharing economy applications, when users are recommended sets/lists of options, and where each individual user's (independent) choice affects the outcome for all (or a subset of all) users. A specific problem can be chosen to guide the development, but the Application Programming Interfaces (APIs) should be kept as generic as possible.This project focusses on developing the Back End.Similar projects have been put up for Android Front End and iOS Front End. The project   can be taken at an individual level or as a group project, where students can coordinate their actions. If this is not possible, then the   student would be expected to develop a simple App Front End to demonstrate Back End functionality.","To program a phone app that will recommend users with sets/lists of items, allowing for easy data gathering from user selections of items in those sets/lists. Focussing mostly on the Back End.",2,1   Easy,"Program a backend that will: output set/list recommendations, anticipate user selections (and error/cancellation signals) from the app, compute 'solutions' from user selections, inform users of their final 'solutions', store information from past interact. Program a phone app (Android and/or iOS) that will: receive sets/lists of items from the backend, present the information to users, allow the users to interact with the lists (at least allow for selecting an item), send this information to the backend, anticipate and receive final 'solutions' from the backend. Compile a Demo from your work.",8,0,f
4503,App for Gathering Experiment Data on Set Recommendation for Multiagent Coordination (Android Front End),ug4,3865,2020 02 24 04:19:17 00,1,"Sharing economy applications need to coordinate humans, each of whom may   have different preferences over the provided service. Traditional approaches model this as a resource allocation problem and solve it by identifying matches between users and resources. These require knowledge of user preferences and, crucially, assume that they act deterministically or, equivalently, that each of them is expected to accept the proposed match. This assumption is unrealistic for applications like ridesharing and house sharing (like airbnb), where user coordination requires handling of the diversity and uncertainty in human behaviour [1].The aim of this project is to produce an App Front and Back End that will fascilitate data gathering in sharing economy applications, when users are recommended sets/lists of options, and where each individual user's (independent) choice affects the outcome for all (or a subset of all) users. A specific problem can be chosen to guide the development, but the Application Programming Interfaces (APIs) should be kept as generic as possible.This project focusses on developing an Android Front End.Similar projects have been put up for iOS Front End and a Back End. The project can be taken at an individual level or as a group project, where students can coordinate their actions. If this is not possible, then the student would be expected to develop a simple Back End to demonstrate App functionality.","To program a phone app that will recommend users with sets/lists of items, allowing for easy data gathering from user selections of items in those sets/lists. Focussing mostly on the Android Front End.",2,1   Easy,"Program a backend that will: output set/list recommendations, anticipate user selections (and error/cancellation signals) from the app, compute 'solutions' from user selections, inform users of their final 'solutions', store information from past interactions.  Program a phone app (Android and/or iOS) that will: receive sets/lists of items from the backend, present the information to users, allow the users to interact with the lists (at least allow for selecting an item), send this information to the backend, anticipate and receive final 'solutions' from the backend. Compile a Demo from your work.",8,0,f
4506,Web color palettes using MCMC with people,ug4,1393,2020 02 24 11:13:40 00,1,""";Markov Chain Monte Carlo with People""; (MCMCP) is an experimental method for learning about people's concepts.This project is about adapting MCMCP to understand how people feel about combinations of colors, and to identify palettes that are appropriate for a particular purpose, e.g., a specific website or slots in a CSS file.It will involve A. Implementing a simple web application using html/javascript with the following features:* Render example sites side by side different ensembles of colors, determined by parameter vectors.* Allowing people to submit hundreds of choices about the better of the two sites, according to some criterion they have been given.* Storing user choices in a database and making them available for subsequent data analysis.B. Running user studies:* designing experiments, including participant instructions and debriefing information.* securing ethical approval from the informatics ethics committee* recruiting participants using Amazon's Mechanical Turk (AMT) or another service.* pilot testing the web application and its interface with the recruitment service.* planning data analyses before data have been collected.C. Data analysis:* Using visualizations Bayesian or frequentist inferential statistics to determine whether/how the project's methods succeeded or failed, and any lessons learned.This project is easy in the sense that other students with no programming experience and limited statistical training have successfully completely similar projects, and no familiarity with MCMCP or other concepts outside the core informatics curriculum is expected. That said, it will likely involve a substantial amount of work.","Design, use, and analyze the results of a web application to elicit information about how people think about color palettes on web sites",1,1   Easy,"Completing items A, B, and C in the project description above.",8,0,f
4507,Understanding font preferences using MCMC with people,ug4,1393,2020 02 24 11:24:39 00,1,""";Markov Chain Monte Carlo with People""; (MCMCP) is an experimental method for learning about people's concepts.This project is about adapting MCMCP to understand how people feel about fonts, and to identify fonts that are appropriate for a particular purpose, e.g., an advertisement, a party promotion, or academic writing.It will involve A. Implementing a simple web application using html/javascript with the following features:* Render example documents side by side with different fonts, determined by parameter vectors. This may require some initial work with metafont or another parametric font rendering tool.* Allowing people to submit hundreds of choices about the better of the two sites, according to some criterion they have been given.* Storing user choices in a database and making them available for subsequent data analysis.B. Running user studies:* designing experiments, including participant instructions and debriefing information.* securing ethical approval from the informatics ethics committee* recruiting participants using Amazon's Mechanical Turk (AMT) or another service.* pilot testing the web application and its interface with the recruitment service.* planning data analyses before data have been collected.C. Data analysis:* Using visualizations Bayesian or frequentist inferential statistics to determine whether/how the project's methods succeeded or failed, and any lessons learned.","Design, use, and analyze the results of a web application to elicit information about how people think about fonts.",1,2   Moderate,"Completing items A, B, and C in the project description above.",8,0,f
4404,A front end for the probabilistic model checker Storm,ug4,1324,2020 02 19 16:07:17 00,1,"Storm is an analysis tool for studying systems where randomness or probabilist phenomena play an important role.  Given a logical formulae and a quantitative model it can determine if the model satisfies the formula, thereby allowing developers to check important safety properties of the systems which they are building.  Storm allows modellers to specify their model in a range of formalisms, including Markov automata, Petri nets, dynamic fault trees, and reactive modules.  However, our interest here is "";cpGCL"";,  the conditional probabilistic Guarded Command Language, an extension of Edsger Dijkstra's language of guarded commands which includes probabilistic choice and conditional observations.Storm is a modern, efficient implementation of a probabilistic model checker but it is run at the command line, detached from an IDE which would make model construction easier through providing language support for the cpGCL language.   (Other input formalisms such as Petri nets and reactive modules have dedicated IDEs, but cpGCL does not.) The idea of the project is to add language support for cpGCL in Eclipse through the use of Xtext/Xtend, a framework for developing programming languages and domain specific languages.   With such support in place it would be possible to investigate extensions of the cpGCL languages, including a variant with a Java based syntax which would be easier for Java developers to adapt to, or a variant with Kotlin based syntax which would be suitable for Kotlin developers.The front end developed for cpGCL would allow modellers to interpret their models before submitting them to the Storm model checker as a first pass aimed at detecting bugs in the cpGCL model before applying model checking to the model.",To design a front end which makes the Storm probabilistic model checker easier to use.,1,1   Easy,An Eclipse application for editing cpGCL models which connects to the Storm model checker.,8,0,f
4509,Secure mobile applications using Trusted Execution Environments,ug4,1332,2020 02 24 11:50:05 00,1,"The reliable protection of sensitive operations and valuable assets is the utmost priority of any security critical application. A challenging task arises when dealing with mobile platforms, whose highly extensible and open nature, increases the chances of a system breach. The rapid growth of the mobile market has led to unprecedented demand and development of mobile applications. The MobileOS market share surpassed traditional desktop operating systems earlier this year. This fact reflects the diversion of the users preference towards smartphone or tablet platforms for their everyday activities like communication, entertainment, and financial transactions. Users have unrestricted access to a wide range of applications they can easily install, without taking the security implications of their systems into consideration.Device and OS manufacturers, on the other hand, have to ensure that their devices meet all the functional and safety requirements without affecting the overall user experience. This open ended approach significantly increases the devices susceptibility to threats, and key stakeholders have raised concerns about the potential security issues that could arise. Recent reports showed an all time high in mobile infections in 2016 and revealed a 400 percent increase in smartphone malware attacks.Various defensive approaches have been proposed, mainly focusing on using tamper resistant hardware for providing security services and storage. The lack of standardisation as well as the rigid implementation process, constitute such techniques impractical and expensive. A recent alternative is the use of a Trusted Execution Environment (TEE). A TEE could simply be described as a technology that allows the parallel execution of two operating systems (OS) on a single device. The Trusted OS hosts Trusted Applications (TA) that provide security related services to Client Applications (CA) running in the Rich OS. The main idea is to decouple and isolate the security sensitive operations, like key management and transaction signing, from the rest of the application. This project aims to examine the benefits and limitations of Trusted Execution Environments as currently implemented in ARM TrustZone chips. You will evaluate its limitations by implementing some complex sensitive applications such as a @FAor an EMV compliant payment protocol",Secure a mobile application Using Trusted Execution Environments,3,3   Hard,"1. Prototype implementation and evaluation of the selected application with the core cryptographic functions implemented as TA. 2. Securing the TA through the TUI. 3. Bind the Client application with a high level, user friendly Android application. Theoretically using the Adroid NDK API.",8,0,f
4510,Formal analysis of E cclesia,ug4,1332,2020 02 24 11:51:00 00,1,"E cclesia is a decentralized self tallying e voting protocol that could be implemented on the blockchain. The protocol is based on the ideas of Zerocoin and Time Lock Encryption schemes. E cclesia is formally proven to preserves voter privacy. Given the public setting of the protocol, we believe that a proof of (individual and universal) verifiability would be possible and not too different from the treatment for privacy, though the need for incorporating verification requests could make it more complex. A less explored avenue for potential future work would also be in verifiability of eligibility (as opposed to only satisfying eligibility as part of the correctness properties). The goal of the project is precisely to formally analyse E cclesia with respect to verifiability type properties. ",The goal of the project is precisely to formally analyse E cclesia with respect to verifiability type properties,1,3   Hard,1. Either prove that E cclesia is end to end verifiable: provide a formal proof. 2. Or disprove that E cclesia is  end to end verifiable: provide an attack and propose improvements to the scheme.,8,0,f
4511,User studies on Bitcoin wallets,ug4,1332,2020 02 24 11:51:32 00,1,"Goals of the project:Compare by hash  is a common technique used in sensitive applications to ensure integrity of data. Such a mechanism requires from its user to manually inspect two strings of random alphanumerical characters and confirm their equality. Requiring the user's input is a measure for ensuring protection against man in the middle attacks   two strings not matching may indicate that some malicious party has tampered with one of them. Protocols relying on this compare by hash technique thus need also be usable as their whole security often rely on the users' performance on the hash comparison. The goal of this project is to study the usability and security of compare by hash based mechanisms as deployed in real world applications. In particular, if funded this project will look at the usability and security of hardware bitcoin wallets. Hardware bitcoin wallets are meant to be secure even if the host environment is infected with malware trying to tamper the payment details that have been provided by the user. To that end, companies have introduced a second factor authentication mechanism to ensure that the payment details have not been tampered by an attacker.   The baseline mechanism requires the user to confirm the payment details before the wallet processes them. The hypothesis of such authentication mechanism is that a user confirmation of the transactional details ensures the validity of the transaction.   However, as such mechanism is entirely based on the user's capability to identify a tampered transaction,   the success rate of the security mechanism is analogous to the user error rate. You will evaluate the usability of one (or more) hardware bitcoin wallets such as KeepKey, Trezor, Digital Bitbox, and Ledger Nano S. More precisely, you will study study the success rate of this mechanism with regards to a particular implementation approach and identify the best solution for ensuring the integrity of the transaction details. Methodology:The effectiveness of using hash comparison as a way of ensuring the validity of a transaction boils down to the user's capability of identifying potentially tampered data. There are several factors that may impact the user's precision, such as time limitation, the way the hash is presented as well as the percentage by which the hash has been tampered. The goal of this initial study is to identify the factors that influence the security guarantees of this approach, with respect to the human error. To do so, you will design test scenarios which you will test with real users. These scenarios will include different attack cases in which the adversary is able to tamper different parts of a given hash. For instance, altering the whole string versus parts of it. This will help us identify at what percentage the user is able to address a change of the hash. To achieve a broad usability test on human subjects, you will conduct your experiments through Amazon Mechanical Turk. This service allows to conduct large scale experiments in a distributed way across the internet. The users are given a task to complete, which in our case will be the confirmation of Bitcoin payments, and a time frame for that task. The task will consist of a list of Bitcoin payments that they need to make and they will ask to confirm these payments. A set of these payments will be intentionally tampered. As such, the users will be asked to confirm the originality of the tampered payments. In case that a user fails to identify the faulty payment details, this will add on the failure rate of the compare by hash technique that is currently tested. Finally you will compare the conditions that changed and contributed to the user accepting a faulty payment",Evaluate Hardware Bitcoin wallets with respect to usable security,1,1   Easy,Development of a web platform where a simulation of the usage of hardware wallets occurs. Then the platform will be used to perform a large scale study.,8,0,f
4512,Improving a formal verification tool for cyber physical systems,ug4,1363,2020 02 24 11:55:29 00,1,"Cyber physical systems are systems which involve computational elements interacting with the physical world in real time.Cyber physical systems are ubiquitous in society today and our safety frequently depends on them. Examples include all kinds of transport systems (cars, boats, trains, planes, spacecraft), robots, chemical plants, and medical devices (e.g. heart pace makers and life support systems).There has been significant research interest recently in developing formal verification techniques that can be used to ensure safety and other desirable properties of cyber physical systems.  In formal verification we mathematically prove system correctness, so ensuring higher degrees of confidence in a system than if we use testing or simulation based verification approaches.One interesting current tool for formally verifying cyber physical systems is Flow*.  When verifying a system with Flow*, one starts with a description of some initial set of states.  Flow* then automatically generates a `flow pipe' from these initial states, a description of the set of states this initial set evolves into at each future point in time.  A system is then certified as safe if this flow pipe never intersects some designated set of `bad' states, states in which something bad happens or the system can be said to have failed.This project can pursue some combination of activities, depending on the interests and background of the student.  Ideas for activities include:Improving performance.  Recent work by one of my PhD students has  highlighted a class of problems on which Flow*'s performance is very  poor.  Adjusting Flow* options seems to make some difference, but it  is very difficult to figure out how to adjust these options.Improving instrumentation.  Instrumentation has to do with adding  code to log intermediate aspects of Flow*'s computations so one can  understand how well or poorly the computations are proceeding and  get guidance on how to change its options.  Extending its capabilities.  An ongoing PhD project has suggested  needs for further capabilities that might be realisable in the time  of an MSc project.  Developing documentation.  While Flow* comes with some  documentation, it is unclear what some options actually do.  Code  could be explored to discover this.  Further, any extension of its  capabilities would require first some understanding and record of  its current architecture.Improving code quality.  Some parts of the code are rather  repetitive.   Refactoring these parts could be a very useful and  necessary first step to take before trying any extensions to the  code. Flow* is written in C  , making extensive use of templates, so good familiarity with C   or Java and Java generics is highly desirable.  Extending a library of examples and evaluating Flow*'s performance on them.    Evaluation on a variety of examples is essential for understanding the tools limitations and ways in which it needs to be improved.    Examples need not only be drawn from the domain of cyber physical systems, but could come from other domains such as systems biology or synthetic biology.Any student interested in taking on this project, is strongly recommended to first talk with the proposer to check if their background is appropriate and gain further information about the project.","To improve some combination of features (e.g. performance, capabilities) of the Flow* formal verification tool for cyber physical systems.",2,3   Hard,Significant contributions on at least two of the activities listed above.,8,0,f
4513,AI for game theory,ug4,5572,2020 02 24 11:55:30 00,1,"Game theory is the study of strategic interactions among (supposedly rational) decision makers. In the past decades it has been found to have applications in many fields, from economics and finance to healthcare and politics. Experimental game theory is a branch of this discipline in which human subjects are asked to play games and their behaviour is then analysed. In most games, it is found that people use unexpected strategies that imply that they do not play rationally. More recently, game theory has started to be tackled by computer scientists and the field of algorithmic game theory has dealt with developing algorithms and AI systems to perfectly play highly complex games.In this project, we want to merge experimental and algorithmic game theory: that is, students will explore famous games using AI, and develop intelligent agents that will act like humans in such games. Developing such agents will allow us to understand why humans play these games the way the play, and which features of the human behaviour contribute to their strategies the most. Interestingly, this project will mostly focus on repeated games (where players play against each other multiple times) and games on networks (where players are only connected to some of the other players), which are the most interesting scenarios as they show complex, unexpected emerging behaviour.The games we will use include (but are not limited to):  Prisoner's dilemma  Public Goods game  Ultimatum game  Naming gameNo prior knowledge of game theory is strictly required, but it would be a plus. A mark of 60 or higher in Informatics 2D is required.Practical information  Four students will work on this  project.  Despite the topic being the same, each student will work on their own idea and with their own choice of methods (to be discussed with me) to produce original work. However, I highly encourage the students who will be selected to help each other, share ideas, share papers and reading lists, etc. Students are still expected to conduct most of the work by themselves, but collaboration is still an important aspect of the project!  The project difficulty is "";variable"";, to reflect the flexibility of the project. Each student will decide what to focus on, and the difficulty they would like to have.    I will have 1v1 meetings with ALL the students who express their interest until March 22nd. After that, meetings will be scheduled subject to availability. Students MUST attend the meeting to be marked suitable.",To reproduce existing results in game theory using intelligent agents instead of human subjects,4,Variable,Develop agents that can reproduce existing game theory experiments.,8,0,f
4514,Exploring the capabilities of the Lean interactive theorem prover,ug4,1363,2020 02 24 11:59:53 00,1,"The Lean interactive theorem prover [1] is a new theorem prover which has been attracting significant attention [6,7] in the last 5 years.  Key features include:A significantly richer type theory than current popular theorem  provers such as Isabelle/HOL, introduced in the Automated Reasoning  course.  A rich type theory is useful for precisely and elegantly  capturing mathematical concepts, making the maths easier to read and  understand.   The type theory is most similar to that used by the Coq theorem prover and in the same family as that used by the Agda tool.  The incorporation and collection together of a novel set of  procedures for supporting the elegant input language and automating  reasoning.Good support for classical logic.  Before Lean, most theorem provers  with rich type theories required mathematics to be formulated in a  constructive logic, a logic which does not provide the law of the  excluded middle (For any proposition P, either P holds or its  negation holds).  Classical logic underpins virtually all the maths  that those with good mathematical backgrounds are familiar with.The aim of the project is to explore the prover's capabilities by undertaking a case study development using as a starting point an existing development in some other interactive theorem prover such as  Isabelle/HOL, Coq or Agda.   Possible topics include, but are not limited to:Some topic in maths, perhaps from algebra, analysis or number theory.   Several academics in maths departments are currently exploring translating undergraduate course materials into Lean, and Lean's mathematical library [2] is rapidly expanding.Programming language semantics.   The project might take as a starting point formal textbooks developed for Isabelle [3], Coq [4] or Agda [5] theorem provers.Software verificationHardware verificationProtocol verification (e.g. cache coherence protocols)Any student interested in listing this project as one of their choices is strongly recommended to first meet with the proposer to discuss the project further and check if their background is appropriate.",To explore through a case study the capabilities of the Lean interactive theorem prover.,2,3   Hard,Production of a new development in the Lean theorem prover. An evaluation of the development process and of the qualities of the development.,8,0,f
4515,Evaluation and comparison of static software verification tools,ug4,1363,2020 02 24 12:05:03 00,1,"The standard approach to verifying that software has been implemented correctly is to run a series of tests and check that the observed behaviour is as expected.      In virtually all cases it is impractical to exhaustively test software for all possible inputs, so testing usually cannot guarantee that software is bug free and does exactly what it should.    In contrast, static software verification tools analyse the logical structure of programs and provide mathematical guarantees that programs have certain desired properties and/or fully meet their specifications.    Traditionally static verification techniques have been difficult and expensive, and their use has mostly been justified only for applications that are safety or security critical.   In recent years there has been much improvement in static verification techniques and they are starting to be used more widely.  This project concerns a class of tools where the user writes logical formulas to describe expected behaviour and the tools use fully automated reasoning engines to check whether programs have the expected behaviours or not.       Some such tools target standard languages such as C [6], Java [5] or the SPARK Ada subset [7].   Others such as Why3 [1] and Dafny [2]  work with simpler languages and can be easier to learn and use.   The basic aim of the project is to explore the ease of use of one or two of these tools: the eventual hope of the researchers developing these tools is that virtually any programmer should be able to use such tools, so that they could be used in undergraduate courses on algorithms and data structures, for example.   These tools are not quite there yet and one project goal is identifying ways in which these tools still need to be improved.    The proposed approach is to first work through tutorials on one or two of the tools and then to apply each tool in turn to one or more common case study programs.    The choice of case study programs is open.   For example, they could be algorithms the student has covered in previous courses.     It is not necessary for the chosen case study program(s) to be particularly complicated.   In total they may have no more than a few dozen lines of code.  The project does not require the student to understand how these tools work: it is fine if they are treated as black boxes.   If the student has an interest in automated reasoning, the project could optionally explore how the tools work and/or look at how proof assistants such as Isabelle/HOL [3] or Coq [4] can be employed to interactively complete more difficult verification proofs.      If these options are pursued, the difficulty of the project rises from `Easy' to `Moderate', `Hard' or even `Challenging'. To make the project tractable, very likely it would focus on program verification using a single tool.   To be suitable as an MInf project, use of some proof assistant would be necessary.",To assess the ease of use of current static software verification tools,2,1   Easy,The basic goal is to statically verify some case study program using one or two of the tools and to compare and discuss the respective verification efforts.  Further goals include  explaining something about how the tools work and/or completing program verification tasks that requires the use of a proof assistant.,8,0,f
4516,Simulation of an existing quantum algorithm emulating an unknown quantum process on a classical device,ug4,1332,2020 02 24 12:36:17 00,1,"The Quantum Emulation Algorithm (QEA) is an algorithm that outputs the action of an unknown unitary matrix on a new state using some known inputs and outputs states of that unitary. The goal of this project is to simulate a simple version of this algorithm for two and three qubit unitary matrices on a simulator platform. The first stage of the project consists of getting acquainted with the algorithm and understanding its circuit and gates. In the second stage, you need to decompose some of the gates to standard single and double qubit gates. The third stage will be to develop the code of the circuit on a simulator platform that also incorporates noise in the quantum algorithm and finally, in the last stage, the output fidelity (benchmark of system's performance) will be calculated and analysed for different cases. This project will be an interesting way of having a quantum computing experience from learning a quantum algorithm to implement it on an existing quantum simulation language.","Given some knowledge of an unknown quantum process (specifically a unitary matrix) of a certain size, the objective is to benchmark the performance of an existing quantum algorithm to emulate such a process on new data. The benchmark requires simulating this algorithm on one of the existing simulator platforms.",1,2   Moderate,Developed code of the quantum algorithm on the simulator platform together with the benchmark analysis for different cases,8,0,f
4517,Formal Proofs of Cryptographic Protocols,ug4,1331,2020 02 24 12:44:43 00,1,"Multi Party Computation is an cryptographic framework for solving problems (such as voting or auctions) where several parties jointly compute a function of their inputs, while keeping inputs private from one another.   The function is computed by exchanging a series of message between the parties.   A classic example is a group of millionaires who wish decide which of them is the richest without learning one another's actual wealth.   At the end of the protocol, all of the millionaires know which one of them is the richest.   Security proofs of these protocols follow the simulation paradigm, in which an ideal world (secure by construction) is shown to be indistinguishable from the real world in which the messages of the protocol are exchanged.In recent PhD work (by David Butler), we have extended a framework for reasoning about cryptography inside the Isabelle/HOL theorem prover, to be able to construct rigorous computer checked proofs of cryptographic arguments previously written on paper (potentially clarifying confusion or mistakes in the paper proofs).The goal of this project (or group of projects) is to consider some extensions to this framework, for example, to add new formalisations of protocols we haven't yet considered, or to develop detailed example case studies.   The difficulty of such projects will depend on exactly what is tackled and how much is achieved; some of the simplest examples would be Easy but most cases are Moderate tending to Hard/Very Hard.   There will be a fairly steep learning curve depending on the background courses you have taken; it is not recommended to select this project if you haven't taken the courses listed.A further project variant is to work on a system to automatically extract programs from the Isabelle proofs    this will be challenging in a different way, involving software engineering (potentially a bit of Standard ML programming), as well as proof engineering.",To build security and correctness proofs inside the Isabelle theorem prover extending a framework for Multi Party Computation,4,Variable,"As agreed with the supervisor, according to specific instances of the project.  This may be a suite of examples with definitions and proofs, a new MPC protocol or protocol construction, or some successfully extracted programs to implement MPC examples.",8,0,f
4518,Visual tools and extensions for Attack Trees,ug4,1331,2020 02 24 12:45:36 00,1,"Attack trees are a simple notion used for threat modelling.   A basic attack tree is an AND OR tree whose root is the goal of an attacker.   The attack is decomposed into steps which must (AND nodes), or may as alternatives (OR nodes), be carried out to achieve the attack.   The leaves are actions that may help lead to an attack.   Edges in an attack tree can be labelled with probabilities or costs, to reflect risk modeling.Various extensions of attack trees have been studied in the academic literature, for example, modelling defences as well as attacks, and distinguishing between steps which must be carried out in order from those that can occur in any order.   But so far the real world use of these extensions has been limited; current commercial tools used in industry present a more spreadsheet oriented view, losing the rich semantic information about the structure of attacks.The aim of this group of projects is to: study a new modular structuring notion, which may help build larger and more realistic attack trees, and extend the research prototype tool ADtool, to add hierarchical modules or other featuresexperiment and design other attack tree variations and alternative modelling tools and visualisationsThe notion of hiearchical attack tree is being developed in ongoing research.   The core idea of this project is to work with the researchers to help evaluate the design of hiearachical attack trees, by implementing an extension to ADtool, a graphical modelling tool implemented in Java, to add modules (boxes around subtrees), so large trees can be broken up into pieces, possibly recursively (nested modules).   The user of the tool should be able to zoom into and out of boxes, or fold/unfold them.For students interested in theoretical foundations, there may also be a chance to contribute to the development of the underlying theory of the tool (which could extend the difficulty to HARD/VERY HARD).     The underlying theory is being developed by combining the approaches mentioned in the academic papers cited on hiearchical proof trees [2] and attack trees with sequencing [3].    Understanding these papers completely is not needed for a programming only version of this project.For students more interested in the programming side of this project (rated EASY), one variation will be to focus on developing new layout and visualisation algorithms (extending to MODERATE/HARD depending on sophistication) and their implementation in other frameworks.   One appealing framework to extend is D3.js.","Study extensions of Attack Trees with nested modules, extend a Java based (or design your own) modeling tool to implement modules, zooming or other features",2,Variable,"An extension of ADtool (or another framework) which implements operations corresponding to the features being implemented, for example, to ""box"" a collection of nodes, and too ""zooming"" in and out by opening/collapsing the box.",8,0,f
4519,Chase Termination for Linear Existential Rules,ug4,1370,2020 02 24 12:58:42 00,1,"The chase procedure is considered as one of the most fundamental algorithmic tools in database theory. It takes as input a relational database D, and a set of existential rules (a.k.a. tuple generating dependencies) R, and it constructs a (possibly infinite) instance J that satisfies R. The chase procedure has been successfully applied to different database problems such as data exchange, and query answering and containment under constraints, to name a few. One of the central problems regarding the chase procedure is all instances termination, that is, given a set of existential rules R, decide whether the chase under R terminates, for every input database. It is known that this problem is, in general, undecidable [1]. However, it has been shown that for some special classes of existential rules, the above problem is decidable [2]. More precisely, if we focus on linear existential rules, a central class of existential rules with several applications, the problem is NL complete (assuming predicates of bounded arity). The algorithm for establishing the NL upper bound relies on a graph based syntactic characterization of the fragment of linear existential rules that guarantees the termination of the chase. The goal of this project is to provide an effective implementation of the chase termination algorithm in question.",Implementation of a graph based algorithm for deciding whether the chase algorithm terminates under a set of linear existential rules,1,2   Moderate,1. An effective implementation of the chase termination algorithm for linear existential rules. 2. Experimental evaluation of the implemented algorithm.,8,0,f
4520,A Software Tool for Recognizing Rule based Ontology Languages,ug4,1370,2020 02 24 12:59:16 00,1,"It is widely accepted that existential rules form a natural and convenient way for modeling ontologies. However, if we allow arbitrary existential rules to occur in an ontology, then the basic reasoning tasks, such as satisfiability and query answering, are undecidable. This has led to a flurry of activity for designing decidable ontology languages based on existential rules; see, e.g., [1, 2, 3]. The main ontology languages rely on three decidability paradigms: guardedness, acyclicity and stickiness. The goal of this project is to develop a software tool that assists users to design ontologies by (i) checking whether a given rule based ontology complies with any of the above decidability paradigms, and (ii) if not, to help the user to modify the ontology in such a way that it complies with at least one of the above paradigms.","To develop a software tool that checks whether a given rule based ontology, which is essentially a first order theory, falls in a known ontology language",1,2   Moderate,"Develop a software tool as described above for the main rule based ontology languages (i.e., linear and guarded existential rules, (weakly )acyclic existential rules, and (weakly )sticky existential rules)",8,0,f
4521,Query Rewritability for Rule based Ontology mediated Queries,ug4,1370,2020 02 24 13:00:10 00,1,"In ontology mediated querying,  ontologies are used to enrich incomplete data with domain  knowledge which results in more complete answers to queries.  Among KR researchers there is a clear consensus that the  required level of scalability in ontology mediated querying can only be achieved  via query rewriting, which attempts to reduce the problem into the  problem of evaluating an SQL query over a relational database. The goal of this project is to implement and experimentally evaluate the standard resolution based rewriting algorithm that has been proposed in [1].",Implement and experimentally evaluate a query rewriting algorithm for rule based ontology mediated queries,1,2   Moderate,"A correct implementation of the rewriting algorithm, and a thorough experimental evaluation of it.",8,0,f
4522,Linear Existential Rules with Transitivity,ug4,1370,2020 02 24 13:00:59 00,1,"Ontological query answering is an important problem that naturally appears in several different scenarios such as ontology based data access. Given a relational database D, an ontology O (which is essentially a first order theory), and a Boolean conjunctive query Q, the question is whether the knowledge base consisting of D and O entails Q, or, in other words, whether every (possibly infinite) model of D and O is also a model of Q. In its full generality, the above problem is undecidable. However, there are several well behaved ontology languages that make the above problem decidable. Such an ontology language is linear existential rules [1]. Unfortunately, linear existential rules are not powerful enough for expressing transitivity, a key modeling feature for knowledge representation purposes. The problem of ontological query answering under linear existential rules extended with transitivity has been recently investigated in [2]. However, the problem is not fully resolved since the decidability result of [2] holds only if we apply certain syntactic restrictions either on the ontology or the conjunctive query. The goal of this project is to close the above challenging open problem.","The goal of the project is to fully resolve a challenging problem, that is, ontological query answering under linear existential rules extended with transitivity",1,4   Very Hard,Close the problem of ontological query answering under linear existential rules extended with transitivity.,8,0,f
4523,Anonymous Messaging on a RaspberryPiNet,ug4,5142,2020 02 24 14:21:32 00,1,"Protecting network communications from powerful adversaries and criminals is challenging. Even if the contents of messages are hidden using encryption, the meta data, such as the identities of recipient and sender or message size, leak a rich set of information.  Low latency Anonymous Communication Networks like Tor provide meta data protection, however, they suffer from timing correlation attacks and can not resist a global adversary. An alternative approach known known as mix networks provide stronger protection at the cost of some reduced performance and functionality.The goal of this project is to take the Katzenpost implementation of the Loopix mixnet design and deploy it on a network of raspberry pis.  ",To deploy a mixnet on a network of Paspberry Pis and tune it using network telemetry data,2,3   Hard,Set up the mix network on a small local network composed of Raspberry Pis. The student should produce a test suite (by writing client software that interfaces with the network) that provides empirical evidence that messages are flowing and being delivered properly.,8,0,f
4524,Optimizing mix network security parameters,ug4,5142,2020 02 24 14:27:53 00,1,"Mix networks are anonymity networks that allow individuals to privately communicate and prevent powerful adversaries (such as governments and large corporations) learning about communication patterns of the users. These networks have been refined over a number of years and are now practical to field. However, one of the "";last mile""; obstacles is that it is difficult to know exactly how to tune the security parameters of the mix network (such as the amount of message delay and the amount of cover traffic) for best security and performance results.This project will look at developing techniques (e.g. using linear optimization, machine learning, etc.) to produce a solution to the problem.Network simulators to produce datasets to analyze will be provided.",Produce an optimization functionality that allows a system administrator of a mix network to optimally tune message delay and cover traffic for the most efficient network operation.,2,Variable,"Minimum: A design of an optimizer, with analysis of its properties in terms of optimal privacy and performance. Better: An implementation of the above and empirical results to back up the analysis above. Best: A design that is robust to adversarial attempts to manipulate the optimizer with malicious inputs.",8,0,f
4526,Machine learning for modelling cell metabolism,ug4,6535,2020 02 24 16:14:41 00,1,"Cellular metabolism is a complex web of biochemical reactions that convert nutrients into energy needed to sustain life. These reactions depend on parameters that control the speed at which chemical substrates are converted into one another. Yet measuring such parameters is extremely time consuming and requires carefully designed experimental setups. In this project you will explore the use of machine learning techniques to reconstruct metabolic models from multi omics datasets that can be routinely acquired with modern experimental methods. The goal is to use supervised learning to infer metabolic time series from snapshot data, as well as bayesian inference to estimate metabolic parameters. Throughout the project you will become familiarised with state of the art methods recently proposed in the literature, implement them on published datasets, and explore potential improvements in performance and accuracy. The project will be co supervised by Dr Linus Schumacher from the MRC Centre for Regenerative Medicine.The student will join the Biomolecular Control Group,  co located at the Schools of Informatics and Biological Sciences, and will benefit from a diverse and multidisciplinary research environmental at the interface of biology and computation.Interested students should contact Diego OyarzÃºn to arrange a meeting.",Implement machine learning methods to infer metabolic models from large dimensional data,1,3   Hard,Implementation of a Python/Matlab package to infer metabolic dynamics from proteomics and metabolomics data.,8,0,f
4527,A machine learning approach to model whole bacterial cells,ug4,6535,2020 02 24 16:28:45 00,1,"In this project you employ supervised learning to predict the growth of microbes from DNA sequence data. Using a recently published dataset (the largest of its kind), we will build models for cellular growth rate and compare the performance of various learning methods such as generalized linear models and neural networks. The project will produce cutting edge methods for growth prediction, with many applications at the interface of biotechnology and biomedicine. The outcomes of the project will be useful in virtually any discipline where bacterial growth rate is of importance, in particular in biotechnology applications when  bacteria are genetically engineered to produce therapeutic proteins.The student will join the  Biomolecular Control Group,  co located at the Schools of Informatics and Biological Sciences, and will benefit from a diverse and multidisciplinary research environmental at the interface of biology and computation.Interested students should contact Diego OyarzÃºn to arrange a meeting.",Build supervised learning models for the growth of microbes,1,3   Hard,Build working ML models of bacterial growth rate in python and/or matlab,8,0,f
4528,Privacy Research Artefacts Evaluation,ug4,5142,2020 02 24 16:41:01 00,1,"Scientific research produces knowledge that is written up and published. Sometimes datasets, source code, or data processing scripts (the research artefacts) are also made available for use by other researchers in the community. While this is commendable, one can not depend on these artefacts working. It would be very helpful to have someone verify these artefacts for their usefulness. The privacy community has started this initiative with an artefact review[1]. There are many perspectives from which to evaluate an artefact, such as the licencing terms, the correctness of its output, or the versions of the dependencies.The aim of this project is to 1) develop an methodology to evaluate privacy artefacts that have already been released in the public domain and then to 2) evaluate 10 (to be agreed upon) artefacts using this methodology.  ",To evaluate a set of a wide variety of privacy research artefacts based on a methodology proposed by the student.,3,1   Easy,"Develop methodology for evaluating privacy artefacts. Evaluate 10 artefacts of various types (source code, data sets, scripts, VM & Docker images, etc).",8,0,f
4529,Apply existing prefetching techniques to TLB for large data centre applications,ug4,7765,2020 02 24 18:23:26 00,1,"Modern data centre services which running on massive dataset typically experience frequent memory access with poor locality. Hence it cannot take advantage of performance improvement brought by TLB and results in a large number of misses. To handle these TLB misses, the system has to carry out long page walk i.e. go through multiple levels of page table to figure out the address translation, which significantly impairs the system performance. In fact, TLB miss handling has been shown to constitute as much as 40% of execution time and up to 90% of a kernel's computation [REF1]. Current studies on improving TLB usually make use of large pages but few have looked into prefetching technique which is commonly used in cache. The best study on TLB prefetching dates back to 2002  [REF1].A recent project [REF2] on exploiting opportunities for TLB prefetching shows interesting patterns on memory access for big data applications which can be used to construct prefetcher for TLB. Hence this project, building on its result, explores the possibility of applying existing prefetching techniques to TLB and aims to build a TLB prefetcher.",Explore possibilities of building TLB prefetcher,1,Variable,Analyse the feasibility of applying current prefetching techniques on TLB and build a prefetcher based on observed memory accessing pattern.,8,0,f
4530,Firefighter games,ug4,1320,2020 02 24 19:09:16 00,1,"Wednesday 11pm (GMT): I have had a large number of applications for this project, and am not going to consider any new 'expressions of interest'. It's not fair to the students who got in touch ages ago.Firefighting games use a spreading fire as a metaphor for any spreading process on a network, such a disease or a rumour.    We will investigate the use of firefighting games to understand disease control.   In the classical firefighter game on a network, one player (the fire) attempts to escape control by the other player (the firefighter).    The nodes of the network represent places or individuals that can be infected (`on fire'), and the links between then the contacts that transmit the disease.    The fire chooses a place to start, and then on each of its turns spreads to every unburnt node adjacent to a burning node.    Once a node is burning (or, in our case, infected), it can never be extinguished.    The firefighter attempts to protect the network by protecting (or, in a disease version of the game "";vaccinating'') a fixed number of nodes per turn.    A protected node stays protected forever, and can never be infected. The firefighter's objective is to protect as much of the network as possible.The study of firefighter games has grown to include a number of variants, including more powerful firefighters who can protect multiple nodes at each time, and probabilistic versions in which the fire spreads imperfectly.    A major focus has been on identifying on which networks the firefighters can protect most of the network, and strategies to achieve this.    In our case that is equivalent to identifying when we can vaccinate to protect most of the population.    In general, this question cannot be answered efficiently (many many variants of the problem are NP complete), but there are some types of networks with helpful structural properties on which we can efficiently find optimal firefighter strategies, or determine that the network cannot be protected.Students would implement and test exact and heuristic approaches to finding good firefighting sets on networks derived from real world datasets, potentially including networks from the SNAP Stanford network repository, and British agricultural datasets.    Alternatively there is some scope for theoretical work for someone who might prefer that  For this project, a student should have strong programming skills.    An interest in graphs and networks or epidemiology is required.   ",To develop and run experimental code investigating the suitability of burning number and firefighter games as a tool for epidemiological monitoring of networks.  There is also the potential for a theoretical project for anyone who would prefer that.,3,Variable,Appropriate experimental or theoretical results on firefighting on classes of graphs related to real world networks.,8,0,f
4531,Developing tokens for Erasure Encoding methods for protecting time streamed data,ug4,1320,2020 02 24 19:09:41 00,1,"This project has its original motivation from the world of public health and environment   the concern being privacy issues surrounding the aggregation and management of streamed data generated from participating households in a health environment study in the US.   One of the goals is to store user data in a decentralised filesystem in a distributed (and duplicated) manner, in order to achieve two goals   to protect an individual's data from being available to an adversarial "";node""; of the network, and also to provide some redundancy to protect against network failures.    Systems already exist to achieve these goals, and one popular approach involves the "";Erasure Encoding""; protocol, which is discussed in detail in reference [1] below.   The most well known of these methods is Tahoe LAFS, described in reference [2].   This is open source software which can be downloaded and set up on an independent network (not the goal of this project!).In these erasure encoding of the methods, a stream of data will end up split into a number of small blocks which are distributed across the network.   The user looking for their own data will need to recover sufficiently many of these in order to reconstruct their own set of data streams.   As things stand the user receives a long string of random numbers to encode the details of where to access the small blocks.   The aim of this project is to design a token scheme to encode the necessary details of these small blocks, and then realise this in an implementation.     This proposal has been developed in discussion with William Waites of the School of Informatics, and Zoltan Nagy of the University of Texas, and we will include them in discussions from time to time.     ","To develop a ""token"" scheme for erasure encoding methods for the case of time streamed data",1,3   Hard,Design and implement a token scheme for an erasure encoding protocol for time series data.,8,0,f
4532,A Java IDE allowing the editing of code and UML simultaneously.,ug4,7115,2020 02 24 22:26:15 00,1,"  This project aims to explore the concept of combining a traditional Java development environment workflow (for instance using IntelliJ IDEA or Eclipse) with a UML diagram in a way that may allow an experienced software developer to more easily visualise what relation the code they're writing has with the rest of the software system.Here's a mock up of a potential user interface that could allow the engineer to make system refactoring and understanding others' code quicker than a traditional file based IDE:Both the UML and the Java would update if the other was modified. Primitive UML editing options would be avaliable for rearranging classes, creating relationships, and modifying abstractions. It is not known whether this interface and workflow would create an improvement to development times over more traditional file based software editing, and so a user study will likely be required to evaluate the success of the project.Creating a fully fledged editor is a very ambitious task. However, a small proof of concept should be able to be built with both simple text and UML editing features and the ability to compile down to a .jar for execution. It may be possible to build this functionality as a plugin to an existing IDE, so that avenue could also be explored if it looked like it would allow the project to be condensed to a more manageable workload.",Provide a steamlined development environment combining the high level abtraction of UML and a traditional Java IDE.,1,3   Hard,"A minimal, proof of concept development environment where existing Java projects can be loaded, edited and saved. The Java should be editable in the UML interface. Both the UML and the Java code should update if the other is modified. Primitive UML editing options should be avaliable for rearranging classes, creating relationships, and modifying abstractions. Ability to compile to an executable .jar file.",8,0,f
4533,"Algorithms for computing equilibria in submodular games, and games with strategic complementarities",ug4,1353,2020 02 25 11:17:20 00,1,"Submodular games (also known as supermodular games, when the players aim to maximize their payoff, instead of minimize their cost), are a class ofgames where the players all have strategy sets which are partially ordered (lattices), and such that   the cost/payoff functions of the players exhibit submodular/supermodularin a suitable sense.     These games were studied originally by Topkis (1978), who showedthat such games always have a pure Nash equilibrium   (whereas of course general games, even finite games, need not have any pure NEs, only mixed ones).A closely related, slightly more general class of games, called   "";games   with strategic complementaries""; were studied later by Milgrom and Roberts (1990),who similarly showed they always have a pure equilibrium. For both classes of games, roughly speaking,   for each player i, if other players "";increase""; their strategy (relative to their own partial order), then player i is never incentivised to "";decrease""; its strategy, reletive to its own partial order.      In this sense, the game exhibits "";strategic complementaries"";.   Such "";strategic complementarities"";   occur naturally in many interaction scenarios that can be usefully modelled via game theory. Indeed, such games have been used very extensively by, to model many economic phenomena such as "";oligopoly"";, certain forms of bargaining,bank runs,   and in other settings, such as arms races, etc.   There are known iterative algorithms for converging to a pure Nash equilibrium, and more generally all pure Nash equilibria, in   such games   with strategic complementarities.   Unfortunately, these algorithmscan theoretically be very slow.         The question is whether these algorithms are nevertheless efficient enough in practice, or whetheralternative algorithms can be much more efficient.The aim of this project is to implement several iterative algorithms for   finding a pure Nash equilibrium (and all pure NEs) in      submodular/supermodular gamesand games with strategic complementarities, and to experiment with these algorithms on a variety of examples, to see how they perform in practice.(A very very strong project would actually involve novel new more efficient algorithms for these games, but this is not a requirement for the project  assuming the standard algorithms perform well enough in practice.)","The main of this project is to implement algorithms that are as efficient as possible for computing a pure Nash equilibrium in submodular games, and games with strategic complementaries",2,Variable,"Functioning, reasonably efficient (in practice only) algorithms for finding: (a) a (any) pure Nash equilibrium in a game with strategic complementarities"", (b)finding all pure NEs in such a game.    Experimentation with a variety of classes of games (taking these from the literature), to assess the performance of the algorithms on examples, and how the performance ""scales"" with the size of the examples.",8,0,f
4534,implementing efficient (randomized) algorithms for comparing integers succinctly represented by arithmetic circuits,ug4,1353,2020 02 25 11:20:13 00,1,"Consider the following trivial sounding problem:we are given two integers   n and m, and we want to decide whether the numbers are equal or not.This is trivial to do if the numbers n and m are represented in binary.However, for this project,   the numbers n and m are not represented in binary notation.     Instead, they are represented more succinctly, by   arithmetic circuits (straight line programs),   that use addition and multiplication (and binaryintegers as inputs to these arithmetic circuits).The only known efficient (polynomial time) algorithm for this problem when both addition and multiplication are allowedrequires the use of randomization (and finding a deterministic algorithm would constitute a major breakthrough, because itwould de randomize "";polynomial identity testing"";).The randomized algorithm is simple:     randomly choose a "";suitably large enough""; prime number p , and   compute botharithmetic circuits mod that random prime number.       Then compare the result (mod p) of the two circuits.       If these are equal,then answer "";yes"";.   If it is not equal then answer "";no"";.       If the two circuits define equal numbers n and m, then this algorithm always answers "";yes"";.     If the two circuits define different numbers n and m, then this algorithmanswers no "";with high probability"";.In a second scenario,     imagine that the two arithmetic circuits representing m and n     ONLY use multiplication.In this case we do have an efficient (polynomial time) deterministic algorithm for comparing the integers.(See [Etessami Stewart Yannakakis'14]), which is an iterative algorithm that requires repeated computation of the greatest common divisor of pairs of integers (which can of course be computed efficiently).Furthermore, for numbers n and m reprsented by circuits using only multiplication,       assuming certain deep number theoretic conjectures are true,there is also an efficient (polynomial time) deterministic algorithm for deciding whether     n is greater than or equal to m.     ( This algorithm requires   computation oflogarithms to desired accuracy, and also relies on the deterministic algorithm for checking equality. (See [Etessami Stewart Yannakakis'14]).This project asks to implement all three of these algorithms, and to see how they perform in practice by   testing them on many instances of integers defined by arithmetic circuits.","the goal is to implement a basic (in fact classic) randomized algorithm for checking equality of two integers when these integers are each represented succinctly by arithmetic circuits using addition and multiplication,     and also implementing a deterministic algorithm for equality checking in the case where only multiplication is used.",1,1   Easy,Working implementation of at least two of the three described algorithms for comparing integers represented by arithmetic circuits.,8,0,f
4535,reachability algorithms (both qualitative and quantitative) for Branching Markov Decision Processes and Branching Stochastic Games,ug4,1353,2020 02 25 11:20:50 00,1,"Markov decision processes are a fundamental model of stochastic dynamic optimization and optimal control.Branching Markov Decision Processes are a class of finitely presented but infinite state MDPs. Branching MDPs generalize a classic stochastic model called Branching Processes, which have been studied in probability theory for a long time.These models consists of the following: there are a finite number of distinct types of objects.Associated with each type T of object is a set of actions A_T,and associated with each type T and each action a \in A_T, is a probability distribution p_{T,a},on the sets of "";offsprings"";  (multi sets of types)that an object of type T can give rise to in the next generation, if the given action a is chosen for that object.In each generation, for each object of a given type in the population, the controller chooses an action.The actions chosen for each object together determine a probability distribution on the state in the next generation, where a state is defined by the number of distinct objects of each type that are present in the population in that generation.The controller may wish optimize various goals.In this project, our focus is the following: the controllers goals are either to maximize or miminize the probability of eventually **reaching** a population that contains objects of a desired/undesired type.For example, one setting where multi type branching processes arise is as models for cancer tumor progression, where the different types denote cell types based on what mutations the cell has undergone.  Some cell type, which has undergone certain mutations, may be maligant.In some cancer treatment scenarios, we may be able to inject multiple drugs into the tumor, in order to effect the offspring generation probabilities of certain cell types.This could potentially be modeled by the control of the branching MDP. Then the goal is to find the control strategy (i.e., the drug injection strategy) that minimizes the probability of reaching a population that contains the malignant cell types.Recently, in [ESY'ICALP15],  building on prior work in [EY'05,EY'06,ESY'11,ESY'12], we have developed efficient, polynomial time algorithms for computing  the optimal (maximal or minimal)probability of reaching a population that contains a given type, up to any desired accuracy epsilon &gt; 0(in general, the probability itself can be an irrational value, so the aim is not to write it exactly), and for computing (epsilon )optimal strategies for the controller to achieve this. These algorithms involve a non trivial, extension of multi variate Newton's method using linear programming, which we usefor computing the Greatest Fixed Point solution of certain classes of probabilistic min(max) polynomial Bellman equations.These solutions encode the sought optimal probabilities. Moreover, one can also compute (epsilon )optimal strategies for thecontroller in polynomial time.The aim of this project is to implement and experiment with these algorithms.A more general model is Branching stochastic games, where there are two players who choose actions, with adversarial goals (one wants to maximize reachability,while the other wants to minimize it).          Efficient algorithms for determining whether the first player can ensure almost sure   (and limit sure)   reachability (regardless of the actions of the other player) have been developed   (and these are easier than the quantitative approximation algorithms mentioned above).       A second possible task is to implement these qualitative algorithms and experiment with them   ","Understand and implement new algorithms for computing optimal reachability probabilities for Branching Markov decision processes, and/or limit sure and almost sure reachability for branching stochastic games",2,Variable,"Completion will be judged by whether there is a running implementation of the quantitative approximate reachability analysis algorithms for Branching MDPs,  or the qualitative (almost sure and/or limit sure) reachability algorithm for branching stochastic games.",8,0,f
4536,Approximation algorithms for analyzing one counter Markov Decision Processes and stochastic games,ug4,1353,2020 02 25 11:24:46 00,1,"One counter stochastic games are a class of finitely presented but infinite state 2 player zero sum stochastic game models.One counter Markov decision processes are the one player stochastic optimization versions of these games.The game consists of the following: there are a finite numberof distinct control states, and there is a non negative unbounded counter (which keeps track of a discrete value which onemay think of as \"";wealth\"";).At each control state, one of the players chooses a move,and that move determines a probability distribution on thenext state, and also a probability distribution on a changeto the counter value (by at most 1).One of the players wishes to miminize the probability of making the counter reach  0 (bankruptcy), while theother adversary player wants to maximize this probability.As hinted, these kinds of MDPs and games can be usedto model certain kinds of perpetual investment scenario with risk averseinvestors, and to analyze optimal investment strategiesin such settings.They also have a number of other applications.Recently, in [BBEKW\'10,BBE\'10,BBEK\'11], algorithms have been developed to decide whether the optimal/pesssimal probability of not going bankruptin such a model using the best possible strategy (regardless of what strategy the adversary uses) is 1 or is 0,and also of ***approximating** the optimal probabilityof going bankrupt.These are algorithms for so called \"";quantitative and qualitative terminationquestions\""; related to these models.These algorithms employ linear programming methods(and run in polynomial time for the one counter MDP case).  There remain some intruiging open questions related to whetherdecision procedures exist for some quantitative problems (e.g., is the optimal probability of bankruptcy &gt;= a given probility p), related to one counter stochastic games.This project asks to implement, and experiment with, the algorithms involved for the qualitative and **approximate** termination problems for one counter MDPs,and one counter stochastic games,Time permitting, the student may also implement heuristics for quantitative analysis of these models.",Understand and implement new algorithms for analyzing  one counter Markov decision processes and stochastic games.,1,3   Hard,"Completion will be judged by whether there is a running implementation of the quantitative approximate reachability analysis algorithms for Branching MDPs,  or the qualitative (almost sure and/or limit sure) reachability algorithm for branching stochastic implementation of the qualitative and **approximate*** analysis algorithms for one counter MDPs and one counter stochastic games.",8,0,f
4537,Improving the performance of Mixed Integer Linear programming algorithms for legged robot navigation with  domain specific heuristics,ug4,5659,2020 02 25 13:25:08 00,1,"Mixed integer problems are optimisation problems where some of the variablescan only take integer values (as opposed to floating point numbers).Such formulations can be used to address combinatorial problems [1].  Mixed integer problems are NP complete and thus a specific instance can require a significant amount to time to be solved.Commercial mixed integer solvers use generic heuristics that have proved useful for large sets of problems to try to speed up the computation. This project propoposes to do the opposite: the student(s) is(are) expected to propose heuristics that are specific to robotics problems, with the hope thatthey will have a positive impact on the performances of the algorithm.The application context is robotics, where one important combinatorial problem consists in determining which effectorof the robot is going to create a contact with which surface. [2]2 paths can be considered to develop the heuristics.One consists in formulating heuristics based on common sense or state of the art knowledge on the problem.The other path consists in using machine learning algorithms to automatically infer such heuristics.The student will be free to choose either path.The provided application framework will consist in an implementation of a variant of [2] developed in a python environment.The student will have to understand in details how integer programming works and use an existing optimisation framework to implement and testhis heuristics.requirements:Strong background on algorithms and optimization           Good programming skills in both C   and pythonSome knowledge about robotics is a plus       ",Specialize a generic optimization framework to make it more efficient for specific legged robotics applications,2,4   Very Hard,The student will demonstrate a framework allowing to easily test the heuristics he will have defined for solving mixed integer problems for legged robot navigation..,8,0,f
4538,Collision free motion generation using a convex optimisation on Bezier curves,ug4,5659,2020 02 25 13:26:20 00,1,"Generating collision free motions for poly articulated robot is an open problem.In general,   numerical optimisation solvers   are used to generate the motions of the robot, but formulating collision avoidance constraints for these solvers usually result in a dramatic loss in the performances.The reason for this is that collision avoidance constraints are numerous (one constraint for each possible surface in collision). Worse,these constraints are not convex.The idea of the subject is to implement a new formulation of the collision avoidance constraints, based on a convex reformulation of the problem that relyon Bezier curves. This reformulation was introduced in a completely different context [1], and the task of the student(s) will be to adapt it tothis new problem.Once the formulation is written, the student(s) will generate complex motions on a simulator, and if time allows for it, on a real robot.",Implement a framework to synthesize collision free motions for legged robots,1,2   Moderate,Implement a trajectory optimisation framework based on a bezier curve formulation [1] Demonstrate the interest of the method within a provided simulator.,8,0,f
4539,Co design for robots and environments,ug4,5659,2020 02 25 13:27:16 00,1,"Feasibility is a big issue in robotics: how can we make sure that a robot is able to perform successfully a task or not ?Often the only way to prove feasibility is to demonstrate a successful case, but even then we are not able to guarantee thatthe robot can robustly and consistently execute such task.In this project we will consider robotics tasks for which we can prove the (un)feasibility and tackle the so called co design problem, that is the issueof finding the minimum amount of changes in the design or the robot or its environment that will guarantee that the robot will be able to reliablyachieve its task.To do this the student will have to propose and implement a genetic algorithm that will evolve specific design parametersin order to find the design of a robot that suits best the task he is destined to fulfill.",Design a method that proposes the minimum amount of design modifications that will allow a robot to successfully perform a task,1,1   Easy,Demonstrate a genetic algorithm able to improve the design of a robot to make it compliant for specifc sets of tasks.,8,0,f
4541,Dynamically modelled trajectory planning for aggressive optimal control,ug4,7576,2020 02 25 15:47:07 00,1,"The project requires  implementing a path planning algorithm which uses a model of the vehicle. This could be determined in two ways   once by  manually deriving the dynamics using traditional physics analysis and  machine learning tools. The alternative would be a reinforcement  learning approach utilising a neural network.  In the end, the two  approaches would be compared to  validate any performance improvements. The  other major part of the project is the implementation of a complex  controller that would utilize the constraints and references computed  by the path planner and provide control outputs that keep the vehicle  driving at its physical limits. The controller would be a nonlinear  MPC using  a powerful enough optimizer.  The performance of the system  would be evaluated in a simulation environment.  The project is  aimed at enhancing the performance of the Edinburgh Univeristy  Formula Student vehicle in the Formula Student UK competition.",Implement an aggressive model based path planner to pass the precomputed dynamic constraints to a model predictive controller for better aggressive driving performance.,1,3   Hard,Successfully implement a realistic dynamic vehicle model and manage to achieve optimal reference and constraints to pass on to an MPC. This would then successfully drive the car around a simulated track to its very physical limit without going off track and posting a lap time comparable to a human driver performance.,8,0,f
4542,Analysis of complex networks using Ricci Curvature,ug4,1375,2020 02 25 17:54:02 00,1,"This research project is for students with strong interest in mathematics and/or theoretical computer science, machine learning theory etc. Curvature is a local description of shape in smooth objects. Ricci curvature is a way to define curvature in small neighborhoods of graphs, so ricci curvature gives  shape  to graphs like we can talk about shapes of objects. The project is about applying Ricci curvature to do network analysis. For example, we would like to do community detection, identify cross community  bridge  edges in a social network, aligning two similar networks etc, by using Ricci Curvature as a feature.  The project is open ended: the student has to explore and develop own approaches based on Ricci curvature and Ricci flows. Of course, this will be done with help of the advisor and possibly others working on the topic. For the interested student, this  project has potential for some great research.If you are interested in research in this area, get in touch via email, with a description of your interests in maths, theoretical computer science, machine learning theory etc. Also note that the requirement is that you should take STN.",Investigate the value of Ricci Curvature as a feature for network analysis,1,4   Very Hard,One of the following: * An algorithm that uses ricci curvature to find communities or perform other network analysis operations * A rigorous characterization of positive and negative curvatures associated with local structures of social networks,8,0,f
4544,Transportation network analysis with Map Directions API data,ug4,1375,2020 02 25 18:46:35 00,1,"Transportation network consists of the road network, coupled with the available transport systems, like driving, buses, trains etc.  In this project, we aim understand more about the transportation networks in cities by querying map APIs. Openstreetmap and Google Maps provides directions apis [1,2] that let us query travel directions between pairs of points in the city. The objective in this project is to extract such data from the apis and thus create abstract models of the city to do one or more of the following:         Learn representations of multimodal transport in the city (a model the that is not only the basic graph, but represents multi modal connectivity)         Predict future travel times         Find road patterns that are correlated with congestion          Multiple diverse route suggestions           Compare utility of google vs openstreetmap information in prediction The student needs the following skills:          Good programming skills (preferably python)         Preferably experience in using web APIs and json         Machine learning         Network analysisThe project is open ended: the student has to explore and develop own approaches. If you are interested in research in this area, get in touch via email, with a description of maths and theoretical computer science courses you have taken   like algorithms, graph theory, network analysis, machine learning theory etc. If you have done a relevant project like STN project, or other similar network analysis or algorithms oriented project, include that in the email. ",Use map directions data to model transportation systems,1,Variable,Algorithm and evaluation with one of the objectives described above.,8,0,f
4545,Analysing Blockchain Protocols,ug4,1342,2020 02 25 18:52:20 00,1,"During the last decade there is a lot of interest in the formal analysis of blockchain protocols. The questions that arise in this context are related to   aspects of scalability, incentive structure, privacy and programmability. In this project you will be asked to focus on one of these areas and contribute in the following sense: (i) define the property of interest within a general framework of analysing blockchain protocols, (ii) analyse existing protocols with respect to the property and discover potential vulnerabilities or deficiencies, (iii) propose possible improvements and mitigations and analyse them.  ",Contribute to the analysis of blockchain protocols,4,Variable,A full technical report covering satisfactory the three points above.,8,0,f
4546,Fairness characterisation of Urban services,ug4,1375,2020 02 25 18:56:12 00,1,"Algorithmic fairness is the mathematical study of bias properties in machine learning and decision making. If the data supplied to an analytic process (like a training algorithm) is biased, then the output can also be biased. For example, if a particular subgroup is small compared to the overall dataset, then the trained model will not align well with this subgroup. Fairness is a major emerging research area in machine learning and algorithms.  This project is about asking the same question regarding the design of a city like Edinburgh. Is the infrastructure of the city biased in favour or against some groups, for example, certain localities? Are some regions (like sparsely populated ones) systematically getting poor service? The student will study various theoretical fairness metrics, adapt them to geospatial data such as the city and develop a system to visualze them. This will involve both a theoretical study and understanding of algorithmic fairness, and a data oriented aspect, where various datasets and APIs will have to be accessed and analysed.  This project is suitable for students interested in fundamental properties and theories of ML, algorithms, data mining etc.  If you are interested in research in this area, get in touch via email, with a description of your interests in maths, theoretical computer science, machine learning theory etc.The student should take the STN course. This project has a capacity of 1. ",Measure and plot the fairness of various infrastructure services in Edinburgh,1,3   Hard,"A comparative study of fairness of various infrastructures in Edinburgh (or another city), with a general software that allows one to perform the same analysis on different regions of UK.",8,0,t
4547,Topological analysis of biomedical data,ug4,1375,2020 02 25 19:14:47 00,1,"This research project is for students with strong interest in mathematics and/or theoretical computer science, machine learning theory etc. Computational topology adapts the mathematical concept of topology to discrete datasets, and topological data analysis (TDA) is the application of topology to data mining and learning. The idea in this project is to investigate the use of TDA in biomedical data, for example in identifying patterns corresponding to medical conditions. It is an open ended project, meaning, the student has to investigate datasets and possibly devise suitable TDA methods.  For the interested student, this project has possibility of some unique research.  If you are interested in research in this area, get in touch via email, with a description of your interests in maths, theoretical computer science, machine learning theory etc.It is recommended that the student take the STN course.  The project has a capacity of 1 student.",To understand the utility of topological data analysis in extracting insights from biomedical data,1,4   Very Hard,One of the two: * Topological insights on one or more sets of biomedical data * A software to easily apply computational topology to biomedical data,8,0,t
4548,A VR based diagnosis platform for ADHD with VR Based Tye tracking and Heart Rate Variability (HRV),ug4,7932,2020 02 25 20:17:01 00,1,"ADHD is the most commonly diagnosed behavioral problem while being the most misdiagnosed condition among children and youth. The condition is mostly described as being unable to control impulses and/or being unable to pay attention to  non interesting  activities for prolonged periods of time.The main aim of this project is to design and implement a full test suite that has potential to be more accessible through providing a cheap diagnostics tool for ADHD compared to already existing cumbersome tests. The platform will include VR implementations of tests that exist in different mediums such as 2D eye tracking and real world head mounted tracking. This will provide a future opportunity for researchers to compare the performance of both mediums and decide if VR possesses a potential to be the next breakthrough in mental health.Moreover, the platform will include several VR centric tests that are designed by me from scratch to take advantage of VR. These tests will focus on possible theories/arguments that are hard or impossible to test in real physical environments.Newly designed tasks will employ stimulants included but not limited to virtually designed environments with events/tasks in a game like manner where the actions of patient will be recorded and used in statistical models.A key aspect of the project is to develop libraries that are the core of diagnosis: statistical tools and machine learning. These libraries should take the input from created tests and provide vital information such as saccade duration, variance...",Create a software platform that is designed to aid the diagnosis of people with ADHD based on VR and HRV by providing statistical tools for diagnosis.,1,Variable,Implementation of a test suite with appropriate assessment that is able to perform statistical analysis of the data gathered. A side aim is to make the platform extendible to other cognitive disorders.,8,0,t
4549,Task scheduling in heterogeneous ISA processing platforms,ug4,7197,2020 02 25 21:04:39 00,1,"It is likely that future computer platforms will be built with heterogeneous ISA CPUs (see references [1] [2] [3]). On a platform with heterogeneous ISA CPUs each CPU strictly executes its own software. This is because, traditionally, the code compiled for a CPU of a specific ISA cannot run on a CPU of a diverse ISA. The Popcorn Linux project (www.popcornlinux.org) removes this barrier by offering an operating system that runs among heterogeneous ISA CPU, and a compiler that creates applications that can   start on CPU of any ISA and additionally migrate during execution. Despite that, Popcorn Linux doesn't come with a proper task scheduler for heterogeneous ISA CPUs. This project aims at the design, development, and evaluation of such a scheduler on an actual heterogeneous ISA platform built with an ARM and an x86 server connected via non transparent PCIe bridge running Popcorn Linux. The scheduler can be implemented either in user or in kernel space. It is expected that different scheduling algorithms, eventually integrating ML/AI, will   be tested on different set of workloads.","Design, develop, and evaluate task scheduling algorithms for platforms built with heterogeneous ISA CPUs.",2,3   Hard,"Code compiles, executes correctly, and its characteristic times have been measured and compared with the original software.",8,0,f
4543,Scalable Software Messaging Layer for Multiple kernel Operating System,ug4,7197,2020 02 25 18:34:58 00,1,"Multiple kernel operating systems are a recent alternative to traditional monolithic operating system, such as Linux, BSD, Windows, and OS X (the latter 2 are hybrid, actually). Compared to other operating system designs, multiple kernels offer different advantages, including better application's scalability, and the ability to exploit any esoteric computer architecture.   A key component of multiple kernels is the software messaging layer, which enables different kernel instances to communicate   a performant multiple kernel   operating system requires a performant software messaging layer. This project looks at the design, development, and evaluation of a new scalable software messaging layer for the Popcorn Linux OS (www.popcornlinux.org), and consists of two modules.A first module aims at rewriting the existent shared memory messaging layer implemented in Popcorn Linux OS in order to improve its scalability on single and multiple sockets multicore computers. A clear way to attack this problem is by replacing the current locking mechanisms with more scalable implementations (e.g., https://github.com/cameron314/concurrentqueue , https://github.com/rigtorp/MPMCQueue ), but more creative ideas are sought. In addition to this, the scalability of the messaging layer should also be studied for NUMA topologies with asymmetric access latencies.A second module aims at tight integration of the messaging layer within the Linux kernel itself, including the scheduler. This is similar to the way messaging layers are integrated in microkernels operating system   i.e., a messaging layer is a core part of a microkernel operating system. Additionally, this module requires to apply common software engineering design practice to the Popcorn Linux messaging layer in order to seamlessly support multiple messaging communication medias, and the message routing among them.","Design, develop, and evaluate a scalable software messaging layer for the Popcorn Linux multiple kernel operating system.",2,2   Moderate,"Code compiles, executes correctly, and its characteristic times have been measured and compared with the original software.",8,0,f
4550,Interactive Classifier Training and Data Optimization in the NASA/JPL Complex Data Explorer,ug4,7683,2020 02 26 06:54:52 00,1,"Space borne science missions generate large amounts of complex data, and it is critical that that data is fully understood before beginning traditional analysis as missing important facets of the data early on can lead to distorted or wrong conclusions down the line. This is even more pressing in the time sensitive environment of Mission Operations, where problems may need to be detected, explored, and understood in a matter of hours. For data that is both large in volume (100k  samples) and dimension (dozens or 100's of simultaneous, related measurements), no available application permits fast, graphical data exploration.The Complex Data Explorer (CODEX) aims to provide a general, re usable, visual analysis environment for graphing, data driven exploration, hypothesis falsification, and intuition building. CODEX provides machine learning powered insights into large datasets, picking out features and clearly highlighting areas of interest. CODEX is designed to operate on data it has never seen before, making it ideal for wide adoption across NASA and JPL missions as well as a general tool for large scale data analysis applicable to fields as diverse as healthcare research to geological operations to economics studies.The goal of this proposed honours project is to optimize and extend the CODEX application. To meet the goals of analyzing extremely large datasets in an interactive manner, CODEX is a necessarily complex application. It is implemented as a web app, built with React.js ( Redux  Plotly) on the frontend and Python 3 and Tornado on the backend, supported by a custom data system and Scikit/SKLearn/Pandas. There are several sub goals within this project:(Very Hard) Implement an interactive classifier to enable on line classification of particular features or groups of features, including frontend/backend hooks and proper visualization. This will require careful planning, as classifier training is a computationally intensive task and to be interactive requires user feedback and completion estimates.  (Hard) Enable context switching between a local server and a high performance datacenter server, as this is a natural workflow for data science. This can happen on a workspace wide or granular (per task) level.(Very Hard) Streamline the internal data handling to enable higher speed data throughput. This will require careful profiling of both the frontend and the backend, hot code path analysis, as well as potentially rewriting portions of the code handling in C/Rust/WebAssembly.This project will be carried out with the support of the Machine Learning and Instrument Autonomy Group (MLIA) within the NASA Jet Propulsion Laboratory.","Implement an on line classifier for timeseries data within the NASA/JPL Complex Data Explorer (CODEX), and deliver optimizations along the way in order to better enable science.",1,Variable,"Minimum result: Completion of Task 1 as above stated. Good result: Completion of Tasks 1 and 3, as above stated. Excellent Result: Completion of Tasks 1, 2, and 3 as above stated. All three tasks necessitate documentation and appropriate unit tests.",8,1,f
4540,A BECTS Diagnosis Tool using Paediatric EEG Data,ug4,1384,2020 02 25 14:00:14 00,1,"Benign Epilepsy with Centrotemporal Spikes (BECTS) is a type of epileptic syndrome that affects children. One of the important diagnosis tools for BECTS is electroencephalography (EEG) where neural activity is measured in the form of electrical measurements that are recorded using multiple (typically 18) electrodes carefully attached to the scalp. Typically, recorded data consist of measured activity gathered at around 240Hz for several minutes or potentially even hours. Once recorded, it currently takes an expert paediatric neurologist several hours to identify characteristic features that will help diagnosis. The economic cost of diagnosis (to the NHS) is high due to the manual component especially since the search for characteristic features can currently only be performed by experts. Any tool that can reduce the time spent in searching for these patterns could greatly impact overall cost to the NHS.  The technical component of the project will be to implement a simple supervised learning system for the 18 channel time series data and to develop a GUI which can be used by an expert to peruse the EEG data faster than by having to manually scan the entire signal. This project will be co supervised by Dr. Jay Shetty (Royal Hospital for Sick Kids) who will provide access to a modest amount of anonymised paediatric EEG data along with labelled features that are considered  interesting' for diagnosis.  ",To develop a tool that uses machine learning along with a GUI to expedite the diagnoses of BECTS from EEG data,1,2   Moderate,1. Develop and evaluate an automatic classifier for characteristic labelled features in the EEG. 2. Development of a GUI that allows rapid exploration of EEG signals. 3. Integrate 1 and 2 to perform quick recommendations and suggestions to the expert (user).,8,0,t
4551,A computer assisted translator of Latin to English,ug4,8020,2020 02 26 12:15:34 00,1,"Latin is a dead language with a rich history and a sophisticated literary tradition. It is no longer actively spoken or translated into, rather the focus of current Latin education is on the ability to read the language. One of the reasons this is not an easy task for humans, let alone for machines, is that the order of the words is almost arbitrary. There may be a subject, an object, and multiple relative clauses before you get to the main verb. The nouns that the relative clauses refer to are likely not adjacent, and the clause itself could be split into different parts scattered across the sentence. This makes for an interesting puzzle when translating Latin works.NLP efforts for Latin have been given a huge boost by the creation of the Classical Language Toolkit (CLTK) [1]. Still, tools for working computationally with the meanings of ancient texts are only recently emerging. The main Latin software efforts include:The Perseus Digital Library [2], which has a word study tool [3] and the Scaife Viewer [4], a reading environment of parallel Latin English collections (among other languages; with the focus on pre modern corpora). The Latin WordNet [5], an on going collaboration between the University of Exeter and the Linking Latin Project [6] to create a comprehensive lexico semantic database of the Latin language.Blitz Latin [7], a Latin to English translation program developed and analysed in an academic environment [8], obtaining accurate translations. In general, other options like general machine translators are inappropriate for the language and perform poorly, especially on novel input (which is often used as educational material).Whilst all of these achieve impressive results and are state of the art in what they provide, one thing they lack is sentence analysis. Currently they all provide individual word analysis, but do not support exploring syntactic dependencies within sentences. This functionality, among others, would aid students in their task of learning how to translate Latin. This project will investigate how to best assist human translation of Latin in a way that supports learning. It will do this for both existing translated corpora and novel input. The following steps will be taken:Latin language research: establish a working knowledge and comprehension of the Latin language.User Interface and User Experience research: determine the optimal design and functionalities to achieve high usability and support for human translation.Lemmatization: return the possible headwords of a given word.Part of speech tagging: mark each word in a sentence with its word category, relying on conjugation rather than context because of the arbitrary word order.Syntactic dependency analysis: perform constituency and dependency parsing to identify clauses and relationships.Lexico semantic analysis and word sense disambiguation: use AI heuristics to determine which meanings for a word are the most probable.UI implementation of individual word analysis: collate the information from steps 2 6 to provide the user with individual word analysis, similarly to the existing software previously mentioned.UI implementation of step 5: develop a tool that will output these clauses and relationships using the user friendly design defined in the UI and UX research (step 2).Translation: provide translations for the clauses determined at step 5. This will either involve matching up the English translation (for existing corpora) or generating the most probably sequence of English words (for novel input).Implementation of other functionalities: develop other tools that at the UI and UX research (step 2) emerged as aiding translation and student learning.","To develop a beginner friendly, user friendly computer assisted translator of Latin to English to aid student learning",1,3   Hard,"Do steps 1 10. Steps 1 7 involve (re)producing the functionalities of the current software efforts, and will be supported by the CLTK and previous open source code work. Steps 8 10 will introduce novel functionalities to support human translation of Latin and student learning.",8,0,f
4552,Solving terms in the Lukasiewicz mu calculus.,ug4,1338,2020 02 26 13:23:54 00,1,"The Lukasiewicz mu calculus was introduced by Mio and Simpson and incorporates probability when reasoning about systems. A term in this calculus denotes a number, the problem is to find the number given a term. An algorithm was given by Mio and Simpson but they state that the worst case analysis leads to an "";abysmal""; bound. (However it seems very likely that the algorithm is good in practice.) An alternative algorithm was developed by Kalorkoti, the worst case here is single exponential (again the runtime is likely to be better in practice). The aim of this project is to implement the second approach with some timing studies as well as check the effect of some heuristics on the runtime of the algorithm.",Implement and evaluate one algorithm for solving Lukasiewicz mu calculus terms.,1,3   Hard,A complete implementation and timing studies of the main algorithm.,8,0,f
4553,Algorithms workbench.,ug4,1338,2020 02 26 13:32:14 00,1,"The aim of this project is to extend the fucntionality of open source software originally built by Eziama Ubachukwu and by other students later on.   The code for the latest version by Nevena Balgoeva is available at  https://github.com/nevenaBlagoeva/AlgoBench.Here is the original proposal:Inf2B has a thread on Algorithms and Data Structures in which many fundamental algorithms are introduced along with their worst case runtime analysis. Algorithms are given by means of pseudocode. It would be useful to have a system that enables students to implement algorithms in a language quite close to pseudocode and then enables them to experiment with runtime studies (perhaps also space used). The tool must make this process as convenient as possible thus encouraging experimentation. For example if a student wishes to compare quicksort with mergesort he/she should be able to express these in the course pseudocode and then ask for runtimes based on data created either by the student or the system. After that timings should be produced both in tabular form as well as plots. The system should also determine the constants involved in the standard upper bounds. A refinement would be to enable the student to ask for the timings of sections of the code so that the critical parts can be identified.Although the original proposal was for a specific course it is now time to widen the scope to other and morfe advanced algorithms.    Aside from this another desirable aim would be  to port the software to a virtual server with a web interface.   Whatever is the case there is a range of desirable extensions that can be agreed between the student(s) and supervisor.   As an example, the system has the capability of creating pdf reports which can then be incorporated by the usert into a document that is edited so that notes and observations on experiments can be recorded.   It would be desirable to have a lecturer account mode so that exercises/experiments can be suggested or set as part of assesment with a submit facility linked for the student answers.Please note that it is a precondition of this project that all the code is open source and the result is placed on a public repository such as GitHub.",Add significant new functionality to a tool that enables students to study the performance of algorithms.,1,3   Hard,"A significant extension to the existing software (underlying algorithms, interface).",8,0,f
4554,Tool to assist with assessing evidence for academic misconduct.,ug4,1338,2020 02 26 13:50:33 00,1,"MOSS is an online system for testing software similarities. The user submits a set of files (code) and the system proceeds to test all pairs for similarities (it is sophisticated and is not fooled by such things as changes of variable names or movement of blocks). In general this is a good way to bring to the user's attention possible collusion/copying. However it is subject to the problem that if each one of n types of similarities are not in themselves that unlikely then for a large class there is a reasonable probability that there will be a pair in which all n similarities appear. Since MOSS only looks at pairs the user of the system does not get to see that the single similarities are in fact not too surprising and so it is difficult to assess the significance of seeing them altogether in one pair of files. Going through the entire set of files manually to examine the evidence is extremely time consuming and usually not realistic.The major aim of this project is to produce a tool that automates much of the process with the user directing the system at which types of similarities to consider. Ideally the tool should also be able to carry out some form of naormalisation of the input code to improve detection. It should be able to work with submitted code files in at least some of the major languages used in the School.   Another significant aim is to assess the strengths and weaknesses of MOSS.The project will improve and build on existing quite advanced code from previous MSc and UG4 projects.   The exisitng system runs on a virtual server.   There are many aspects that can be improved or added, e.g., the ability to run detection on more languages.   A start was made on this previosuly with good success but ther were some remaining problems to be solved (in connection with the efficeincy of ANTLR).    Another important aspect is to carry out more testing in order to enusre reliability under many circumstances. A user trial invo,ving Informatics staff woudl also be very desirable.The latest version of the software can be obtained from  https://github.com/ajpetersons/Misconduct Assessment Tool and  https://github.com/ajpetersons/si.",A tool to help in assessing the probabilty of similarities in software submissions and display them.,1,3   Hard,A significant addition to the inherited software.,8,0,f
4555,The distribution of roots of random polynomials.,ug4,1338,2020 02 26 13:57:44 00,1,"In various forms of research it is imporant to know the distribution of the complex roots of random polynomials.   There are theoretical  answers when the  coefficients are drawn from certain probability distributions but other cases remain very hard to analyse.   For this project we will consider in the first instance Littlewood polynomials of a given degree, these have coefficeints plus or minus 1.   Their roots  display a very interesting pattern which is difficult to explain.   The aim will be to design a platform to enable experiments to be carried out and conjectures tested.A previous project considered polynomials of a given degree drawn uniformly at random with integer coefficients from the interval [ B,B] where  B  is a chosen parameter.   A further technical  condition was that the polynomials do not have repeated roots (easily arranged) and the gcd of their coefficients is 1 (again easily arranged).   Results there suggest a sdimilar pattern to the Littlewood  polynomials and if time permits further investigation of this will be undertaken.The projedt is rated as easy in the sense that a basic system is stragithforward.   A more ambitious version woudl also be possible.",Implement an exact root finding algorithm and test a hypothesis on root distribution.,1,1   Easy,An implementation of an efficient reliable root finding algorithm and experimental results.,8,0,f
4557,Multicore Processing Support for InfOS,ug4,7936,2020 02 26 16:40:42 00,1,"The research operating system that is the subject of this project is called InfOS. It is a 64 bit x86 operating system written entirely from scratch in C  . The purpose of this project is to add multicore processing support to InfOS. This will involve developing the boot protocol for multiple processors, adapting the scheduler to schedule processes on different CPUs, and adapting internal data structures.",To add multicore processing support to a research operating system.,1,3   Hard,Demonstration of at least one multithreaded program running on multiple cores.,8,0,f
4558,Flying vehicles guided by insect navigation algorithms,ug4,7705,2020 02 26 18:36:55 00,1,"The development of electric vertical takeoff and landing aircrafts (VTOL), also known as Flying cars, has advanced considerably in the past years and they could soon become a reality. It has been found that, when fully loaded and over long distances, VTOLs could have a lower carbon footprint than ground electric cars [1]. Moreover they would be a step forward towards reducing road mortality and making cities more liveable.Flying cars SLAM methods have been successfully tested in both real and simulated environments for localisation and mapping in autonomous vehicles [2]. However, in recent years, there has been considerable interest in implementing some of the insights gained from the study of vision and navigation in insects to the guidance of terrestrial and aerial vehicles. It is believed that biological inspiration will provide novel solutions to difficult and persistent problems in the design of navigation systems for autonomous vehicles [3]. Insect navigation has been studied extensively and a base model that encompasses the simplest set of assumptions that capture insect behaviour has been proposed [4].The aim of this project is to implement the traditional SLAM methods and insect based navigation in a simulated autonomous vehicle. The two methods will then be tested, compared and evaluated in realistic simulation environments. Finally, depending on the resources available, the implementations could be tested in a real flying vehicle.",To implement and test traditional SLAM and insect navigation inspired algorithms in flying vehicles.,1,3   Hard,Implementing the traditional SLAM methods and insect based navigation in a simulated autonomous vehicle.,8,0,t
4560,Emergence of Language with Multi agent HyperNeat,ug4,7698,2020 02 26 22:36:02 00,1,"Neuroevolution (NE), the artificial evolution of neural networks using genetic algorithms, has shown great promise in complex reinforcement learning tasks.  HyperNeat (Hypercube based Neuroevolution of Augmenting Topologies) is a novel technique for evolving large scale neural networks by not only optimising the connection weights but also the topology of the neural network. "";It [follows] the idea that it is most effective to start evolution with small, simple networks and allow them to become increasingly complex over generations. That way, just as organisms in nature increased in complexity since the first cell, so do neural networks in HyperNEAT""; (Stanley, 2002).In the past years, machine learning has seen significant success in NLP related applications. However, the algorithms work by processing extremely large amounts of textual data, from which the systems extract features and discover patterns. While this has solved many important domain specific problems, there is no indication that the algorithms trained in this way have any deep understanding of language and how relates to the real world. However, it is conjectured that language, and furthermore the ability to learn to communicate through interaction, might be a prerequisite or even the key to the development of General AI.In the recent past, the computational study of language emergence using referential games has received a lot of attention (Evtimova et al. 2018; Lazaridou, Peysakhovich, and Baroni 2017; Mordatch and Abbeel 2018; Havrylov and Titov 2017; Choi, Lazaridou, and de Freitas 2018; Bouchacourt and Baroni 2019). In such games, motivated by the functional aspects of language, two agents have to develop a discrete communication protocol to talk about objects in an artificial, grounded environment. The recent wave of such experiments shows that agents parametrised by neural networks can develop successful communication protocols that allow them to complete their task. Most approaches rely on reinforcement learning techniques but HyperNeat, with its ability to complexify a neural network in the same way that language has become increasingly complex over time, represents an interesting alternative to model the emergence of language.The aim of this project is to evolve agents parametrised by artificial neural networks to solve a referential game by developing a discrete communication protocol.  An ideal starting point is the referential game of Lazaridou et al. (2017) since it involves an  extremely simple signaling game (Lewis, 1969),  in which a Sender and Receiver are exposed to the  same pair of images, one of them being randomly  marked as the  target . The Sender always sees  the target in the left position, and it must pick  one discrete symbol from a fixed vocabulary to  send to the Receiver. The Receiver sees the images in random order, together with the sent symbol, and it tries to guess which image is the target.",Evolve agents parametrised by artificial neural networks to solve a referential game by developing a discrete communication protocol.,1,3   Hard,Implement Lazaridou's referential game using HyperNeat.,8,0,f
4561,Controlling a Virtual Character by 2D Videos,ug4,1330,2020 02 27 07:08:27 00,1,"This project is composed of 2 elements.   The first part is to construct a 3D deformable model of a subject wearing garments.   The subject will make various poses in the body scanner area and a skeleton will be fit to the body.     Once the model is constructed, the arbitrary poses of the 3D deformable model can be produced by changing the skeleton pose.     Next, given the constructed model, this deformable model will be fit to a 2D RGB video of the subject dressed in the same garment.   The initial pose of the body will be guessed by a neural network based 3D body pose fitting algorithm [1].    The 3D model will then be deformed such that the loss function based on the joint positions, pixel colours and regularisation form is minimised [2].  This project is similar to the Face2Face [2] project which is a reenactment project for human face   it is an application of the concept for 3D human body pose.    ",Using RGB videos to control a 3D character that is produced by a 3D body scanner,2,2   Moderate,1.  A 3D deformable model is constructed. 2. An optimization framework to fit the 3D deformable model to the RGB video is completed.,8,0,f
4562,Synthesizing body details of crowd agents by reinforcement learning,ug4,1330,2020 02 27 07:10:12 00,1,"Characters in crowd animation are often visualised by simple cylinders or dots   or in some cases by human characters but with simple walking movements.   The objective of this project is to increase the realism of such animation by making use of the large motion capture database.   More specifically, the student will use Phase Functioned Neural Network (PFNN) by Holden et al. 2017 for this purpose.   The PFNN can produce realistic behaviour such as side stepping, backward walking,   slowing down and accelerating, which can increase the realism of the crowd animation.   This project can be done in several directions:  (1) Easy:    Use a crowd simulator such as Pelechano et al. as the input for the PFNN and animate the scene.(2) Middle:    Compute the offset to the body when applying PFNN to Pelechano et al. by reinforcement learning such that there will be less collisions with the other agents.  (3) Hard:   Compute a reinforcement learner that also control the agent's behaviour so that it doesn't collide with others and also achieves the objective to reach the goal.   (this is replacing Pelechano et al. with a reinforcement learner.   ",Animating behaviours such as side sidestepping and queuing in crowds.,1,1   Easy,A scene where many PFNN characters in a crowd are moving in a realistic way.,8,0,f
4563,Learning in Competitive Games,ug4,7973,2020 02 27 15:46:15 00,1,"I am extremely interested in how computer's learn, whether it's reinforcement based or otherwise   this field is always changing and i would love to explore this further.As I am doing MInf, my project will be ranging over 2 years. For this reason I do not wanting to my project on "";Learning in {Insert Game}"";. I propose that my project will look at a range of games for example Catan, Draughts, etc whilst exploring general algorithms for all the games i will be considering. Not just specific best case algorithms for each game.My tasks will include:1) Creating the games / simulation environments for the games I will be considering2) Designing and implement a general solution that will work on all the games I am considering e.g. deep reinforcement learning,  curiosity driven learning3) Optimise my solution, reporting any short comings, exploits, etc4) Evaluate my models against some baseline models e.g. game specific solutions",To explore a general algorithms for a range of competitive games,5,Variable,Implementation of a variety of game simulation environments; implementation of a general solution for chosen games; testing and evaluation of a player that has learned its strategy from game simulations or otherwise.,8,0,f
4564,Near real time speech transcription service,ug4,5024,2020 02 27 17:16:57 00,1,"Quorate Technology Ltd. has previously investigated the feasibility of a near real time speech transcription service using an in house developed automatic speech recognition library. As part of investigations, a basic prototype service was developed on Amazon AWS with components written in Python. One key component that the service depended on was the AWS API Gateway which was proven to introduce a large overhead and limitations on the service. However, getting rid of this component completely changes the design of the live speech transcription service.The aim of this project is to develop a new near real time speech transcription service using the knowledge gained in previous investigations as guidance on what could be the best approach to doing near real time speech transcription.Project tasks:Research and evaluate work that has already been done in this area. Example projects to look into: Quorate investigations, Amazon Transcribe and Azure Speech to Text.Create the software architecture for the service based on knowledge obtained in the previous step. Take into account potential scalability and fault tolerance problems.Develop a near real time speech transcription service without elements of scalability or fault tolerance implemented.Implement features for:Scalability (having multiple speech recognition components, having multiple connections per speech recognition component, load balancing real time persistent stateful connections, scaling with demand,  ¦)Fault tolerance (fail over, responding to network issues,  ¦)Compare and evaluate the end product with work already done in this area.Potential extensions:Support for both batch and near real time speech recognition",Develop a scalable and fault tolerant near real time speech transcription service using an ASR library developed by Quorate Technology Ltd.,1,3   Hard,"Research and analysis of potential options for doing near real time speech transcription. Implementation and demonstration of the chosen option, and preferably at least four scalability and fault tolerance features.",8,1,f
4565,Neural Network pruning for faster execution and smaller memory usage,ug4,7891,2020 02 27 19:15:51 00,1,"Neural networks have made huge jump in performance in the recent years. The widespread adoption of Convolutional Neural Networks and similar has allowed for multiple breakthroughs in the fields like Computer Vision, allowing for example for autonomous driving.With the increased performance comes also increased complexity and size of the neural nets, where it is often impossible to run the nets on device and the work has to be off loaded to backend services. The goal of this project is to study recent advances in neural network pruning and neural network compression, in particular of Graph Convolutional Neural Networks (GCN), and deliver considerable improvements to the execution time and memory footprint, potentially allowing them to run on device. The project will consider a giant object detection network used and supplied by Toronto Annotation Suite (University of Toronto), but the results will be applicable in general to all GCNs.",To speed up computation of a giant neural network to reduce execution time and memory footprint without affecting performance,1,3   Hard,1.      Study and implement current neural network pruning techniques (medium). 2.      Achieve a reasonable speed up whilst not compromising on performance (hard),8,1,f
4567,Energy costs of Neural Nets,ug4,1987,2020 02 28 09:39:19 00,1,"Co supervisors: Kate McCurdy, Sigrid Hellan, David SterrattThe computation needs of neural networks are currently growing faster than Moore's Law, and recent research has highlighted the need for better, standardized methods for calculating the energy requirements of neural models (Strubell et al., 2019; Schwarz et al., 2019). As the need to reduce energy usage across sectors grows, it is incumbent upon machine learning researchers to develop tools which transparently show the energy costs of neural models to help inform their decisions.This MSc project will focus on comparing energy consumption on the GPU hardware in use in the Forum, using existing libraries to estimate FLOPS in a PyTorch model implementation. It requires basic knowledge of algorithmic complexity and big O notation, familiarity with PyTorch, and strong performance in MLPR or equivalent; familiarity with benchmarking / software profiling is also desirable, but not required. The basic task of this project is relatively straightforward, but for the motivated student, there are many possible directions to expand based on interest. For example, further comparisons could be made across different profiling tools, hardware setups, or efficiency metrics   or for a more model based focused approach, by varying characteristics of the PyTorch model, or assessing FLOPS for other data modeling methods.The primary libraries of interest are PyTorch OpCounter (https://github.com/Lyken17/pytorch OpCounter) and torchstat   (https://github.com/Swall0w/torchstat).We ask that   potential students arrange to meet with us before selecting the project.",Measure energy efficiency of GPU hardware running Neural Nets,1,2   Moderate,"Minimum requirement: select a PyTorch model for a given problem (in domain of choice, although we may choose to constrain); measure energy efficiency (in FLOPS) across different GPU hardware in Forum; write up comparison.",8,0,f
4568,Learning systems biology models from data,ug4,1987,2020 02 28 10:12:13 00,1,"Molecular events in the brain underlying learning and memory can be described using sets of chemical equations, which can be simulated using ordinary differential equations, or simulated stochastically. In both cases, the models are governed by a potentially large number of parameters, such as molecular concentrations, and chemical binding and dissociation rates. Many of those parameters are hard to determine individually by direct experiments. However, there are experimental data which constrain the posterior distribution of the parameters.We (Melanie Stefan in Biomedical Sciences, and David Sterratt) have a set experimental data and corresponding computational models which relate to the activation of calmodulin, a protein involved in synaptic plasticity in the brain. [1]We would like to  use the Python emcee package to infer the posterior distribution of the parameters [2]. We would also like to encode alternative models of calcium calmodulin binding and infer their parameters. There are particular challenges to dealing with the data, due to a large number of potential nuisance parameters, which we need to find some way of marginalising over.There is scope for this to be a joint project, with one student focussing on the inference and the other focussing on coding up the model.The outcome will be a posterior distribution of parameters for the model, which could tell us whether the existing computational model is realistic or not. We will then use the inference method on alternative models, providing the Bayesian evidence for which model best describes the data. The outcome could resolve a long standing experimental puzzle, and we would hope to publish it in a scientific journal.As well as the essential skills, it is essential for you to meet with us before we consider marking you as suitable for the  project. Please get in contact to arrange a meeting.",To find the posterior distribution of parameters in systems biology models related to learning and memory in the brain.,2,2   Moderate,* A posterior distribution for the parameters of the model. * Checks on the validity of the inference.,8,0,f
4569,Computational modelling of biological learning,ug4,1987,2020 02 28 11:44:44 00,1,"Co supervised with Melanie Stefan, School of Biomedical SciencesWhen we learn, connections between neurons (synapses) in our brain get stronger. This strengthening is mediated by Calcium influx into the neuron at the receiving end of the synapse and the subsequent activation of a protein called Calcium/calmodulin dependent kinase II (CaMKII). CaMKII regulates a number of other proteins, ultimately leading to a change in the molecular composition of the synapse, and also changing the shape and size of the cellular microstructure that supports the synapse. It acts in two different ways: First, by catalysing a small chemical modification (the addition of a phosphate group) on other proteins, thereby changing their activity. Second, by forming complexes with other proteins, thereby controlling their location and mobility within the cell. A lot of work in recent years has gone into modelling the biochemical pathways that fine tune the activity of CaMKII and its downstream effects. Thus, we understand quite well how different kinds of Calcium signals coming into the synapse can result in CaMKII activation, and how this in turn results in phosphorylation (and therefore, higher activity) of AMPA receptors, which receive synaptic signals. Those models have also helped us understand competing pathways that interfere with the action of CaMKII, and establish the conditions under which those competing pathways would be activated. But what is less well captured so far in computational models is the structural role of CaMKII, which it exerts through binding to other molecules, including those that make up the "";cytoskeleton"";, the structural backbone of the cell. In this project, you will extend an existing model of synaptic Calcium signalling to include structural effects of CaMKII. The existing model was developed in the Kappa language, which allows for efficient description of biomolecules that can adopt many possible functional states. The aim is to work towards a unified model that captures both biochemical and structural changes that happen during learning and memory.",To extend a model of a subset of the complex molecular interactions underlying biological learning,1,2   Moderate,"Kappa code implementing the new model, and results and analysis of simulations generated by the model.",8,0,f
4559,Blockchain Ticketing System,ug4,7754,2020 02 26 20:55:26 00,1,"Currently, the ticketing landscape is complex. The unregulated nature of secondary markets opens up potential to malpractice, unethical touting behaviours such as fraudulent counterfeits, duplicated tickets and reselling tickets at a much higher price than the market value. The issue of secondary market as an intermediary in fact could be addressed by implementing the blockchain technology in the ticketing system. Through utilising smart contracts, restrictions could be impose on the resales of tickets and the authenticity of tickets are easily verifiable.  An issue to be concerned with is the privacy of the users of this blockchain ticketing system. What data should be made public and accessible on the distributed ledger? This is an area to be experimented with in the project. Possible extensions include improving the efficiency of retrieval and verification process.Blockchain ticketing is currently being experimented by different organisations. Refer to link provided for an open source blockchain ticketing system by Aventus.  ",Build a blockchain ticketing system by utilising smart contracts,1,3   Hard,Partial Implementation of a blockchain ticketing system based on smart contracts. Carry out security and privacy analysis on the system (ie development of a threat model). Identify post improvement,8,0,f
4570,"A ""Permissions"" System in Linux",ug4,5871,2020 02 28 15:44:34 00,1,"  A "";Permissions""; System in Linux  BriefMobile   operating systems such as Android and iOS have "";permissions""; mechanism that allows the user to give, deny, or revoke fine grained permissions to the applications.The aim of this project is to bring such a system in a backward compatible manner to desktop and server Linux.  The Rationale  In the past, multiple human users timeshared a single mainframe.Today, a single human user have multiple computers, using many of them at the same time!In the past, origins of the programs the users have run were quite limited, so programs were easier to trust.Today, computer programs are developed by many 3rd parties, and thus are harder to trus (e.g. JavaScript in the browsers).In the past, the maxim was to protect the users from each other.Today, we should protect the (sole) user from the apps.In the past, conceptually "";everything [was] a file"";.Today, operating systems use "";rich""; objects for different assets and devices: contacts, photos, documents, camera, microphone, ...In the past, Discretionary Access Control was used, which is designed around the notion of "";ownership."";Today, Mandatory Access Control satisfies the needs of users better, and is structured around "";authorization."";  The RoadmapBear in mind that the roadmap is subject to change and provided as an example only.Analyse the security needs of Linux based desktop and server   operating systems which are (a) currently not addressed and (b) within the scope of this project.Clarification on (a): Ransomware are (for instance) not addressed as a threat by the security model of the operating system.Clarification on (b): This project is about exposing high level security permissions to the discretion of the users. So for instance measures against buffer overflow attacks shall be considered too low level and out of the scope of this project.Investigate the security models of the mobile operating systems and their applicability to our domain.Design a backward compatible security model for desktop and server operating systems.Develop an implementation of such model, which would likely involve:An enforcer (of the rules)A user interface (for monitoring and controlling the system)  Test and Benchmark the implementation:Benchmark for performance penaltiesCPU bound benchmarks (e.g. syscalls)IO bound benchmarks (e.g. network bandwidth)Security testsManual testsFuzzing testsUser studies for the UX",Design and implement a backward compatible permissions system for GNU/Linux to protect the privacy of users,1,3   Hard,A working implementation of a system described above.,8,0,f
4556,Neurally Guided Bayesian Learning of Programs,ug4,7830,2020 02 26 14:14:03 00,1,"This project is generally inspired by the question "";how humans are be able to learn so much from so little""; [2] and the ancient wish of forging the god with superhuman cognitive abilities and intelligence [3].The project starts with the assumption that most of human everyday activities can be explained through program induction, and the EV^2 algorithm [1] which contributed a method to grow a DSL and also a neural recognition model that amortizes the cost of searching for the programs, that solves tasks.However, the EV^2 algorithm does not work well in situations where:  1. there is not a high quality corpus of training tasks for it to learn;2. there are latent domain knowledge in the tasks;3. there is some overlap between domains;4. the input data is sparse with respect to its domain;5. there is some contextual information, such as a vector of real value weights, that could easily facilitate discovery of domains in a semi  or indirectly supervised way;6. the initial primitive DSL is relatively universal not domain specific;7. the neural architecture is not suitable for exact task at hand.The goal of this project is to extend the work done in [1], investigating ways to build agents that can also automatically acquire that contextual knowledge needed to navigate different/ new domain.In the end, the new algorithm will be evaluated against the baseline [1] with synthetic, and/or real world, data. A performance measure of the percentage of the solved problem and the time cost can be compared on tasks ranging from list processing, text editing,  handwriting  recognition to synthetic data from causal microworlds.",Re implement and extend the EC^2 algorithm to automatically acquire domain knowledge for program induction.,1,4   Very Hard,"Design and implement a model/algorithm for bayesian program learning, conducting experiments (e.g. list processing, text editing) testing the approach as described in the project description.",8,0,f
4573,Developing tools for audio visual speech recognition,ug4,1996,2020 03 01 11:52:26 00,1,"In noisy environments, humans make use of visual information to aid their understanding of speech from a conversational partner.   For hearing impaired listeners, visual information may be even more important to understanding, yet current hearing aids are unable to make use of such information, and often perform very poorly in noisy conditions   particularly in the presence of multiple speakers.Ultimately, we would like to use video  information  to separate, enhance and recognise speech from a particular speaker.   A promising start has been made by [1],  who have developed methods for training models for lip reading speech recognition on real world video data.   This project will work on implementing initial versions of these methods, with the aim of harvesting a collection of videos with reliably aligned acoustic and lip motion features that can be used for downstream tasks.   This will require integrating a number of auido and video processing tools.   Automatic speech recognition models will be supplied.    ",Build a system for aligning speech audio and video from YouTube style data,2,Variable,Creation of a collection of videos with matched acoustic and lip reading features.,8,0,f
4574,Mobile measurement of urban interference and spectrum utilisation patterns,ug4,1366,2020 03 01 12:05:49 00,1,"Coverage or signal strength allows inferring mobile performance only at lightly loaded network conditions. More commonly, the extent and nature of network load (in terms of number of users, their distribution and service requirements) play a vital role in determining user perceived mobile performance. This can be better understood via more detailed characterization of interference in realistic and challenging mobile network scenarios as in dense urban areas. Increasing density of mobile infrastructure with small cells and the greater heterogeneity of cell types (macro, pico, femto, etc.) as well as impending use of unlicensed spectrum by mobile networks makes interference a bigger and complex concern. With this in mind, the goal of this project is to conduct a measurement study in Edinburgh using commodity Android smartphones augmented with spectrum analyser capability and leveraging the mobility of public transport vehicles. ",Measurement based characterization of interference and spectrum usage in dense urban areas,1,1   Easy,Spectrum measurement methdology design; measurement collection and analysis of data to draw insights.,8,0,f
4575,Experimentation and Optimisation of Mobile Media Delivery,ug4,1366,2020 03 01 12:10:57 00,1,"Efficient delivery of media content to mobile users hinges on being able   to dynamically adapt the mode of content distribution over the air interface (unicast versus different flavours of broadcast) in response to spatio temporal consumption patterns. In order to experimentally understand the potential efficiency improvements from such dynamic adaptation, the first goal of this project is to realise an end to end experimentation setup incorporating the 3GPP specified framework for broadcast services using open source platforms (e.g., OpenAirInterface, srsLTE). The second part of this project will focus on developing and experimenting with an optimised alternative to the 3GPP standard approach that exposes the application layer HTTP sessions in the core to the content provider so as to dynamically steer the traffic through unicast or broadcast modes of delivery.",To setup a testbed for experimental study of 4G broadcast and use it to evaluate an optimised mobile media delivery approach that opportunistically leverages 4G broadcast.,2,Variable,Setting up a working experimental testbed for 4G/5G broadcast studies. Design and evaluation of optimised approaches for mobile media delivery.,8,0,f
4577,Analysis of media content consumption patterns and modelling mobile media delivery approaches,ug4,1366,2020 03 01 12:14:37 00,1,"Efficient delivery of media content over access networks is highly dependant on the spatio temporal consumption patterns. Keeping this in mind, the goal of this project is two fold: (1) to analyse relevant TV/media content datasets (e.g., BARB) to characterise content consumption with respect to various key aspects such as temporal dynamics of content consumption and content popularity distributions. (2) Based on the insights from the first part and feeding it as input to a modelling/simulation framework to assess the potential benefit from using broadcast over the air interface as the delivery mechanism as opposed to the default unicast mechanism. Datasets required for the first goal will be made available. This project could be taken by two students, each focusing on a different goal although they have interdependency to factor in the insights from the first part into modelling/analysis in the second. ",Analysis of media content consumption patterns and modelling mobile media delivery approaches,2,Variable,Data driven analysis of media content consumption patterns using real world datasets to obtain insights; Modelling unicast/broadcast based mobile media delivery approaches to assess their relative pros and cons with realistic consumption patterns.,8,0,f
4578,Unsupervised phonetic speech segmentation,ug4,1996,2020 03 01 15:00:35 00,1,"Phonemes are the perceptually distinct units of speech, from which words are composed.   Phoneme models are an important component of a system for Automatic Speech Recognition (ASR).   For many languages, the set of phonemes is well known.   However, for a new language   particularly a minority or endangered language   it may be very difficult to obtain the set of phonemes without linguistic expertise, especially given the very high dimensional nature of the acoustic feature space.This project will investigate unsupervised approaches to automatically segment a speech signal into phonemes, without any knowledge of the phoneme set itself, sometimes referred to as "";blind""; segmentation [1, 2].   This is an important task because many methods for building ASR systems in such scenarios (often called "";zero resource""; speech recognition [3]) rely on the existence of a phoneme segmentation.   The quality of the segmentation will be measured with respect to gold standard labels obtained from a supervised ASR system.This project could be extended to train systems for zero resource speech recognition, for tasks such as spoken term detection or information retrieval.",Develop and evaluate an algorithm for performing unsupervised segmentation of a speech signal into phonemes,2,2   Moderate,"The project should develop at least one method for blind phoneme segmentation, and evaluate the quality of the segmentation.  Some thought should be given to suitable evaluation metrics for this task.",8,0,f
4576,Unsupervised grapheme to phoneme conversion using Word2Vec,ug4,1996,2020 03 01 12:14:09 00,1,"In spoken language, phonemes are discrete, perceptually distinctive acoustic units that combine to form words, and usually form the basis of acoustic modelling in automatic speech recognition (ASR).   In text data, the equivalent units are the symbols of the writing system, often termed graphemes.   In most writing systems, there is a close correspondence between graphemes and phonemes, and the mapping between the two can be specified using a hand crafted pronunciation dictionary, or learned from a set of training examples.   This mapping is important in ASR to enable speech data to be processed into text form.However, for a new language of interest   particularly if it is a minority or endangered language   it may not be possible to obtain a pronunciation dictionary, as this requires linguistic expertise.      In this case, there will be no training data to learn a mapping between graphemes and phonemes in a supervised manner.  This project will investigate alternative unsupervised methods, where we have both acoustic and text data available from the new language, but no paired data.   Motivated by [1], we hypothesise that it may be possible to learn continuous embeddings for both grapheme and phoneme representations (essentially corresponding to the spelling or pronunciation rules of the language) and thus infer a mapping between the two representation spaces.   This project will investigate the use of representations based on Word2Vec [2], with the potential to investigate alternative embedding methods.   This could be extended to sub word units, using, for example, Byte Pair Encoding [3].     ","Generate continuous representations for phonemes in speech, and graphemes in text, and find a mapping between between the two embedding spaces",2,3   Hard,An investigation of models to generate continuous embeddings for phonemes and graphemes; development of one or more methods to generate mappings between the two embedding spaces.,8,0,f
4579,Automatic segmentation of biomes in medium resolution satellite imagery,ug4,8022,2020 03 01 19:01:25 00,1,"The rate of deforestation all around the world is increasing with every year [1]. This is having a destructive impact on the environment and is the primary threat to the survival of many species such as the orangutan [2]. In some parts of the world rainforests are illegally being cut down to make way for agriculture [2], while in other parts of the world wildfires consume and eradicate the landscape leaving little more than ash behind [4, 5]. If it were possible for  satellite imagery to identify different biomes such as rainforests, grasslands, tundra and deserts etc. then the boundaries between biomes could be recorded and differences in the size of each biome also recorded. This would allow conservation groups such as Amazon Watch   a non profit organisation founded to protect the Amazon rainforest   to know exactly where illegal deforestation is taking place [3].This project is to be split into the following steps:1. Collect a substantial amount of medium resolution satellite images2. Annotate those images which are not already annotated (if any)3. Appropriately split the data into a training and testing data set3. Train machine learning algorithms on the training data set4. Analyse the performance of the algorithms on the testing set and   adapt the algorithm  depending on performance5. Build a toolbox around it for educational purposes where users can input an image with its history and see what has changed over the years.  ",Automatically detect and classify the border between different biomes using medium resolution satellite imagery.,1,2   Moderate,1. Above steps [1][ 3]. 2. Above step [4]. 3. Above step [5],8,0,t
4580,Recording Execution Traces through JAVA programs,ug4,1335,2020 03 02 07:06:06 00,1,"In this project, you will develop a tool to record execution traces of java programs. The instrumentation to record the traces will be done in the SOOT analysis framework (https://github.com/Sable/soot), which is an analysis tool for JAVA programs. The execution traces will gather sequence of method calls, the parent method and class from which it was called, arguments and return values of the called method. The tool will be evaluated over JAVA programs and test inputs from the SIR repository (https://sir.csc.ncsu.edu/php/previewfiles.php).","Build an instrumentation tool to record sequences of method calls, argument and return values, class names for JAVA programs",2,3   Hard,"The tool will be evaluated based on feasibility of using it over all the JAVA programs in the SIR repository, level of automation, and overhead in instrumentations",8,0,f
4582,Approximate haplotype calling from short read sequencing datasets,ug4,1335,2020 03 02 07:27:48 00,1,"Immunotherapies represent ground breaking new therapeutic options for cancer treatment. However, patients respond differently to these therapies, which is partially related to patient specific heterogeneity in antigen recognition. HLA typing is one way to understand these differences. The established standards for HLA genotyping rely on targeted DNA sequencing techniques. Few options exist to call HLA types from short read sequencing data. A methodology for HLA typing from these data would enable a re examination and re interpretation of existing cancer data. The student will assess tools for haplotype calling from short read sequencing data and for patient specific haplotype or multi patient haplotype family calling. The student will apply these tools in high throughput against existing datasets with the end goal of applying machine learning to predict patient response to immunotherapies.  The goal of the project is to develop a new feature over existing locally generated and publicly available cancer datasets and to see if the new feature can be used to predict patient outcomes. Hence, there are many existing datasets examining hundreds of patients to choose from. Cancers of interest include Oesophageal, Sarcoma, Melanoma and Glioma. In this project the student will assess existing tools for working with short read sequencing data for patient specific haplotype calling (i.e. https://www.ncbi.nlm.nih.gov/pubmed/29858812). If the student decides existing tools are sufficient, the student will develop a pipeline of existing technologies that will be applied across a large cohort of cancer studies. If the student decides existing tools are not sufficient, the masters project will be to develop such a tool before predictive modeling. Once a pipeline or tool has been developed, the next stage will be to run the pipeline across a large cohort of existing cancer patient data and to use machine learning strategies to predict patient outcomes from HLA type. Supervision    Dr. Ajitha Rajan will act as the primary supervisor, providing guidance on the software engineering, testing and machine learning aspects of the project.  Dr. Ted Hupp from IGMM and Dr. Javier Alfaro will act as co supervisors, helping with the clinical and biological utility of the project, provide data sets,   as well as support the validation of research findings.","Immunotherapies represent ground breaking new therapeutic options for cancer treatment. However, patients respond differently to these therapies, which is partially related to patient specific heterogeneity in antigen recognition. Understanding how the patients own genetic make up leads to differences in response to therapy is of high unmet clinical need. The project will focus on developing tools to understand differences in patient responses.",1,2   Moderate,Ability to  run the tools developed across a large cohort of existing cancer patient data and ability to use the machine learning strategies to predict patient outcomes from HLA type.,8,0,t
4583,Application for the Rendering of Subdivision Surfaces,ug4,7963,2020 03 02 10:20:25 00,1,"Subdivision surfaces provide a non parametric means of representing and rendering smooth surfaces with arbitrary topology, including those with holes and boundaries. They are represented by a combination of a control mesh and a refinement rule used to recursively split faces or vertices.     Subdivision surfaces were first devised in 1978 and have come to play an important role in computer graphics and animation. Various algorithms have been developed that lead to different limit surfaces given the same initial control mesh. Notable among these the loop, butterfly, Catmull Clark, and Doo Sabin schemes.The purpose of this project is to investigate the mathematics of recursive subdivision algorithms and the data structures needed to represent them, and to implement a computer application that performs 3D rendering of complex geometric objects using a variety of subdivision schemes.",to research a range of algorithms for rendering subdivision surfaces and to implement them in a computer application.,1,3   Hard,"write up of mathematical / computational framework of subdivision surfaces, working application allowing 3D rendering, using a variety of recursive subdivision schemes, of complex geometric objects.",8,0,f
4584,Search and recommendation techniques to enhance rich media in education,ug4,1984,2020 03 02 11:58:23 00,1,"Current educational practice is full of examples of resources being made available with little consideration for how they will be used. Lectures, for instance, are recorded and put on the web, with little support for re use other than simply playing through them. This project is about making these resources more useful by helping to locate and connect other related materials.Videos are available from many sources, but are usually not usefully linked to other material. Documents, notes, slides, are often available but again allow only very limited use. These resources tend to arise from old conceptions of learning based on relatively passive consumption of information. We see a need to support active development of learning, encouraging inquiry led processes in which information is sought out and transformed, assisting finding and exploiting materials that can be reworked and integrated.The objective of this project is to investigate intelligent search and recommendation techniques, specifically in relation to educational rich media materials. This will involve surveying the types of resources that exist, and working to develop methods for automated analysis of them. This can include e.g. topic modelling, event segmentation, methods based primarily on analysis of speech, text or imagesAnother potential focus is around ontology construction. Recruited materials will need to be integrated into a developing context in which learners are creating their own conceptualisation of a learning domain. To the extent that resources may have metadata when found, it will require adaptation; in many cases, metadata will have to be constructed. This will not occur within a fixed ontological framework, but rather in relation to one that is dynamically emerging. Therefore, techniques of ontology acquisition and adaptation will need to be extended.","To investigate intelligent search and recommendation techniques, specifically in relation to educational rich media materials",2,Variable,Negotiable. A substantial feasibility study would be very valuable. Some practical development work is certainly also desirable.,8,0,f
4586,Machine Learning for Earth: Using computer vision and machine learning to monitor land degradation caused by climate change,ug4,7989,2020 03 02 15:31:25 00,1,"IntroductionLand degradation is the consistent loss of ecosystem functionality due to human activities and climate change. It affects biodiversity and ecosystems, ultimately impacting the well being and livelihoods of 1.5 billion people globally (1). In 2015, the United Nations designed 17 Sustainable Development Goals, aimed at ending poverty and protecting the planet. One of these goals   Sustainable Development Goal 15   aims to directly   protect, restore and promote  sustainable  use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse  land degradation  and halt biodiversity loss .A number of non profit organisations have been working with governments in order to assist with land optimisation projects in order to meet this Sustainable Development Goal 15. One of these key organisations is  Conservation International    a large non profit environmental organisation based in Washington D.C.The research team at Conservation International's Moore Centre for Science have been working in partnership with Lund University, NASA and with support of the Global Environmental Facility to develop  Trends.Earth; an online tool that uses various global data sources to monitor land degradation. Full details about Trends.Earth along with reports can be found here:CI Trends.Earth webpage:  https://www.conservation.org/about/trends earthTrends.Earth website:  http://trends.earth/docs/en/Github Repo:  https://github.com/ConservationInternational/trends.earthHonours Project DetailsIn this honours project, I will work in collaboration with the research team at Conservation International's  Moore Center for Science  in the second phase of Trends.Earth to develop a tool for monitoring the effectiveness of individual sustainable land management projects using machine learning and computer vision.There are a number of data sources that can be potentially used for the purposes of the project. A full range of sources are listed in this very useful github repo:  https://github.com/rockita/Environmental_Intelligence. Others can be found below:Multi Use DataGoogle Earth Engine: A planetary scale platform for Earth science data &amp; analysisLand CoverESA CCI Land CoverPrecipitationGPCP v2.3 1 monthGPCC V7CHIRPSPERSIANN CDRSoil MoistureMERRA 2ERA IVegetationAVHRR/GIMMSMOD13Q1 coll6The outcome of the project would be to develop a mapping tool to monitor land management projects aimed at limiting land degradation. Furthermore the tool will assesses the potential ecological and economic impact of the projects, estimating the financial cost of depleting natural capital caused by climate change.Skills RequiredGood programming skills (Python, R)Experience with using web APIs and JSONGood understanding of machine learning, especially computer visionUnderstanding of basic environmental science: ecology, biology and geography","Collaboration project with Conservation International to develop maps for monitoring land degradation using remote sensing datasets, satellite data and global data tools",1,3   Hard,"1) Develop a mapping tool to monitor land management projects aimed at limiting land degradation. 2) The tool will assesses the potential ecological and economic impact of the projects, estimating the financial cost of depleting natural capital caused by climate change.",8,1,t
4587,Building a Music Recommendation System,ug4,6996,2020 03 02 17:30:58 00,1,"A recommender system takes data about a user's ratings or interests in something and for a new item compares the similarity between it and a user's previous activity, or the similarity between users, to predict how much the user would like the new item. This project will involve building a recommender system using data from a music social media site which provides information about artists, songs, and albums, to learn what to recommend to a user based on their previous listening or interests. This will include researching what approach is best to take for recommendation (content vs collaborative filtering), finding data and analysing what attributes are most relevant for similarity finding.  ",To build a recommender system that uses data from a social music website to recommend music to users based on their listening history.,1,2   Moderate,Demonstration of a system that successfully makes recommendations to a user based on their music taste.,8,0,f
4588,Modern Analog Computing,ug4,5861,2020 03 02 18:05:39 00,1,"Analog computers have served humanity for a very long time, first appearing in the Ancient Greece to predict movement of celestial bodies (Antikythera Mechanism). They had been widely utilised until 1970s when the digitalisation took over, and analog devices have been mostly forgotten [1]. However, we are approaching a point at which digital architecture is reaching its limits in terms of computation time and efficiency, and some research suggest that in order to move forwards we should reconsider coming back to our roots [1,2,3]. Analog architecture has multiple advantages over digital. It represents numbers via encoding them in continuous signals, for example voltage and current [2], in contrast to bits. Moreover, it has no clock [1,2]   iterations are done over continuous time, while digital computers use discrete time steps. As a result, it can process a lot more information at a time. It is also straightforward to implement gates not present in the digital paradigm, for example analog integrators [2]. Hence, such an architecture is desirable in solving differential equations governing, for instance, biological systems [3]. Another benefit of using analog is continuous reaction to the input signals, which is useful when using analog sensors [1]. Naturally, analog computers have also their downsides. They are not as versatile as digital computers, and are fit only for very limited purposes [1]. And even when solving one problem, they have to scale with the problem's size, as they don't have dynamic memory [2]. As a result, an Analog Digital Converter (ADC) and Digital Analog Converter (DAC) are needed to provide an interface with the digital system, and they are usually inaccurate, making analog computers useful only for approximate results [2]. This project focuses on research, design and development of an analog/hybrid system that takes advantage of the benefits that analog computers provide over digital architecture, and attempts to mitigate the risks involved. The nature of the system is to be determined in the project. Potential ideas include a small robot performing simple actions based on continuously processed analog inputs (NASA Venus Rover [4] is a good model, although probably too complex for this project), or an analog accelerator for simulating non linear physical systems. The design will be simulated using a SPICE tool to validate its functionality [5], and potentially manufactured. ",Develop an analog system,1,3   Hard,1. Develop a well justified idea for the system. 2. Design and simulate the chosen system. 3. (Optional) Manufacture the system,8,0,f
4525,Developing an app for children in hospital,ug4,5128,2020 02 24 15:03:40 00,1,"Play is an important part of childhood development, but children who are unwell may have fewer opportunities to do so. The purpose of this project cluster is to develop an iPad app suitable for children who are hospital patients so that they can have fun, stretch their imaginations, relax, escape, be creative and reach out to friendsWe have a prototype app and art assets developed by a team at Moray House School of Education and Sport and Edinburgh College of Art. During this project, the   student will extend the app to include a range of additional games and interactive creativity tools which all fit around a central theme and characters (including Lolly the Space Sheep). The student will decide which age group they wish to target; decide which type of play they wish to support; design the user experience and interface; implement their design within the existing code base; evaluate their app with children. An iterative user centred design process will be used, with healthy children as co designers.If you would like more information, come to an information session on 25th March at 1pm online at  https://eu.bbcollab.com/guest/835918614f404cc2aca6f32df28ca212  come to a face to face session on 18th March at 11.30am   The face to face session is cancelled.However, I strongly encourage you to  sign up for the project here so I can  make  your suitability  before  then:  https://forms.office.com/Pages/ResponsePage.aspx?id=sAafLmkWiUWHiRCgaTTcYfGGyJ7a27VCp2bYzy08L0RUNTJPUTRFRVJSNlZHVVJJQkpYWkJCMFRROC4u",To develop a mini game or feature for an iPad game for child users in a hospital,2,1   Easy,"The project is complete when the student has designed, implemented and evaluated an appropriate game or creative application suitable for children in a hospital setting",8,0,f
4589,"A smart button for recording ""teaching moments""",ug4,5128,2020 03 02 20:18:30 00,1,"In the Data Education in Schools project, our purpose is to educate young people about data (https://dataschools.education/). One of the main ways we do this is through professional learning for courses for teachers. We would like to record the impact our courses have on teachers' practice in the classroom, but this is quite hard using traditional research methods. It would be possible to issue a follow up survey for teachers to ask them to remember when they used their new knowledge in the class, but surveys often have low response rates and are retrospective. It would be possible for a researcher to visit the class to observe lessons, but this is costly and unlikely to capture naturally occurring behaviour. Time use researchers would use diary techniques to prompt participants by text message to ask them to complete a diary entry in an app at regular intervals, but this is intrusive and distracting for participants.However, this project proposes a novel method of gathering this information in real time. Using LORWAN and a smart button, teachers will simply press small button (which they will keep in their pocket or on their desk) to indicate that they have used the content of a professional learning course in a "";teaching moment""; in their class. The device will send a time stamped message with user ID to the LORAWAN gateway at the University of Edinburgh where it will be stored in a database. There will be a visualisation tool which shows the Data Education in Schools team a graph of teaching moments by time and user over the course of a year.This project would suit a student who has previously studied electrical engineering.&lt;Note: the student will have support from a staff member in Electrical Engineering at UoE&gt;",To develop a prototype smart button which teachers can use to record when they use particular teaching techniques,1,2   Moderate,"The project will be completed when a small wireless button has been developed. When pressed, sends a message via the LORWAN gateway to a University of Edinburgh server to indicate that a ""teaching moment"" has happened with a time stamp and user ID. There will also be a front end visualisation which shows the data of ""teaching moments"" recorded by multiple devices over time.",8,0,f
4590,CPU Design using the RISC V Instruction Set Architecture,ug4,7060,2020 03 03 00:41:25 00,1,"This is a project in which I will create a RISC V CPU design. This will include implementation and testing of the design.This project will be conducted with my previous experience with Xilinx Vivado and the Zync 7020 from the Computer Architecture and Design laboratory [1].The main development platform will be a Xilinx Zync 7020 FPGA running on a PYNQ Z2 board from Tul[2]. An attached dual core ARM Cortex A9 host system will be the front end to the RISC V processor (implemented within the FPGA fabric). This will load programs into the target system's memory and run the target processor (or single step it), displaying the result of every instruction.  CPU design involves the following aspects: micro architecture, hardware design, hardware verification, performance modelling, physical design implementation (this includes synthesis, place and route etc.) and compiler optimisations. I plan to focus on as many aspects as possible to build a verified RISC V CPU with as much performance as possible.The processor will be built upon the open source RISC V ISA [3] which is starting to build momentum. As it has become evermore popular in industry due to its free to use instruction set, good quality software such as compilers, binutils, etc have also been created.",To Design a RISC V processor to be Implemented,1,Variable,A working RISC V Processor ready for tape out,8,0,f
4591,Data Mining Wikipedia,ug4,8152,2020 03 03 05:29:00 00,1,"Wikipedia is at the centre of high quality debate on the internet due to its high popularity, diverse user community, and highly collaborative editorial base. The website currently has good vandalism detection tools for individual edits, but has no tools that classify users on their behaviour over time. This puts excessive load on volunteer moderators as they need to go through a user's edits manually to see if there is a pattern of bad behaviour. Not only does this waste valuable volunteer time, it can only catch the editors with the most egregious editing histories.I am currently doing research in this area during my study abroad year at the University of Virginia. This entails the creation of a database of all edits on Wikipedia since 2001, for subsequent classification of good and bad actors.  As the project is yet to be completed, it is unclear what specific questions will be of interest. However, this is a fertile area of research with applicability for network analysis, natural language processing, classification, prediction and digital humanities. This work will also strengthen the University of Edinburgh's current ties with Wikipedia and encourage future research into the mentioned areas. Finally, as Wikipedia is a multilingual project, my work will hopefully benefit people internationally.I will have the tools to create the database by the time I finish my current research, so I currently imagine future steps to be:  Detection of more complex forms of misconduct, such as  complaining or discussion in  bad faithDeveloping a system that will ingest information in an online fashion and flag users that have reputation scores that are decreasing. This is so that it can be integrated into the Recent changes feed.Creating online community discussion around adding this feature to Wikipedia   many avenues to do this with and without endorsement from the Wikimedia Foundation.  Disclaimer: I have contacted Walid Magdy and Mirella Lapata at Edinburgh however they have not replied due, I assume, to strike action. I could not attempt to meet them in person as I am currently studying abroad in Virginia. I am passionate about this project and would like to continue my work as I think it is important, so I ask you to take this into consideration and allow me to contact them before my proposal is dismissed.","To classify actors on Wikipedia with a variety of techniques for flagging of ""bad"" users",1,Variable,Detection of complex forms of misconduct. Creating a online model that can flag actors in real time,8,0,t
4592,Food Supply Tracking through Blockchain,ug4,7959,2020 03 03 08:20:46 00,1,"To create a food supply tracking system using blockchain. It will serve in tracking contamination and diseases, the origin of raw ingredients: location, sustainability standards, organic standards, etc.  I also want to integrate carbon credit, health effects, water and air pollution caused, ozone depletion, etc. into the system, time permitting. This system can be implemented in sectors such as clothing, electronics, and other consumables.The aim is not to create an end app for the individual user, but to accurately track and safely store the data, which other companies can use in a multitude of ways.The idea is that by creating transparency, we will be driving appropriate self desired behaviors; by opening up about their ingredients, energy, labor, etc., companies will be incentivized to create less waste, cleaner energy, lesser water, etc.",Have a self reliant system that is easy to use and hard to tamper with.,3,3   Hard,"Build a blockchain that is easy to use, a logic easy to follow and data hard to tamper with or skew. Time permitting, I would like to try to accurately calculate carbon credits, water and air pollution, health effects, sustainability and organic standards, etc.",8,1,f
4593,Exploring gamification techniques in children's computer science learning,ug4,7907,2020 03 03 10:47:56 00,1,"This project is focused on investigating how systems for games based learning of computer science can be developed, the effectiveness of gamification elements and the challenges developers must overcome to implement them.Games based learning is using digital games together with traditional teaching practices in education. (AdvanceHE, 2017)An example of this in the UK is Sumdog   the company makes educational games for learning maths and spelling. The Sumdog January 2020 impact report claims that regular Sumdog users made double the progress over 1.5 years in maths fluency than non regular users (Sumdog, 2020)Gamification in education is using principles of game design in non game contexts, such as learning exercises. (AdvanceHE, 2017)An example of this in New York City is a public school called Quest to Learn, where the study curriculum and assessments are all game based. The teachers meet with game designers and curriculum designers every trimester to plan learning activities. The school boasts an average student attendance rate of 94%, and that they outperform other local schools: 54% of Q2L students passed the 2015 ELA exam, while the pass rate city wide was only 30.4% (Quest to Learn, 2020)These examples show clear benefits of education that employs games based learning. However, despite the benefits, such learning systems have not replaced traditional learning, and one of the main reasons is that they are time consuming and difficult to implement. And even then, they only target a specific target audience. (Dicheva et al., 2015)There is a lot of literature about gamification, that could provide useful insights (Hamari, Koivisto and Sarsa, 2014). Furthermore, existing systems, such as Codecademy, Codingame, CodeCombat, Hour of Code projects all provide gamified learning experiences for computer science fundamentals, which, if the project topic was computer science, would also be useful reference points for finding and summarising different elements of gamification.  The key questions to investigate are:How elements of gamification (e.g. leaderboards, coins, points, badges, instant feedback, achievements, or others) impact learners' engagement?How can elements of gamification be implemented (such that the implementation is accessible, with e.g. poor internet connection, low end devices, limited technology)?Initial project plan:Choose a specific topic of learning, examine existing literature and work into gamification and children's learning of the topicIn consultation with teachers and game designers, gather requirements for a prototype system to explore gamification techniquesDevelop a prototype game based learning for primary school childrenEvaluate system in local primary schools to see the impact of gamification on children's engagement with the topics, motivation, and self efficacyImprove or broaden scope of the system and repeat",Investigate how digital game elements in education can impact primary school learners' engagement and how they can be implemented in practice,1,Variable,"Easy: The project will be complete when a prototype containing at least 3 principles of gamification has been evaluated with at least a small group (5 individuals) of members of the target user group. Moderate: The project is complete when at least 5 principles of gamification are implemented and evaluated with at least 3 cross validated groups of at least 5 individuals, as well as with a control group. At least 1 feedback development cycle should be completed. Hard: Different implementation approaches should be discussed, including research into feasibility of adopting the system in schools, costs and compatibility. The system should be compatible with multiple operating systems / devices. Very hard: An entire framework to facilitate development digital games for supporting learning could be developed.",8,0,f
4594,Biologically Inspired Multiagent Pursuit Evasion Models,ug4,7867,2020 03 03 11:26:46 00,1,"The pursuit evasion game is a common bio inspired computing model in which a predator agent attempts to capture a prey agent, and the prey agent attempts to evade capture, with applications in mobile robotics [1]. The environment is often modelled as a graph for the sake of simplicity, and usually there is only one prey and one predator modelled. However, recent work has extended this work to multiple agents, with promising results [2].In this project, a simulated pursuit evasion system will be created, and various pursuit and evasion tactics will be employed for packs of predator agents and swarms of prey agents. Strategies for the hunter agents will be drawn from pack hunting behaviours that occur in nature. This may include various cooperative foraging and pack hunting insects such as ants, as their relative homogeneity lend themselves to the task well, and various other pack hunting species, such as Harris hawks, orcas, wolves, and more. Similarly, prey agent strategies will be drawn from flocking animals and swarms, such as herring, krill, and antelope.Ideally, this project would take place over two years for a masters project, with the main component comprising the design, implementation and analysis of a simulated multiagent pursuit evasion system, modelling and comparing the behaviours of a selection of different predator and prey species. Possible extensions of the project could consist of the extension of the system to three dimensions, refining the model to fit a certain application, or ideally, realizing the model with robotic agents, however there would be significant cost considerations with the latter.",To extend the model of the pursuit evasion game to multiagent and swarm systems as inspired by pack hunting and herd flocking behaviours.,1,Variable,"Create a simulated system in which multiple software agents participate in the pursuit evasion game, employing biologically inspired methods of cooperation and coordination to optimize behaviour. Quantitatively and qualitatively analyse various strategies for their efficacy and efficiency, in particular with regards to scaling up to realistic numbers of agents. Ideally, implement the most promising strategy with a number of mobile robots to analyse effectiveness in the real world when dealing with various sources of uncertainty and greater restrictions on sensing and movement.",8,0,f
4595,Transcription of Music into Drum Sheets,ug4,8063,2020 03 03 14:01:51 00,1,"  Aim:Sometimes it is painful for a drummer to find the sheet of whichever song he/she is searching. Either he/she has to write the sheet by his/her hand or buy it online if exists. Since I have been playing drums for 7 years, I understand this situation and would like to find a solution for it.This project aims to the musicians like me to transcript the pre recorded song that we would like to play or a live played music which we would like later on play or integrate into a song.  Road map:                  Please bear in mind that this is not the latest version but a first draft of a road map.              (1) Investigating on how to extract the drums from the song to create more encapsulated and accurate transcriptions. Later on there will be analysis on finding the genre and                                BPM (beats per minute) of the song.              (2) Investigating on how to translate the song or a piece of recording into its waveform to later on analyzing its amplitude, period, frequency, wavelength. These information                                 can give insights about which wave belongs to which component of the drum kit (just an initial search online ¹  and found the range of frequencies of the components, but this                           will be later on deeply investigated by a help of sound engineer)              (3) Analyzing according to the genre and time signature to recognize patterns in the waveform will be done. Segmentation and feature analysis on each segmented piece will                                be done and might be later on passed to a machine learning algorithm.              (4) After the machine learning algorithm, each segmented piece will be labeled. With this knowledge and the knowledge of BPM, the note values eg. breve, semibreve, minim                             to structure the drum sheet to its final version.  Open source libraries which might be useful:            (1) A Github open source for encapsulating and extracting the drums from the song:  https://github.com/deezer/spleeter            (2) A library called PyAudio to translate sound into waveform:  https://github.com/jleb/pyaudio  References:1.  https://www.musical u.com/learn/percussion frequencies part 1 drums/","To translate a piece, which can be either live or pre recorded, to its drum sheet",1,2   Moderate,A program which will accept a piece of recording and translate it into its drum sheet,8,0,f
4596,Improving read accuracy of floppy disks using software techniques,ug4,5061,2020 03 03 14:44:51 00,1,"Since the late 1990s floppy disks have gradually been superseded by more modern and advanced technologies such as rewritable CDs and DVDs, and USB memory sticks. However, despite their obsolescence, floppy disks are still used in industries where the rate of adoption of modern technology is low such as government and critical infrastructure systems. For example, it was only in 2019 that the US Defence Department transitioned away from using eight inch floppy disks in the US's nuclear command and control systems [1].As time progresses, the reliability of the floppy disks themselves is an increasing concern. The reliability of disks that have been stored for years or even decades cannot be guaranteed, despite some of these disks containing critical and/or unique information. An industry has grown to support migration from these these legacy systems, including products such as the KryoFlux [2] and FluxEngine [3] that are able to produce large raw images of disks, suitable for later analysis in software to identify and potentially correct errors that occurred during the read operation.Despite their names, these products do not actually read the raw magnetic flux as it is read from the disk by the read/write head of the floppy drive. Instead they connect to the standard interface on the drive (the 34 pin connector in the case of 3.5 inch floppy disk drives) and as such, the signal that they receive is a digital signal that has been processed by the controller circuitry on the floppy disk drive. The controller board on such drives has a number of conflicting requirements, including accuracy but also speed. Therefore several trade offs will have been made in the design of these boards to ensure a suitable balance of accuracy and speed given the requirements of the time. These requirements have now changed, with a far greater importance placed upon accuracy and reduced emphasis on speed, particularly if the goal is to make a single 'gold master' of floppy disk for archival purposes.The goal of this project is to investigate whether modern software techniques can result in increased accuracy when reading floppy disks, particularly in the cases where existing systems produce corrupted data or are unable to read the disk at all. The system as it is envisaged currently would consist of two subsystems:A hardware circuit board to replace the floppy disk drive controller board and provide a very high resolution analogue read of the raw magnetic flux data as read directly by the head in a floppy disk drive.A software system to read the analogue data returned by the hardware system and perform a variety of signal processing and pattern matching techniques in order to produce a raw digital disk image of the floppy disk.The majority of the work on the project would be on the software system. This would allow for investigations of various techniques such as signal processing (identifying weak transitions in the magnetic flux that the hardware floppy disk controller would not have identified) and pattern matching (recognising that the data format of certain types of floppy disks is rigid, and therefore only a subset of all available codewords are valid) in order to produce a system that is more accurate at reading floppy disks than the existing solutions.",To determine if the accuracy of floppy disk drives can be increased by replacing the hardware controller board with a software solution,1,Variable,"A system that is able to produce a raw digital image of a given floppy disk, along with detailed analysis and evaluation comparing the accuracy of the system against the existing solutions available on the market.",8,0,f
4597,Solving the Plumbing Problem in Web Components using Lenses,ug4,3595,2020 03 03 15:10:47 00,1,"Most web component frameworks define a component in the following terms:1. The type of an internal "";state"";2. A "";render""; function to turn the internal state into HTML, optionally building further subcomponents3. The type of "";properties""; provided / "";messages""; sent from a parent component4. An "";update""; function for converting those properties / messages to state changes5. Optionally  in strongly typed pure languages, an algebra for side effectful requests / in browser behaviourThe framework will then provide a series of "";hooks""; for activating and listening to automatic behaviours provided by the framework, such as rendering of the component, or triggering of state recomputations.Unfortunately, the property system introduced in parts 3 and 4 often have issues when facing deeply nested or complex UIs, in that data must travel up and down the tree through multiple components. This is tackled with one of three solutions:1. The programmer must write a large amount of error prone "";plumbing""; for propagating data both up and down the tree to and from properties.2. We use an external system to receive and send messages in a publish subscribe model (for example, Redux).3. Combine subcomponents with their parents into a single monolithic component, so that state is immediately accessible to subcomponents.These three methods each have their own drawbacks. The first risks that the programmer mistypes an argument or misunderstands a property in writing their boilerplate, and is time consuming besides. The second eliminates many guarantees of encapsulation in that components can listen to and act upon each other's state changes, and also requires some boilerplate. The third eliminates the concept of UI reusability, as subcomponents become tightly coupled to the logic of their containing monolith.We propose to try to apply concepts of lenses and/or other bidirectional data systems to allow properties to be propagated in both directions rapidly and simply. Furthermore, using lens composition, any two jumps for two layers of the tree could feasibly be turned into a single jump for both, allowing component designers to build more complex components that are still easy to query by packaging the lenses for their properties. Finally, the use of prisms and traversals could add yet more capability to lens design, allowing a single lens to update and query the state of large swathes of components simultaneously.",Attempt to solve the issue of well typed communication between components in web uis that are deeply nested using lenses' ability for bidirectional data transfer.,1,Variable,"Design component system within which web lenses operate and compose. Design prism system for multiple targets. Use system to implement a simple webapp, such as a Todo.",8,0,f
4599,Multi Level Cache Sizing Strategies and their Efficacies at Scale,ug4,6714,2020 03 03 15:52:29 00,1,.,"Investigate cache hierarchy sizing strategies, their effectiveness compared to existing multi level cache systems, and their effectiveness when used in a large memory system (e.g. a network)",1,Variable,"1. Research existing cache hierarchy sizing strategies with focus on read/write speed constraints. 2. Simulate these strategies in a virtual cache system, comparing against other simulations of existing cache systems. 3. Use these strategies at a large scale to determine optimal node sizes in a network, given read/write speed constraints, and report their effectiveness.",8,0,f
4598,Improved navigation of recovered teletext data,ug4,5061,2020 03 03 15:43:39 00,1,"In the past ten years, there has been an increase in interest in the recovery of historical teletext data from off the air TV recordings [1] [2]. This is due in part to the uniqueness of such data: weather reports, travel information, and news articles published by the BBC in an electronic format existed almost exclusively on teletext until the introduction of the BBC News website in the late 1990s. Initial recovery attempts required S VHS tapes that captured higher resolution recordings than standard VHS tapes, however recovery of teletext data from regular VHS tapes is now possible due to advances in deconvolution, image processing and signal processing algorithms.Despite the work done to extract the teletext data from tapes, little work has been done to improve the user experience in navigating the extracted data. Work is typically confined to emulating the teletext experience as closely as possible   including emulation of the slow loading times and limited navigation capabilities provided by the "";page number""; system, where each page (one TV screen sized amount of data) can be accessed by a numerical index.The goal of this project would be to investigate ways to improve the user experience of browsing recovered teletext data. As a fairly open project, there are a number of ways in which this could be achieved, including:  Development of a "";Wayback Machine""; styled viewer [3] to enable users to view the data through the time dimension as opposed to just the data available at one particular point in time.Linking together of related articles and data, including articles and data that were originally broadcast at different times. (For example news about canvassing for the 1992 general election could be linked to the election results).Although the deliverable for this project would be a software system for navigating the data, much of the project would be spent investigating improved methods of user experience specific to teletext systems, and analysing the possible options through methods such as user studies.",Creation of a system that improves the navigation and access of recovered teletext data,1,Variable,"A new system for navigating and accessing recovered teletext data, along with detailed analysis and evaluation of the improvements in user experience that this system delivers.",8,0,f
4602,Online Repository of Teaching and Learning Evaluation Approaches II,ug4,2097,2020 03 04 13:53:37 00,1,"The evaluation of teaching and learning, through a continuous process of feedback collection and gaining insight into students' learning experience, is crucial for effective teachers [1][2], in order to improve their teaching and, consequently, their students' learning and satisfaction with their studies. In the University of Edinburgh, it is  seen as one of the strategic principles/priorities across all three colleges in our university  [3][4][5]. However, the main, official, way of conducting an evaluation of teaching and learning for any credit bearing course in our university, the Course Enhancement Questionnaire ([6]), offers insufficient feedback to teachers. This is why schools also advise them to conduct their own individual evaluation. For this purpose, teachers have a choice of either using off the shelf approaches, which may be too general to give them valuable enough information, or develop their own  innovative  approaches. Such innovative approaches may be better catered to the context of use (discipline, type of course, student profiles, environment, etc.), needs of the teacher and course team, and any constraints such as time available for analysis.Anecdotal evidence shows that some teachers have developed very interesting innovative approaches. Unfortunately, there is no way for them to share them, or their experience with them, with other colleagues.  To address these problems, in 2018/19 an MSc student gathered requirements from academics in the School of Informatics and designed the UI of an online repository of teaching and learning evaluation approaches using the Figma design tool [7]. The prototype repository contains pages for creating blogs for teaching evaluation approaches and their evaluation, sharing them, filtering/searching through blogs posted by other users, reviewing them, adding them to favourites, and setting up email notifications to receive blogs with certain characteristics.  The results of the prototype's evaluation with 4 teaching support members of staff and 6 academics were very positive, with many participants seeing the repository as useful, well designed, quick to use, and with good potential impact in our school. Moreover, it was suggested that it could also be very useful for other schools and the university as a whole.Going forward, this project's main aim is the continuation of this work through the implementation and evaluation of the online repository of teaching and learning evaluation approaches.  Students may choose the following steps in addressing this project:Review literature on teaching and learning evaluation approaches which were found useful in the Sciences and, in particular, Informatics.Review educational repositories and blog systems which constitute related work.Review design guidelines for educational repositories and blog systems.Implement the online repository following the screens designed in previous work, the feedback received during their evaluation and what was learned in 3).Populate the online repository with the description of approaches from 1).Evaluate the usability of the repository with teachers from the University of Edinburgh.This project is intended for 1  student.We will arrange to meet with you for a chat before the project deadline.","Designing, developing and evaluating an online platform for recording, building, sharing and reviewing approaches of teaching and learning evaluation",1,2   Moderate,1.     A review of approaches to teaching and learning evaluation used in the literature. 2. A review of related educational repositories and blog systems. 3.  A high fidelity prototype (e.g. website) of an online repository of teaching and learning evaluation approaches. 4. Evidence based evaluation of the prototype with teachers from the University of Edinburgh,8,0,f
4603,"The MarkEd tool for Marking, Feedback and Moderation II",ug4,2097,2020 03 06 18:53:01 00,1,"Marking is undoubtedly one of the most important teaching activities . It is a central part of assessment, which is crucial to foster learning as well as the relationship between teachers and learners as stated in the University of Edinburgh's latest Taught Assessment Regulations [1]. As an essential component of marking, offering students feedback  can help them check understanding and progress in a course, clarify expectations, offer encouragement, identify where they can improve and be constructive as to how. Finally, moderation is the important process of overseeing the marking process by academics, to ensure that there is  fairness and consistency between and across markers.In Informatics, marking, feedback and moderation processes vary considerably between courses due to several reasons including the nature of the courses and their assignments (e.g. a programming assignment vs. an essay), a lack of standard tools covering all needs, and a lack of integration between these tools. This results in difficulties to monitor and work towards improving these processes as a whole, and sometimes in inconsistent outcomes affecting students.  To support the different ways of doing marking, feedback and moderation, but also combine the advantages while also reducing the disadvantages of the existing used tools, two undergraduate students have worked this year towards developing the MarkEd online tool (website). One of them has developed part of its back end and tested its architecture. The other has come up with a low fidelity prototype of the UI design of the tool in Figma [2], in the form of interconnected screens.    The aim of this project is to continue their work and move MarkEd closer to a finished product. This can involve the following steps:1. Continuing the development of the back end, as populated with mock data2. Developing the front end to match the screen designs3. Carrying out at least 2 iterations of this development, with evaluation involving academics, teaching support staff, students and administrators after each4. In the final evaluation, assessing both the usability but also potential impact of the tool for different user groups  This project can be taken by at most 2 students, who will work on different parts of the tool and split the responsibility of seeing potential users.","Continuing of the work of previous students, working on the implementation of the MarkEd tool and its evaluation with academics, teaching support staff and students in Informatics",2,2   Moderate,"increments to the development of the MarkEd tool, involving both back end and front end development, usability and impact evaluation involving academics, teaching support staff, students and admin staff",8,0,f
4601,Database Support for Minority Language Policy Research,ug4,1338,2020 03 04 11:31:49 00,1,"This is a continuation of a previous MSc and MINF project, the software for which is available at  https://github.com/Remy Paige/seeker; this is built using the Ruby on Rails web framework.    The original project proposal (slightly amended) was as follows:The Council of Europe requires member states to produce reports on their minority languages (roughly every three years). A committee of experts then produces an evaluation report and this is followed by a recommendation from the committee of ministers; see  http://www.coe.int/t/dg4/education/minlang/Report (the reports are all in English).The documents all follow a set format (for each type) and are mostly in pdf. However there are by now many of them so that a researcher in minority languages who wishes to extract comparative information is faced with a very lengthy task since the data is spread in the text. The documents are of varying sizes but not enormous; a sample varies from 57KB to 2.1MB. Example questions are: (i) Show all Committee of Experts recommendations under article 7b. (ii) Show all sections relating to Scottish Gaelic.This project has two aims:1. To carry out research and recommendations on appropriate tools. The most likely solution will be either to adopt a relational database approach or a weblog infrastructure (part relational, rest free form), a possible combination is MongoDB and Node.jsA background consideration is that any solution should be adaptable to other scenarios. A particular future one is concerned with the Gaelic Plans that are required to be produced by various bodies in Scotland. These are not in any fixed format but some abstract structure common amongst them and the EU documents will be considered if time allows. (This feature makes the project particularly suitable for MInf students.)2. To implement enough functionality identified in (1) as a proof of concept. Ideally documents in word and html format should be handled but for this pilot study it is acceptable to restrict to pdf format (ensuring that the extra functionality can be added at a later time).Both phases will involve some consulting (arranged via the project supervisor) with Prof Rob Dunbar or Prof Wilson MacLeod from Celtic and Scottish Studies who will provide more detailed user requirements.It is a condition of this project that all software used is public domain and that any new software will also be declared as public domain, the software must be placed on a publicly accessible repository such as github. The main components will be a server (provided as a virtual machine) with web based methods for queries, providing both administrator and user roles. The software must be fully documented, including any functionality that is either missing or is not optimal due to time constraints. This is very important as it is intended to build on the results with a follow up project and a migration to other infrastructure, thus the project would have a future use.All students interested in this project must contact the proposer for a discussion. Please note that no knowledge of minority languages is required.The existing software has achieved the main aims and offers good functionality but there are many improvements/additions possible.   If time permits the scope can be extended to incorporating Gaelic plans that are a requirement from public bodies within Scotland, knowledge of Gaelic is not required for this.Please note that it is a precondition of this project that all the code is open source and the result is placed on a public repository such as GitHub.",Design and implement a database and tools for searching a body of documents.,1,4   Very Hard,A significant addition to the existing software and full deployment on an Informatics virtual server.,8,0,f
4604,Extensive Evaluation of State of the Art Network Stacks,ug4,7552,2020 03 08 22:22:32 00,1,"Network stacks are a key software component that bridges applications and physical networks. As networks get faster and faster (e.g., &gt; 10 Gbps), and requirements get severer (e.g., message oriented workloads and low latency), despite of the slow evolution of CPUs, the efficiency of network stacks has been crucial in modern networked systems, directly affecting application performance. Although many novel network stacks, such as Seastar[1], PASTE[2], TAS[3], mTCP[4] and IX[5], have been proposed in this problem space, no one knows which stack is the best in a given condition, as they have never been extensively compared each other, for example, against various application workloads, network conditions, energy efficiency, latency, available hardware resources and throughput.   This project will make extensive evaluation of these state of the art network stacks and identify their fundamental and practical benefits and problems.This project can be taken by two students, each focusing on experimenting with different network stacks. Given the importance of the problem, this project may produce a highly visible paper.",The goal is to identify benefits and problems of the state of the art network stacks. This study will measure performance and features under various workloads and network conditions.,2,3   Hard,"Measure performance of multiple relevant network stacks (note: that they are often research prototypes, so would need debugging and/or optimizing code) in their best conditions (e.g., hardware and OS parameters) under various workloads (note: would need to implement relatively simple applications on top of the given stack).",8,0,f
4605,Tools for teaching Computation and Logic,ug4,1374,2020 03 10 16:27:54 00,1,"The Computation and Logic strand of Informatics 1A introduces students to logic and finite state machines.  This project concerns the development of tools to support conceptual understanding of the formal manipulations that students are required to master.  One goal for this year is to    develop tools that link directly with the Haskell content of the course.Sudoku visualisation in Haskell, produce an interactive sudoku solver designed to help students understand the DPLL algorithm taught in the CL strand of INF1A.Development of a self paced collection of exercises for one or more topics in the course, with automated marking and feedback.A graphical 'blocks world' style tool that will allow students to develop an understanding of syntax and semantics.  ",To develop tools that will support teaching and learning in INF1 CL,5,Variable,"Basic completion will require the development of an interactive tool that engages users in the chosen formal manipulations. A good project will develop models of user understanding, and misconceptions, and use user testing to develop interfaces that support learning. There is plenty of scope for an excellent project to go beyond this, for example with individualised user modelling, or by providing feedback to instructors identifying common misconceptions.",8,0,f
4606,A compiler (in Haskell) for FDDL,ug4,1374,2020 03 10 17:33:12 00,1,"FDDL is modelled on the Planning Domain Definition Language PDDL.It provides a user friendly language for the first order specification of problems  that (for finite domains) can be translated to SAT problems,Here is a small example(define (domain tournament)    (:types team)    (:constants t0 t1 t2 t3 t4 t5 t6 t7   team)    (:predicates (plays ?x ?y   team))    (:axioms      (and         (forall (?x   team) (not (plays ?x ?x)))        (forall (?x ?y   team) (iff (plays ?x ?y) (plays ?y ?x)))        (forall (?x   team)(= 3 (?y   team) (plays ?x ?y)))      )    ))                ","Translate first order definitions in the finite domain definition language FDDL to propositional form, and integrate with Haskell, and other, tools for SAT solving.",1,2   Moderate,Minimal completion requires production of a correct SAT translation in DIMACS format. A good project should also provide for presentation of counterexamples in terms of the original inputs. The project should also produce a selection of instructional examples suitable for first year teaching.,8,0,f
4608,Deep Reinforcement Learning for Robot Control Tasks,ug4,8070,2020 03 11 16:18:21 00,1,"Deep reinforcement learning is a promising approach to enable robots to self adapt and acquire various skills, e.g. for locomotion and manipulation tasks. The goal of this project is to extend an existing deep reinforcement learning approach such that robots can quickly adapt to new environments in the real world using only a limited amount of data. Unlike other areas of application for machine learning, e.g. natural language processing or image detection, we lack the large amount of data in robotics due to the individual/changing properties of systems and environments. This makes most machine learning approaches developed under the Big Data regime unsuitable, because the acquisition of large amounts of data for a robotic system is costly and time intensive.This masters (MInf) project will investigate in the first stage relevant literature in the field of reinforcement learning and robotics, which allow robots to master tasks, such as locomotion and manipulation, in a data efficient manner by adapting their control strategies and/or their morphology. Then a research question will be formulated based on this prior work which will lead to the development of a novel deep reinforcement learning method. Possible algorithms to consider are model free and/or model based deep reinforcement learning algorithms in an actor critic framework which have the ability to process high dimensional states. The experiments will be conducted on simulated robots and/or manipulators, which are provided by the OpenAI gym or PyBullet environment.The developed approach is going to be compared against relevant prior work and, if possible, evaluated on real robot hardware solving a locomotion or manipulation task in the real world.",Develop a new or extend an existing reinforcement learning approach for continuous control tasks,1,3   Hard,"Literature review. Develop a new or extend an existing deep reinforcement learning approach and/or apply it to a novel scenario. Compare your approach to an existing algorithm/baseline on a simulated task (OpenAiGym, PyBullet, etc). Evaluate your method on a robot in the real world (Optional but desired)",8,0,f
4600,Machine Learning for Stock Price Prediction,ug4,6896,2020 03 03 15:59:41 00,1,"Market Research:  Artificial intelligence (AI) is rapidly transforming the global financial services industry. As a group of related technologies that include machine learning (ML) and deep learning (DL), AI has the potential to disrupt and refine the existing trading &amp; investment industry. Approximately 9% of all hedge funds use ML to build large statistical models. In 2016, Aidyia launched an AI hedge fund to make all its stock trades. Sentient Investment Technologies uses a distributed AI system and DL as part of its trading and investment platform. In 2016, the GIS Liquid Strategies group was managing $13 billion with only 12 people. In October 2017, exchange traded funds (ETFs) were launched that use AI algorithms to choose long term stock holdings. [1] Most empirical research indicates that evidence based algorithms more accurately predict the future than do human forecasters. Dietvorst et al. (2016, 2015).Project Goal:  The goal of this project is to build a ML model that can  predict the future stock price of any public company, any number of days in the future, by using several sources of data:historical time series data (using LSTM)financial news (using NLP)  other financial indicators (such as Financial Ratios, Quarterly Financial Reports, Market Indices...)  The model should be able to predict the future stock price any number of days in the future, but we will split the predictions into 2 categories: short term prediction (up to 7 days) and long term prediction (up to 2 years). Since the project will run over 2 years, the first year will be focused on short term prediction and the second on long term prediction.To evaluate the model, I plan on running an experiment with Economics/Business students to try beating the model at predicting future stock price (using either historical or real time data).The data: There are vast amounts of financial data and APIs available on the web. I plan on using sources such as Yahoo Finance API,  Bloomberg Market and Financial News API, Financial Times API, Alpha Vantage API or possibly Twitter API, to gather most of the data on historical stock prices, financial news and financial indicators.Project Structure: I expect to run this project over 2 years (4th year and 5th year/Minf).In the first year, the project will focus on:Collecting time series data of financial indicatorsDesigning, Building and Optimising LSTM models to predict future stock pricePredicting the trend &amp; stock price in the short term (e.g. Up to 7 days)Evaluating the model's short term predictions on unseen stock data, against humans with intermediate knowledge of Trading &amp; InvestingIn the second year, the project will focus on:Collecting text information from Financial Publications and Financial Market platformsUsing NLP to gain insight into Market Sentiment and News Reading, and combine those in the LSTM modelPredicting the trend/stock price in the long term (e.g. Up to 2 years)Evaluating the model's long term predictions on unseen stock data, against humans with intermediate knowledge of Trading &amp; InvestingResources that are likely to be used: Neural Network libraries such as Tensor Flow, PyTorch, Keras; NLP libraries; Financial APIs, Tools and Publications",Build a ML model (combining LSTM and NLP) for predicting stock price values and trends in both short term and long term,1,3   Hard,"A working ML model which is able to predict stock price values and trends for any public company in both short term and long term. The model's predictions should be evaluated on unseen data, where it is expected to perform better than humans with intermediate knowledge of Trading & Investing.",8,0,f
4368,Hardware Support for JIT Compilation,ug4,1390,2020 02 11 16:49:33 00,1,"Just in time (JIT) compilation is a code generation technique, which generates executable machine code for a given program at its runtime. JIT compilation has many uses, including code generation in language virtual machines (e.g. Java VM) and dynamic binary translation (DBT). Both have in common that overall application performance is not only dependent on the quality of the generated code, but also the speed of the JIT compiler itself.In this project the student will investigate the feasibility of hardware support for JIT compilation, e.g. through instruction set extensions, special purpose functional units, or architectural modifications (e.g. scratch pad memory for translated code). For this the student will work with a processor simulation environment and suitably prototype these extensions. At the same time, a simple JIT compiler would need to be adapted to take advantage of the new hardware extensions and its performance evaluated.",To develop a set of architecture extensions to directly support JIT compilers,2,3   Hard,Prototyped HW support for a JIT compiler in the ArchC/GenSim/Captive framework and evaluated it on small benchmarks.,8,0,f
4455,Performance characterization of serverless computing,ug4,1348,2020 02 21 15:05:15 00,1,"Serverless computing has emerged as a disruptive cloud computing programming paradigm that combines a new type of workload (functions) with a new cloud infrastructure (microVMs [1]). The aim of this project is to  study the implications of colocating hundreds, or even thousands, of microVMs on a single physical server.  This includes characterizing the throughput and tail latency of the server, analysis of scalability bottlenecks, and a study of system and microarchitectural events (e.g.,  VMexits, cache miss rates, TLB miss rates, etc). Another possibility could be building a benchmark suite of serverless functions and evaluate it using function invocation frequencies based on published studies [3].During the project, the student is going to use (and potentially contribute to) a number of open source cloud frameworks and systems, including Amazon's Firecracker [1] hypervisor and Google's gRPC.Important:  If interested in the project, you do NOT need to email me unless you have a specific question. Just indicate that you are interested in the project, and join the online information meeting.Information meeting (online):When: Friday, 20 March, 3 3.30pmAccess link (Blackboard):  https://eu.bbcollab.com/guest/b685965d7bb6479c9211aab7cd09d20d",Conduct a performance evaluation of a serverless computing deployment.,2,Variable,Depends on the specifics of the project.,8,0,f
4460,Color identification app,ug4,1348,2020 02 21 15:33:22 00,1,"Color blindness is a common condition that, by some estimates, affects over 10% of the world's population. The most common type of color blindness is partial, where a person can accurately identify some colors but not others. Indeed, red green color blindness is the most prevalent type and affects up to 8% of males and 0.5% of females of Northern European descent. A person with such a condition can easily distinguish, for instance, a red from a blue color, but might struggle to tell a red apart from green or brown.  In this project, you will design an app that will allow the user to point to a color on screen, and the app will state the color. You can use any platform you wish (desktop or mobile). The goal is to have, by project completion, a functional, user friendly app that's openly available (e.g., via the Android marketplace or Chrome store).The student is expected to produce a first version of the app early enough in the project to have time to gather extensive user feedback (e.g., by releasing the app online and/or through dedicated user studies), then incorporate the feedback into the app, then do another iteration of user studies.  Important:  If interested in the project, you do NOT need to email me unless you have a specific question. Just indicate that you are interested in the project, and join the online information meeting.Information meeting (online):When: Friday, 20 March, 3.30 4pmAccess link (Blackboard):  https://eu.bbcollab.com/guest/20ed14c3ea2442e9a43d0c694f62c9c9    ",Develop an app to help color blind people identify colors on screen,2,1   Easy,Fully functional app on a platform of a student's choice to help identify colors on screen. At least one iteration of gathering user feedback and incorporating it to improve the app.,8,0,f
4482,Representing privacy contexts in multi agent systems,ug4,7139,2020 02 22 15:21:47 00,1,"Managing privacy in online systems is a difficult task, and it would be ideal to develop artificial intelligence (AI) techniques  such as agent based models   that can preserve the privacy of the users automatically. But first we should understand privacy expectations of humans better.Understanding privacy preferences of the users is a challenge in online systems such as IoT systems or online social networks. Various studies show that privacy preferences of the users vary based on particular scenarios [1]; hence, sometimes it is difficult to generalise such preferences.  In this project, the aim is to understand the role of contexts in privacy decisions. Context depends on many factors such as location, people and such [1]. However, it is a difficult notion to model since a user can be involved in various contexts at the same time [2].    The goals in this project are:Developing a formal representation of privacy contextsBuild a dataset of use case scenarios  Modeling the use case scenarios with the proposed formal representationIf you are interested in the project, you should send me an email to arrange a one to one meeting. The deadline to send me an email is 20th of March.",Development of a formal representation of privacy contexts together with a set of use case scenarios,1,3   Hard,Development of a formal representation of privacy contexts together with a set of use case scenarios,8,0,f
4484,A plugin that reminds you the privacy settings in online systems,ug4,7139,2020 02 22 15:34:42 00,1,"In online social systems, users are allowed to create and share content about themselves and others. The content being shared can affect anybody involved in it, and the privacy of individuals can be compromised. Increasingly, users need intelligent tools that can help them to manage their privacy online [1].Users have different online profiles with various privacy settings. It is a challenging task to manage privacy in all these platforms, and remember the privacy settings that are changing all the time. The goal in this project is to develop an ontology to model privacy settings in online systems such as Facebook and Twitter. This ontology will be used by an agent that will run as a browser plugin to help the user to manage her privacy.  If you are interested in the project, you should send me an email to arrange a one to one meeting. The deadline to send me an email is 20th of March.",Development of a browser plugin to help users preserving their privacy online,1,1   Easy,Development of a browser plugin to help users preserving their privacy online,8,0,t
4379,Make an Amazon Alexa talk to people about privacy,ug4,1351,2020 02 12 23:05:09 00,1,"The project is exactly what it sounds like. You will be building an Alexa skill, which includes building server code, that would allow an Alexa user to interact with Alexa's privacy policy through Alexa itself. The challenging aspects of this project will likely come from trying to automatically parse the Alexa privacy policy and interact with it based on natural language. The Usable Privacy Policy Project (link below) has a corpus of labeled privacy policy data which may help.Mark restriction This project is limited to those who generally have high marks. This restriction is unusual, and my other projects do not have it. This is the first year I have run this project and the project content is not well formed. The project is also creative in nature, requiring a student to spend time thinking not only about how to accomplish the goals, but also what the goals even are. In my experience, doing this type of project while also taking 4th year courses is quite challenging from a time management perspective. MInfThis project is suitable for MInf. There are many NLP aspects to the project that a student can explore in the second year. Privacy of smart speakers is also a large issue offering many possibilities for a second year project. MeetingsI am holding virtual office hours every morning 10am 11am at the following link: https://eu.bbcollab.com/guest/bdc065c679e64974a94001f8262dd7d0","Create an Amazon Alexa skill that allows an Alexa to read out the contents of privacy policies, search them via voice command, and read out the outcome.",1,3   Hard,"Create an Amazon Alexa skill that allows an Alexa to read out the contents of privacy policies, search them via voice command, and read out the outcome.",8,0,t
4610,Design a Visual Programming Language for Making Minecraft Server Plugins,ug4,8257,2020 03 18 18:53:52 00,1,"Minecraft is the best selling video game of all time, and is nowadays at the peak of its popularity. As of September 2019 it had 112 million monthly players.These players come from such diverse demographic backgrounds[1] that it is difficult to accurately keep track of, especially as Microsoft don't release any figures themselves, but online polls show that more than 50% of players are less than 21 years old[2].One of the most important parts of Minecraft is the multiplayer mode. Microsoft allows anyone to host their own Minecraft server, and since 2010 there have been more than 450,000 publicly advertised Minecraft servers[3], along with the countless private servers that players create to enjoy the game together with friends.Minecraft has a very healthy and positive modding community, and one of the reasons servers enjoy such popularity is because server owners can run plugins on their servers, which add new exciting aspects to Minecraft gameplay. The most popular servers of today range from arcade like minigame servers to dungeons and dragons style RPG servers, it's very exciting how many possibilities are made available to players by server plugins.However, since it requires proficiency in Java, most users find plugin development a daunting task and never get the chance to bring their own ideas to reality.I would like to make a visual programming language that is easy for non programmers to develop with, and can be used to make Minecraft plugins. The target audience will be the huge number of Minecraft players who have no way to develop their creative vision because they are not confident programmers. I am going to try to focus on younger age groups especially, and hopefully use the project as a way to inspire younger generations to gain interest in programming, since the programming language will have such a strong connection with the Minecraft video game and a strong feedback cycle; it will be easy to see how the code that you write affects the game.Scratch is a visual programming language developed by MIT's Lifelong Kindergarten group[4]. It is designed to help children take their first steps into programming, and is thus very simple and intuitive traits I am aspiring to replicate in the visual programming language that I will produce as the output of this project.I would like to make a webapp interface similar to Scratch, which contains the visual programming language. It will transpile into a scripting language (I will most likely choose JavaScript) which can then be run on the Minecraft server.The main problems I currently look forward to solving are:  designing an intuitive, usable visual programming language  designing a pleasant user interface for it, that won't scare away the target audience  reducing risk to the server from malicious (intentional or unintentional) code",Design and develop a visual programming language that can run on the Minecraft server to customise game behaviour,1,2   Moderate,A working implementation of a visual programming language interface that can be used by novice or non programmers to develop plugins that customise the behaviour of Minecraft servers.,8,0,f
4360,Virtual Reality Squash Trainer,ug4,1334,2020 02 11 12:45:24 00,1,"Cheap, standalone VR systems like the Oculus Quest have great potential for helping users to exercise or improve in real sports.In this project you will build an engine for squash training on the Oculus Quest. You will create a virtual squash court, allow the user to choose from a number of drills and play them on court. The physical simulation will be as close as possible to the real experience of being on court so that a player could actually drill in VR.No AI player would be needed, but could be included if time permits. In addition, non standard drills could be created that are only possible in VR.Evaluation could be by having squash players try the trainer.Why is this project marked 'very hard'?Really it's because I'm not a graphics guy, I've never programmed the Oculus Quest or anything like this before.   That means that you, the student, will be pretty much on your own in terms of technical support for me. If, however, you have done something like this, perhaps building 3d games, then this could be easy.",Build a playable squash simulation in virtual reality on the Oculus Quest,2,4   Very Hard,Build a playable squash simulation in virtual reality on the Oculus Quest,8,0,f
4611,Exploiting Neural Networks using Adversarial Examples,ug4,7757,2020 03 19 17:38:32 00,1,"Image recognition software has become common place in our society. From facial recognition to content filters; with little interpretability of most of these systems it can be easy to assume they are secure. However most have a shared vulnerability.Adversarial examples constructed by applying a small perturbation to an input out a neural network with the goal of drastically altering the output. As the way humans interpret images is understandably different to how machines interpret images, this small perturbation can be unnoticeable to a human eye.  This could mean, for example, if we were trying to classify a picture of a dog (call this input x) using a popular trained model such as ResNet50, we can use gradient descent on x to find some input X', such that the difference between x and X' is minimal, however the output of ResNet50 is still our target class. The result of which will be some carefully applied noise to the image that can successfully trick the classifier.This is a significant problem from a security perspective. For example, this process could be carried out in order to modify a road sign such that an autonomous car might classify a stop sign as a 70mph sign, causing significant risk to life.This project could be taken in multiple directions, given the number of classifying models that exist. For example, the easiest approach would be to alter the pixels of an image on a computer; however one could also train a robust adversarial example that could be trained over multiple lighting conditions, skew, rotation etc, so that an image can be printed and photographed in the real world. Another approach could be to print out a small adversarial patch (Brown, 2018) that can be applied to some environment, or perhaps altering audio slightly to trick some speech recognition software.Adversarial Patch (Brown, 2018)    https://arxiv.org/pdf/1712.09665.pdf",To trick various neural network classification models using small perturbations of the image in order to classify an image as what we would like it to be.,1,Variable,Trick some classification model (Eg ResNet50) using a robust targeted attack that can be recognised in real world conditions. The project should develop at least one of the approaches discussed in the different directions section of the description.,8,0,f
4571,Interpretable machine learning in healthcare,ug4,3901,2020 02 28 22:05:56 00,1,"As machine learning tools are gaining popularity in clinical settings, e.g., for risk prediction, there is a strong need of 'explaining' why a certain recommendation is being made, e.g., why a patient is being characterized as high or low risk, so the clinician can make a more informed decision [4, 6]. Intrepretable machine learning is an emerging area that is addressing this problem, and numerous approaches have been proposed in the last years, e.g., rule based methods, example based methods, model specific and model agnostic method etc. [1]. Recently, many of these techniques have been applied to health care data, e.g., for mortality prediction in ICU [2], for Type II diabetes risk prediction [3], for hypertension prediction [5] etc.In this project, you will apply a selection of interpretable machine learning model on ICU data, and compare their performance. This project will be held in collaboration with Annemarie Docherty from the Usher Institute. You will access a national dataset of all adult patients admitted to Scottish ICUs 2010 2018. This contains details of each patient's ICU stay (Scottish Intensive Care Society Audit Group SICSAG), Scottish Morbidity Record of acute hospital admissions (SMR01) and out patient appointments (SMR 00), Prescribing database, and Scottish death records. Approximately 15,000 patients are admitted to Scottish ICUs each year, providing a nine year cohort of approximately 135,000; around 34,000 patients will have co existing CVD. Data for each patient will be available for five years prior to their first ICU admission, and for two years following admission. You will be guided regarding which variables are clinically important to include in the analyses.If you are interested in the project, please send 1. your CV, and the scores of machine learning/mathematics/statistics courses you have taken, e.g., IAML, linear algebra etc. 2. a short note (up to 200 words) on why you are interested in the project, e.g., if you have any previous experience on related topics, and 3. if you are familiar with ML interpretability and/or critical care data, a short note (up to 200 words) on what you would like to do in the project.    Annemarie and I will hold an information session on 11th March at 9am at Information Forum 1.16. We will describe the project and dataset in detail, and answer any questions you have.",Assess the performance of interpretable machine learning methods for risk analysis in ICU data,2,3   Hard,Finish task mentioned in the project description.,8,0,f
4612,User space thread migration in Linux,ug4,7197,2020 03 21 12:25:29 00,1,"The popcorn Linux project (www.popcornlinux.org) aims at providing thread (and process) migration across different Linux powered machines. Thread (and process) migration is supported between machines with the same processor (e.g., x86 and x86), and different processors (e.g., x86 and ARM). Popcorn Linux thread (and process) migration is implemented in kernel space    which implies that to use it you need to patch your Operating System kernel, not always an option in many environments.This project aims at implementing thread migration completely in user space, as a user library with no Operating System kernel modifications. Note that thread migration requires at least one thread of a process to run on a machine, and the other threads running on another machine. To enable multiple threads to run among different machines a distributed shared memory protocol must be implemented. Additionally, distributed operating system services must be provided. The implemented thread migration will be compared (for perfomance) with the kernel level thread migration existent in Popcorn Linux. It will be eventually compared also with similar projects.",Develop a user space thread migration in Linux and compare its performance with thread migration in Popcorn Linux,2,3   Hard,"Code compiles, executes correctly, and its characteristic times have been measured and compared with the original software.",8,0,f
4585,A Software Tool for Providing Background Information of Posts on Social Media,ug4,8221,2020 03 02 14:13:00 00,1,"It is widely accepted that using social medias to receive news and other information from so called influencers rather than traditional medias. However, because of the leak of professional training, those influencers often cause untrue rumours spreading. This phenomenon is quite common around world. In some regions, due to the relatively strict media censoring, local traditional medias are usually not willing to be the ones that report some sensitive news at the first place, and that is where those influencers come. For example, during events concerning public health like recent coronavirus outbreak, many influencers forwarded posts from unknow sources   for instances, posts about people seeking help and new cases of coronavirus. Of course, most of the time it was out of good intentions, and part of those posts are the truth, but still there are many of them are untrue rumours. With this tool, users will be able to identify the truth with ease by refering to the provided reports from professional media, and also important   this tool is expected can evenly show the comments about particular event from social media users holding different opinions, so that  the spiral of silence  (social group or society might isolate or exclude members due to the members' opinions. This stipulates that individuals have a fear of isolation. This fear of isolation consequently leads to remaining silent instead of voicing opinions) may not be that easy to be constructed. It's also can be expected that knowing the background information can make people are more likely to understand each other, so the hatred and cleavage on the internet may be dissolved a little.",To develop a software tool that for most of posts on social media can provide reliable relavant reports from trusted sources and evenly show the comments about particular event from people holding different opinions,3,3   Hard,"Develop a software tool as described above, which for a social media post regarding to particular social event, this tool needs to provide a summary consisting od relative reports from reliable traditional medias and also some typical comments from other socal media users, classified as 2 (or more) categories based on the opinions that authors holding.",8,0,t
4607,Implementing Federated Learning in Tensorflow.js,ug4,8039,2020 03 11 10:57:07 00,1,"Federated LearningFederated learning  (aka collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging their data samples. This approach stands in contrast to traditional centralized machine learning techniques where all data samples are uploaded to one  server,  as well as to more classical decentralized approaches which assume that local data samples are  identically distributed.Principal GoalThe main goal of this study is to implement an experimental version of federated learning in tensorflow.js. A use case of this is to use the implemented federated tensorflow.js in a browser extension so that it can process sensitive data such as browser history. The user can be recommended similar websites based on his browser history. The user could also be recommended to take breaks at strategic times during the day based on the usage pattern.",The goal is to implement an experimental version of federated learning in tensorflow.js,1,Variable,A working contribution to the overall goal.,8,0,t
4494,Further exploration of Euler's Foundations of Differential Calculus in Isabelle/HOL using nonstandard analysis,ug4,1339,2020 02 23 16:41:22 00,1,"In the first part of his book  Institutiones Calculi Differentialis  (Foundations of Differential Calculus),  first published in 1755, Euler discusses the theory of differential calculus. He introduces the notions of first and higher order increments (for variables), called differences, which he then extends to differentials ("";infinitely small""; increments) and uses to examine changes in functions when their variables are increased or decreased. Through deft manipulations (such as the division of infinitely small quantities) and intuition, he obtains many interesting results about derivatives of functions (e.g.  trigonometric  functions  represented  as power series) that most  modern mathematicians would frown upon and regard as lacking rigour (or even view as being careless but lucky).The aim of this project is to extend the historical, machine driven investigation [6] of some of the concepts and reasoning introduced in (the translated version of) Euler's book by formalising them in the proof assistant Isabelle. Formal notions of infinitely large and infinitely small (infinitesimal) numbers will be introduced using nonstandard analysis, which is available as a library in Isabelle.  This formalisation will require clarifying potential ambiguities not only in Euler's original text but possibly in the translation. Anyone interested in this project must be very comfortable with logic and formal reasoning. Existing experience with a proof assistant (preferably Isabelle) is a must.  Any interested student must discuss the project with the supervisor in order to be considered for it.   As part of the contact email, please send information on the courses you have taken and marks/grades obtained on your degree so far.  See also https://aiml.inf.ed.ac.uk for current and past projects, and other  potential  project topics.","Investigate, represent and formalise Euler's reasoning about differential calculus formally using Isabelle/NSA",2,4   Very Hard,Representation of the notion of orders and formalisation of a substantial part (if not all) of the paper.,8,0,f
4581,Assessing dataset bias in computer vision,ug4,1335,2020 03 02 07:12:24 00,1,"Recent studies demonstrate significant gender and racial bias in datasets used to train machine learning algorithms, such as face recognition systems We address the following objectives in this project    Mitigating bias in datasets: We take popular vision datasets  and explore the extent to which they are balanced in accordance with different criteria, such as  gender, ethnicity, facial expression categories. To do this, we will first enumerate the different types within each category in the existing dataset, such as number of females and males for the gender   category.  We will then explore techniques to augment the vision datasets in several ways so that we produce different balanced datasets, one for each category under consideration (gender, ethnicity, facial expression). In particular, we will explore image transformation techniques to create additional data for underrepresented category types. We will also use manual selection and labeling when image transformation techniques are not realistic.  Assess selection bias in face recognition techniques: Using the balanced datasets created with our first goal, we will explore adequacy of existing face recognition techniques with respect to the categories mentioned earlier: gender, ethnicity and facial expression.","Biased datasets result in poor classification accuracies for underrepresented demographics with serious implications in certain domains, eg. cancer detection, law enforcement. In this project, we focus on computer vision systems and aim to better understand and address the problem of dataset bias with respect to gender and race.",1,2   Moderate,The project should propose techniques to balance vision datasets in accordance with different criteria and assess selection bias using object or face recognition techniques.,8,0,f
4613,Exploration of Neural Symbolic AI,ug4,5226,2020 03 26 21:28:38 00,1,"A neural symbolic system is a system that "";combines the statistical, data driven learning capabilities of neural networks with symbolic reasoning techniques"";. [1]  In a recent MIT lecture [2] by IBM director David Cox, he pointed out that state of the art neural networks do not learn like us humans in many regards.  The most prominent being that neural networks need a lot of training data and can still be easily tricked by adversaries. In some circumstances, neuralnetworks might need thousands if not millions of training data to reach good accuracy while humans only need 1. This made me think about how I learnedto read digits (0   9)  when I was little, did I need to see 60000 images of hand written digits to not mistake a 2  for a 8? Probably not. I do believe that we  do some reasoning like: 'a 2  has a flat bottom and a curved top', but it is also clear that reasoning is not enough to describe what a 2  looks like. Thus I  (and other people [3]) think neural symbolic systems are much more suited to solve problems in some particular domains.In this project, I will explore existing neural symbolic framework, understand the different frameworks researchers have proposed and implemented, benchmarkthem and discuss their pros/cons,  find out when they are more/less effective than neural networks.","To explore neural symbolic approaches to machine learning, discuss their pros and cons, and compare them to neural networks",1,3   Hard,A comprehensive understanding and analysis of existing neural symbolic frameworks.,8,0,f
4620,Place cells in the mushroom body,ug4,1341,2020 09 18 17:23:44 00,0,(none),(none),1,(none),(none),8,0,f
4621,Polished.me,ug4,1341,2020 10 27 12:01:55 00,0,(none),(none),1,(none),(none),8,0,f
4622,testing project,ug4,1341,2020 10 27 12:45:04 00,0,(none),(none),1,(none),(none),8,0,f
4623,Join Ordering with Learned Cardinality,ug4,1341,2020 10 27 12:59:52 00,0,(none),(none),1,(none),(none),8,0,f
4624,Join ordering of SQL optimization,ug4,1341,2020 10 27 13:03:00 00,0,(none),(none),1,(none),(none),8,0,f
4625,Secure and Cloud Native Core Networks,ug4,1341,2020 10 27 14:54:36 00,0,(none),(none),1,(none),(none),8,0,f
4626,A Tabular Data Workbench,ug4,1341,2020 10 27 15:13:45 00,0,(none),(none),1,(none),(none),8,0,f
4627,Deep learning for variant calling from nanopore sequencing data,ug4,1341,2020 10 27 15:43:38 00,0,(none),(none),1,(none),(none),8,0,f
4628,A Semi Automated Tool for Detecting and Mitigating Racial Bias in Visual Datasets,ug4,1341,2020 10 27 20:48:12 00,0,(none),(none),1,(none),(none),8,0,f
4629,Designing A Time Management App For And With Informatics,ug4,1341,2020 10 29 10:51:40 00,0,(none),(none),1,(none),(none),8,0,f
4630,Rubbish sorting game,ug4,1341,2020 10 29 10:52:12 00,0,(none),(none),1,(none),(none),8,0,f
4631,Algorithms for Handling Multivariate Poisson Distributions,ug4,1341,2020 10 29 10:52:31 00,0,(none),(none),1,(none),(none),8,0,f
4632,Cell type specific effects of pharmacological manipulations in the retina,ug4,1341,2020 10 29 10:53:35 00,0,(none),(none),1,(none),(none),8,0,f
4633,Speech recognition for medical teams,ug4,1341,2020 10 29 10:54:45 00,0,(none),(none),1,(none),(none),8,0,f
4634,Robust Know Your Customer using Distributed Ledger,ug4,1341,2020 10 29 10:56:47 00,0,(none),(none),1,(none),(none),8,0,f
4635,Robomorphism,ug4,1341,2020 10 29 11:13:49 00,0,(none),(none),1,(none),(none),8,0,f
4636,Measuring Performance Effect of Tor Bandwidth Publishing,ug4,1341,2020 10 29 11:14:04 00,0,(none),(none),1,(none),(none),8,0,f
4637,Image based Localization and Tracking of a Contiuum Flexible Robot,ug4,1341,2020 10 29 11:14:30 00,0,(none),(none),1,(none),(none),8,0,f
4638,End to End Service based 5G Mobile Network Infrastructure and Use Cases,ug4,1341,2020 10 29 11:15:11 00,0,(none),(none),1,(none),(none),8,0,f
4639,Link and Structure Discovery over HOL Light Theorem,ug4,1341,2020 10 29 11:15:27 00,0,(none),(none),1,(none),(none),8,0,f
4640,MarkEd: An Online Marking Tool,ug4,1341,2020 10 29 11:16:34 00,0,(none),(none),1,(none),(none),8,0,f
4641,Using usernames to predict the survival of accounts,ug4,1341,2020 10 29 11:16:49 00,0,(none),(none),1,(none),(none),8,0,f
4642,Assessment of pseudotemporal ordering methods on single cell RNA seq data,ug4,1341,2020 10 29 11:18:38 00,0,(none),(none),1,(none),(none),8,0,f
4643,Investigating the effect of acoustic and language model quality on semi supervised training for automatic speech recognition,ug4,1341,2020 10 29 11:20:10 00,0,(none),(none),1,(none),(none),8,0,f
4644,Rare Fish Species Extraction And Clustering,ug4,1341,2020 10 29 11:20:43 00,0,(none),(none),1,(none),(none),8,0,f
4645,Mandelbrot Maps: Web application for exploring fractals,ug4,1341,2020 10 29 11:20:54 00,0,(none),(none),1,(none),(none),8,0,f
4646,Assessment of pseudotemporal ordering methods on single cell RNA seq data,ug4,1341,2020 10 29 11:21:07 00,0,(none),(none),1,(none),(none),8,0,f
4647,Implementing Interoperability Between the Vulkan and SYCL,ug4,1341,2020 10 29 11:21:53 00,0,(none),(none),1,(none),(none),8,0,f
4648,CLPractice   Tools for Learning: Computation and Logic,ug4,1341,2020 10 29 11:24:35 00,0,(none),(none),1,(none),(none),8,0,f
4649,Towards the Automatic Synthesis of Coherence Protocols,ug4,1341,2020 10 29 15:15:52 00,0,(none),(none),1,(none),(none),8,0,f
4650,Designing a new language for physic computations with support for precision and physical units,ug4,1341,2020 10 29 15:16:13 00,0,(none),(none),1,(none),(none),8,0,f
4651,UnitÃ©   the physics programming language,ug4,1341,2020 10 29 15:16:26 00,0,(none),(none),1,(none),(none),8,0,f
4652,Visual Sudoku Solver,ug4,1341,2020 10 29 15:16:45 00,0,(none),(none),1,(none),(none),8,0,f
4653,Cancer Data Science and Epigenetics,ug4,1341,2020 10 29 15:16:57 00,0,(none),(none),1,(none),(none),8,0,f
4654,Analysing the robustness of transport ticketing apps,ug4,1341,2020 10 29 15:17:09 00,0,(none),(none),1,(none),(none),8,0,f
4655,Tools for teaching Computation and Logic,ug4,1341,2020 10 29 15:17:21 00,0,(none),(none),1,(none),(none),8,0,f
4656,Assessing Intelligence of Explore Exploit Algorithms,ug4,1341,2020 10 29 15:17:32 00,0,(none),(none),1,(none),(none),8,0,f
4657,Implementing Walukiewicz' completeness proof for the modal mu calculus,ug4,1341,2020 10 29 15:17:44 00,0,(none),(none),1,(none),(none),8,0,f
4658,"Developing Educational Games, Smart Objects, or Other Tools for Supporting Children with Autism (Generic proposal)",ug4,1341,2020 10 29 15:18:14 00,0,(none),(none),1,(none),(none),8,0,f
4659,Tutorial and Sample Project for engineering Domain Specific Languages with DSL Developer Kit,ug4,1341,2020 10 29 15:18:39 00,0,(none),(none),1,(none),(none),8,0,f
4660,Parallel Query Evaluation in Streaming Environments,ug4,1341,2020 10 29 15:19:00 00,0,(none),(none),1,(none),(none),8,0,f
4661,Improved Sequence Alignment For Merging Functions,ug4,1341,2020 10 29 15:19:13 00,0,(none),(none),1,(none),(none),8,0,f
4662,The effect of noise in quantum advantage sampling problems,ug4,1341,2020 10 29 15:19:24 00,0,(none),(none),1,(none),(none),8,0,f
4664,Signal compression,ug4,1341,2020 10 29 15:19:52 00,0,(none),(none),1,(none),(none),8,0,f
4665,Automated trend identification in thematic investing,ug4,1341,2020 10 29 16:44:16 00,0,(none),(none),1,(none),(none),8,0,f
4666,Analysing the Robustness of Transport Ticketing Apps,ug4,1318,2020 11 02 14:44:25 00,0,(none),(none),1,(none),(none),8,0,f
4667,Tutorial and Sample Project for engineering Domain Specific Languages with DSL Developer Kit,ug4,1341,2020 11 02 14:50:02 00,0,(none),(none),1,(none),(none),8,0,f
4668,OwnHome: Retrofitting Security and Privacy Measures to Smart Home Devices,ug4,1341,2020 11 02 15:17:54 00,0,(none),(none),1,(none),(none),8,0,f
4669,Performing KYC with Distributed Ledger Technology,ug4,1342,2019 02 24 21:54:38 00,1,"KYC ("";know your customer"";) is a required process that financial institutions should undertake for  their customers. It has been observed that this process is performed sub optimally at present anda solution based on Distributed Ledger Technology would be  potentially   more effective.In this project you will have the  opportunity  to design a  system  and implement it in ablockchain platform (such as Ethereum or Cardano). The system will make it feasible for an organisation  to onboard a customer and share costs with other organisations that would like to onboardthe same customer.   ",Design and provide a prototype implementation for a KYC system,1,3   Hard,Design document. Security and Performance Analysis. Demonstration of the prototype,8,1,f
4670,Visual Sudoku Solver,ug4,2228,2019 02 06 10:45:21 00,1,"This project is in two parts; firstly to use vision recognition to identify and extract a Sudoku puzzle from a photograph, and secondly to solve the puzzle in real time. When implemented on a mobile phone, the phone user should be able to point the phone camera at a printed puzzle and see the image overlaid with the solution. The first part requires image recognition skills and is hard. The second part is moderately easy. ",To recognize a Sudoku puzzle in an image and then solve it,2,Hard,"Completion of just the second part would be just about sufficient for completion of the project, but completion of both parts is desirable for a complete application.",8,0,f
4671,Cancer data science and epigenetics,ug4,2985,2019 02 20 12:10:53 00,1,"Heterogeneity if the central hurdle to finding a cure for cancer. It is now well understood that evolutionary processes within individual tumours account for a substantial fraction of the genetic heterogeneity in cancers. Much less is known about epigenetic heterogeneity, for example, how changes in DNA methylation occur and evolve during cancer. A critical concept to understand epigenetic heterogeneity is the notion of epialleles, i.e., how many distinct epigenetic states coexist in a tumour sample at a certain genomic region. Recent work by Barrett et al (https://www.ncbi.nlm.nih.gov/pubmed/28743252) proposed a Bayesian framework to infer the fraction of different epialleles in a sample, and subsequently use it in a cancer evolution scenario. A major limitation of this work however is the use of binary epiallele patterns, resulting in an exponential complexity of the algorithms. In this project we will explore the idea of defining epialleles starting from smooth patterns of methylation as in Kapourani and Sanguinetti 2016 (https://www.ncbi.nlm.nih.gov/pubmed/27587656); in particular, the work will focus on understanding the design limitations of such an approach, i.e. what are the sequencing coverage requirements in order to infer robustly epiallele frequencies.The project will involve a fair amount of Bayesian machine learning and will offer the opportunity to learn and understand modern biomedical concepts in a translationally relevant problem. Previous knowledge of biology beyond high school level is not required, but an eagerness to learn is essential.",Discover the robustness of methods to infer epiallele fractions from sequencing data,1,3   Hard,Defining the concept of smooth epialleles; establishing robustness through data simulation,8,0,f
4672,Fast Sequence Alignment for Merging Functions,ug4,2016,2019 02 22 17:37:15 00,1,"Binary size is money when dealing with small, resource hungry devices like IoT sensors, embedded devices, and even smartphones. Strangely, compilers do not do a very good job on that front.  Function merging by sequence alignment[1] is a promising new technique trying to change that. This approach identifies pairs of similar functions and replaces them with a single merged function where identical statements are merged and non identical statements are executed only when the corresponding function would be called. The merged function ends up requiring less space but having behaviour identical to that of the original functions.The most computationally expensive part of this technique is sequence alignment. Sequence alignment is borrowed from bio informatics where it is used to identify regions of similarity in different DNA sequences. When given a sequence of instructions, it identifies the best  way to merge the two sequences without changing the internal order of each sequence.The existing implementation uses the  Needleman Wunsch algorithm, a dynamic programming algorithm that finds globally optimal alignments but has O(mn) worst case performance. The aim of this project is to reduce the cost of sequence alignment by using other, potentially approximate, algorithms[2][3]. Approximating the optimal alignment will affect function merging  negatively, so part of this project will be to evaluate the trade off between alignment quality and speed and develop runtime heuristics to control this trade off.The project will use the existing function merging compiler pass[4] as a starting point.",To make merging similar functions faster through alternative sequence alignment algorithms.,1,3   Hard,Modified implementation of the function merging LLVM pass which will be orders of magnitude faster than the existing implementation when applied on large programs while maintaining most of the benefit of function merging.,8,0,f
4673,Representation learning of Dyadic collaborative Manipulation systems,ug4,5877,2019 03 20 12:07:14 00,1,"Till recently robots were found in industrial setups to be temporally or spatially separated from the humans, to ensure humans' safety. When it comes to today's robots that's no longer the case, as robots are built to work with and next to humans. However, the unpredictability and the variability of humans actions generate scenarios with frequent plan alternations and considerable uncertainty, to the extent that robots fail to successfully complete the collaborative tasks in hand. On the other hand, humans collaborate naturally, as we are adept at co manipulation due to extensive prior experience. In this project, we venture to enhance our understanding of the mechanisms of joint action, towards developing robot partners that co exist with humans and can act collaboratively in pairs, as human dyads do. Towards achieving this goal, this project is focused, on how can a robot policy be partner informed and flexible towards complying with the requirements of a DcM scenario.This project aims first, to discover the hidden interaction and object centric relationships between two collaborating agents, which in our case is a dyad of two humans. Second, we plan to utilise the learned relationships to guide robot's policy within human robot collaboration tasks. More precisely this project will implement the Neural Relational Inference Model (NRI) initially in 2D and later in 3D. NRI is a variational auto encoder in which the latent space represents the underlying interaction graph, based on Graph Neural Networks (GNNs). We plan to train the NRI on the gathered simulation and real world motion tracking data as described in [2], which will allow us to uncover the hidden relationships, between the two agents and the object. If this main objective is met, we will attempt to move towards developing a control system based on similar technology and principles, as described in paper [3]. The most critical part of both goals is the Graph Neural Networks (GNNs), that are being used to recognize relations between the agents. GNNs are a powerful tool to learn relations between nodes in a graph (relational descriptors), in our case the nodes could potentially be, the usual modelling elements of a robotic system such as position/ orientation of the objects, forces, locations of end effectors. These relational descriptors are elementary blocks of final observed behaviours and identifying them could be the key towards distilling complex dyadic behaviours.   Therefore, the main benefit towards extracting relational representations is, that it will improve the predictability of dyadic systems that involve human agents, by utilizing their underlying relational structure.","With Dyadic collaborative Manipulation (DcM) we refer to a set of two individuals jointly manipulating an object. The observed dynamics of such systems appear very complex due to the interplay of the two individuals (agent, partner) and the object. However, in DcM these dynamics are governed by the relative configuration of the individuals and the object, which we refer to as dyadic manipulation manifold. The structure of this manifold can be described by latent motion couplings (relations) between the physical components of the DcM system. Having identified the structure of the dyadic manipulation manifold can allow us to predict the actions of the partner and potentially, generate agent's control actions that are within the constrained solution space described by the dyadic manipulation manifold. Assuming full observability of individuals' action history during a manipulation task and the object's state, the goal of this project is to identify these relations (dyadic manipulation manifold's structure) between the partner (human), the object and the agent (human or robot).",1,Variable,"Basic completion criteria (Baseline): 1.      Review relevant literature on relational learning starting from [2] and [3]. 2.      Reproduce, understand and test methods on relational learning presented in [2] and potentially [3]. 3.      Identify, propose and develop toy gym like (https://gym.openai.com) environments that correspond to Dyadic collaborative Manipulation (DcM) scenarios and apply methods to learn hidden relational representations. 4.      Design experimental setup, to collect kinematic and/or dynamic data of two humans co manipulating an object. 5.      Propose and test solutions on how to apply the relational learning methods mentioned above on DcM data sets.",8,0,f
4663,Human intention estimation in Dyadic Collaborative,ug4,3360,2020 10 29 15:19:37 00,0,(none),(none),1,(none),(none),8,0,f
4674,Toward a Technology to Overcome Anxiety in Children with Autism,ug4,1341,2020 11 04 10:50:21 00,0,(none),(none),1,(none),(none),8,0,f
4675,Visual Tool for Differential Cryptanalysis,ug4,1341,2020 11 15 11:17:45 00,0,(none),(none),1,(none),(none),8,0,f
4676,Designing a new language for physic computations with support for precision and physical units,ug4,1341,2020 11 15 11:20:20 00,0,(none),(none),1,(none),(none),8,0,f
4677,Tools for Supporting Children with Autism,ug4,1341,2020 11 15 11:39:08 00,0,(none),(none),1,(none),(none),8,0,f
4678,Tor's Hidden Service DoS Problem,ug4,1341,2020 11 15 12:00:23 00,0,(none),(none),1,(none),(none),8,0,f
4679,Intelligent edge processing for Lidar point cloud data,ug4,1341,2020 11 15 12:07:05 00,0,(none),(none),1,(none),(none),8,0,f
(297 rows),,,,,,,,,,,,,
